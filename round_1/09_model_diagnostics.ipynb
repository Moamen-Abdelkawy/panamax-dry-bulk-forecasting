{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL DIAGNOSTICS: FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "Objective: Determine if poor P1A performance is due to:\n",
      "  A) Poor feature engineering (features lack predictive power)\n",
      "  B) Market unpredictability (features are good but market is chaotic)\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Data directory: data/processed/\n",
      "  Models directory: data/models/xgboost_core/\n",
      "  Output directory: data/diagnostics/\n",
      "  Analysis horizon: h=1\n",
      "  Permutation repeats: 10\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA AND MODELS\n",
      "================================================================================\n",
      "[LOADED] P1A: Train=(705, 13), Val=(125, 13), Test=(326, 13)\n",
      "[LOADED] P3A: Train=(705, 13), Val=(125, 13), Test=(326, 13)\n",
      "[LOADED] Targets: Train=(705, 11), Val=(125, 11), Test=(326, 11)\n",
      "\n",
      "Feature counts: P1A=12, P3A=12\n",
      "\n",
      "[LOADING] XGBoost models...\n",
      "  OK P1A model loaded\n",
      "  OK P3A model loaded\n",
      "\n",
      "================================================================================\n",
      "PREPARING CLEAN DATASETS\n",
      "================================================================================\n",
      "P1A_82:\n",
      "  Train samples: 705\n",
      "  Val samples:   125\n",
      "  Combined:      830 (used for importance analysis)\n",
      "\n",
      "P3A_82:\n",
      "  Train samples: 705\n",
      "  Val samples:   125\n",
      "  Combined:      830 (used for importance analysis)\n",
      "\n",
      "================================================================================\n",
      "BASELINE MODEL PERFORMANCE (Validation Set)\n",
      "================================================================================\n",
      "\n",
      "P1A_82:\n",
      "  RMSE: $2,554.85\n",
      "  MAE:  $2,086.38\n",
      "  R2:   -0.3474\n",
      "  WARNING: Negative R2 indicates predictions worse than mean baseline\n",
      "\n",
      "P3A_82:\n",
      "  RMSE: $2,104.67\n",
      "  MAE:  $1,800.18\n",
      "  R2:   0.3141\n",
      "\n",
      "================================================================================\n",
      "METHOD 1: PERMUTATION IMPORTANCE\n",
      "================================================================================\n",
      "Computing permutation importance (10 repeats per feature)...\n",
      "This may take a few minutes...\n",
      "\n",
      "[P1A_82] Computing permutation importance...\n",
      "  OK Complete\n",
      "  Top feature: ODV_T (importance: 6.10)\n",
      "  Positive importance features: 3 / 12\n",
      "\n",
      "[P3A_82] Computing permutation importance...\n",
      "  OK Complete\n",
      "  Top feature: FFAPmxOI (importance: 12.53)\n",
      "  Positive importance features: 3 / 12\n",
      "\n",
      "================================================================================\n",
      "PERMUTATION IMPORTANCE RESULTS\n",
      "================================================================================\n",
      "\n",
      "[P1A_82] Top Features:\n",
      "--------------------------------------------------------------------------------\n",
      "  ODV_T                                             :     6.10 +/- 8.82\n",
      "  Atlantic Region Industrial Production Growth      :     2.44 +/- 2.70\n",
      "  PDIC                                              :     0.05 +/- 3.21\n",
      "  PDIOPEX                                           :    -0.00 +/- 0.00\n",
      "  Panamax Orderbook % Fleet                         :    -0.44 +/- 0.83\n",
      "  FFADVPmx_T                                        :    -8.82 +/- 6.87\n",
      "  P1EA_Basis                                        :   -16.95 +/- 15.46\n",
      "  VLSFO                                             :   -27.72 +/- 7.34\n",
      "  BCI                                               :   -41.34 +/- 22.50\n",
      "  Panamax Bulkcarrier 65-100,000 dwt Atlantic Deploy:   -99.23 +/- 26.04\n",
      "\n",
      "[P3A_82] Top Features:\n",
      "--------------------------------------------------------------------------------\n",
      "  FFAPmxOI                                          :    12.53 +/- 18.54\n",
      "  Pacific Region Port Calls - Deep Sea Cargo Vessels:     9.16 +/- 9.61\n",
      "  Panamax Orderbook % Fleet                         :     3.49 +/- 0.91\n",
      "  PDIOPEX                                           :    -0.00 +/- 0.00\n",
      "  ODV_T                                             :    -8.91 +/- 6.69\n",
      "  FFADVPmx_T                                        :   -15.94 +/- 13.30\n",
      "  VLSFO                                             :   -21.08 +/- 12.95\n",
      "  BCI                                               :   -21.42 +/- 6.62\n",
      "  P3EA_Basis                                        :  -101.57 +/- 63.87\n",
      "  PDIC                                              :  -171.98 +/- 30.45\n",
      "\n",
      "================================================================================\n",
      "CRITICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "P1A Feature Quality:\n",
      "  Maximum importance: 6.10\n",
      "  Features with positive importance: 3 / 12\n",
      "  Mean importance (all features): -45.12\n",
      "\n",
      "P3A Feature Quality:\n",
      "  Maximum importance: 12.53\n",
      "  Features with positive importance: 3 / 12\n",
      "  Mean importance (all features): -73.17\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DIAGNOSIS:\n",
      "WARNING P1A features show weak predictive power\n",
      "   -> Consider revising feature engineering or adding new features\n",
      "\n",
      "P3A vs P1A importance ratio: 2.05x\n",
      "\n",
      "[SAVED] plots/permutation_importance.png\n",
      "\n",
      "================================================================================\n",
      "METHOD 2: XGBOOST BUILT-IN FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "[P1A_82] Extracting XGBoost importance scores...\n",
      "  OK Complete\n",
      "  Top feature (by gain): PDIOPEX (0.00)\n",
      "\n",
      "[P3A_82] Extracting XGBoost importance scores...\n",
      "  OK Complete\n",
      "  Top feature (by gain): C5TC (0.00)\n",
      "\n",
      "[P1A_82] Top Features (by Gain):\n",
      "--------------------------------------------------------------------------------\n",
      "  PDIOPEX                                           : Gain=    0.00\n",
      "  BCI                                               : Gain=    0.00\n",
      "  ODV_T                                             : Gain=    0.00\n",
      "  FFADVPmx_T                                        : Gain=    0.00\n",
      "  VLSFO                                             : Gain=    0.00\n",
      "  P1EA_Basis                                        : Gain=    0.00\n",
      "  P1EA_Slope                                        : Gain=    0.00\n",
      "  Panamax Bulkcarrier 65-100,000 dwt Atlantic Deploy: Gain=    0.00\n",
      "  FFAPmxOI                                          : Gain=    0.00\n",
      "  Panamax Orderbook % Fleet                         : Gain=    0.00\n",
      "\n",
      "[P3A_82] Top Features (by Gain):\n",
      "--------------------------------------------------------------------------------\n",
      "  C5TC                                              : Gain=    0.00\n",
      "  PDIOPEX                                           : Gain=    0.00\n",
      "  BCI                                               : Gain=    0.00\n",
      "  ODV_T                                             : Gain=    0.00\n",
      "  FFADVPmx_T                                        : Gain=    0.00\n",
      "  VLSFO                                             : Gain=    0.00\n",
      "  P3EA_Basis                                        : Gain=    0.00\n",
      "  P3EA_Slope                                        : Gain=    0.00\n",
      "  Pacific Region Port Calls - Deep Sea Cargo Vessels: Gain=    0.00\n",
      "  FFAPmxOI                                          : Gain=    0.00\n",
      "\n",
      "[SAVED] plots/xgboost_importance_gain.png\n",
      "\n",
      "================================================================================\n",
      "METHOD 3: SHAP VALUES (Shapley Additive Explanations)\n",
      "================================================================================\n",
      "Computing SHAP values (this may take several minutes)...\n",
      "\n",
      "[P1A_82] Computing SHAP values...\n",
      "  OK Complete\n",
      "  Top feature: FFAPmxOI (SHAP: 2461.26)\n",
      "\n",
      "[P3A_82] Computing SHAP values...\n",
      "  OK Complete\n",
      "  Top feature: PDIC (SHAP: 2783.52)\n",
      "[SAVED] plots/shap_summary_p1a.png\n",
      "[SAVED] plots/shap_summary_p3a.png\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "[SAVED] Permutation importance CSVs\n",
      "[SAVED] XGBoost importance CSVs\n",
      "[SAVED] SHAP importance CSVs\n",
      "[SAVED] diagnostics_summary.json\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTICS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "All results saved to: data/diagnostics/\n",
      "\n",
      "Key files:\n",
      "  - diagnostics_summary.json: Summary of findings\n",
      "  - p1a/p3a_*_importance.csv: Detailed importance scores\n",
      "  - plots/: Visualizations for all three methods\n",
      "\n",
      "================================================================================\n",
      "\n",
      "FINAL ANSWER TO YOUR QUESTION:\n",
      "  \"Features may need improvement\"\n",
      "\n",
      "  CONSIDER revising feature engineering.\n",
      "  Features show weak predictive power.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Diagnostics - Feature Importance Analysis\n",
    "================================================\n",
    "\n",
    "Purpose: Validate feature engineering quality by measuring predictive power\n",
    "         of features for both P1A (poor performance) and P3A (excellent).\n",
    "\n",
    "Key Question: Do P1A features have ANY predictive power, or is the\n",
    "              Atlantic market fundamentally unpredictable?\n",
    "\n",
    "Methods:\n",
    "1. Permutation Importance (model-agnostic)\n",
    "2. XGBoost Built-in Feature Importance (gain, cover, weight)\n",
    "3. SHAP Values (Shapley Additive Explanations)\n",
    "\n",
    "Expected Outcomes:\n",
    "- If P1A features show near-zero importance → Feature engineering needs work\n",
    "- If P1A features show positive importance → Market is unpredictable\n",
    "\n",
    "Author: Data Science Pipeline\n",
    "Date: 2025-10-17\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import shap\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "DATA_DIR = 'data/processed/'\n",
    "MODELS_DIR = 'data/models/xgboost_core/'\n",
    "OUTPUT_DIR = 'data/diagnostics/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f'{OUTPUT_DIR}plots/', exist_ok=True)\n",
    "\n",
    "HORIZON = 1  # Focus on h=1 for diagnostics\n",
    "N_REPEATS = 10  # Permutation importance repetitions\n",
    "RANDOM_STATE = 73\n",
    "TOP_N_FEATURES = 12  # Show all 12 CORE features\n",
    "\n",
    "print('='*80)\n",
    "print('MODEL DIAGNOSTICS: FEATURE IMPORTANCE ANALYSIS')\n",
    "print('='*80)\n",
    "print('Objective: Determine if poor P1A performance is due to:')\n",
    "print('  A) Poor feature engineering (features lack predictive power)')\n",
    "print('  B) Market unpredictability (features are good but market is chaotic)')\n",
    "print('='*80)\n",
    "print(f'\\nConfiguration:')\n",
    "print(f'  Data directory: {DATA_DIR}')\n",
    "print(f'  Models directory: {MODELS_DIR}')\n",
    "print(f'  Output directory: {OUTPUT_DIR}')\n",
    "print(f'  Analysis horizon: h={HORIZON}')\n",
    "print(f'  Permutation repeats: {N_REPEATS}')\n",
    "\n",
    "# ==============================================================================\n",
    "# LOAD DATA AND MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('LOADING DATA AND MODELS')\n",
    "print('='*80)\n",
    "\n",
    "# Load CORE features (12 features per route)\n",
    "p1a_train = pd.read_csv(f'{DATA_DIR}p1a_core_train.csv')\n",
    "p1a_val = pd.read_csv(f'{DATA_DIR}p1a_core_val.csv')\n",
    "p1a_test = pd.read_csv(f'{DATA_DIR}p1a_core_test.csv')\n",
    "\n",
    "p3a_train = pd.read_csv(f'{DATA_DIR}p3a_core_train.csv')\n",
    "p3a_val = pd.read_csv(f'{DATA_DIR}p3a_core_val.csv')\n",
    "p3a_test = pd.read_csv(f'{DATA_DIR}p3a_core_test.csv')\n",
    "\n",
    "# Load targets\n",
    "targets_train = pd.read_csv(f'{DATA_DIR}targets_train.csv')\n",
    "targets_val = pd.read_csv(f'{DATA_DIR}targets_val.csv')\n",
    "targets_test = pd.read_csv(f'{DATA_DIR}targets_test.csv')\n",
    "\n",
    "print(f'[LOADED] P1A: Train={p1a_train.shape}, Val={p1a_val.shape}, Test={p1a_test.shape}')\n",
    "print(f'[LOADED] P3A: Train={p3a_train.shape}, Val={p3a_val.shape}, Test={p3a_test.shape}')\n",
    "print(f'[LOADED] Targets: Train={targets_train.shape}, Val={targets_val.shape}, Test={targets_test.shape}')\n",
    "\n",
    "# Feature columns\n",
    "p1a_features = [c for c in p1a_train.columns if c != 'Date']\n",
    "p3a_features = [c for c in p3a_train.columns if c != 'Date']\n",
    "\n",
    "print(f'\\nFeature counts: P1A={len(p1a_features)}, P3A={len(p3a_features)}')\n",
    "\n",
    "# Load trained XGBoost models\n",
    "print('\\n[LOADING] XGBoost models...')\n",
    "p1a_model = xgb.XGBRegressor()\n",
    "p1a_model.load_model(f'{MODELS_DIR}P1A_h{HORIZON}_core_model.json')\n",
    "print(f'  OK P1A model loaded')\n",
    "\n",
    "p3a_model = xgb.XGBRegressor()\n",
    "p3a_model.load_model(f'{MODELS_DIR}P3A_h{HORIZON}_core_model.json')\n",
    "print(f'  OK P3A model loaded')\n",
    "\n",
    "# ==============================================================================\n",
    "# PREPARE CLEAN DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('PREPARING CLEAN DATASETS')\n",
    "print('='*80)\n",
    "\n",
    "def prepare_clean_data(X_train, X_val, y_train, y_val, feature_names, route_name):\n",
    "    \"\"\"Prepare clean dataset by removing NaN values.\"\"\"\n",
    "    train_mask = ~y_train.isna()\n",
    "    val_mask = ~y_val.isna()\n",
    "\n",
    "    X_train_clean = X_train[feature_names][train_mask].reset_index(drop=True)\n",
    "    y_train_clean = y_train[train_mask].reset_index(drop=True)\n",
    "\n",
    "    X_val_clean = X_val[feature_names][val_mask].reset_index(drop=True)\n",
    "    y_val_clean = y_val[val_mask].reset_index(drop=True)\n",
    "\n",
    "    # Combine for robust importance estimation\n",
    "    X_combined = pd.concat([X_train_clean, X_val_clean], axis=0, ignore_index=True)\n",
    "    y_combined = pd.concat([y_train_clean, y_val_clean], axis=0, ignore_index=True)\n",
    "\n",
    "    print(f'{route_name}:')\n",
    "    print(f'  Train samples: {len(y_train_clean)}')\n",
    "    print(f'  Val samples:   {len(y_val_clean)}')\n",
    "    print(f'  Combined:      {len(y_combined)} (used for importance analysis)')\n",
    "\n",
    "    return X_combined, y_combined, X_val_clean, y_val_clean\n",
    "\n",
    "# P1A data\n",
    "X_p1a, y_p1a, X_p1a_val, y_p1a_val = prepare_clean_data(\n",
    "    p1a_train, p1a_val,\n",
    "    targets_train[f'P1A_82_h{HORIZON}'],\n",
    "    targets_val[f'P1A_82_h{HORIZON}'],\n",
    "    p1a_features, 'P1A_82'\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# P3A data\n",
    "X_p3a, y_p3a, X_p3a_val, y_p3a_val = prepare_clean_data(\n",
    "    p3a_train, p3a_val,\n",
    "    targets_train[f'P3A_82_h{HORIZON}'],\n",
    "    targets_val[f'P3A_82_h{HORIZON}'],\n",
    "    p3a_features, 'P3A_82'\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# BASELINE MODEL PERFORMANCE\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('BASELINE MODEL PERFORMANCE (Validation Set)')\n",
    "print('='*80)\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, route_name):\n",
    "    \"\"\"Evaluate model performance on validation set.\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f'\\n{route_name}:')\n",
    "    print(f'  RMSE: ${rmse:,.2f}')\n",
    "    print(f'  MAE:  ${mae:,.2f}')\n",
    "    print(f'  R2:   {r2:.4f}')\n",
    "    if r2 < 0:\n",
    "        print(f'  WARNING: Negative R2 indicates predictions worse than mean baseline')\n",
    "\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "p1a_perf = evaluate_model(p1a_model, X_p1a_val, y_p1a_val, 'P1A_82')\n",
    "p3a_perf = evaluate_model(p3a_model, X_p3a_val, y_p3a_val, 'P3A_82')\n",
    "\n",
    "# ==============================================================================\n",
    "# METHOD 1: PERMUTATION IMPORTANCE\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('METHOD 1: PERMUTATION IMPORTANCE')\n",
    "print('='*80)\n",
    "print(f'Computing permutation importance ({N_REPEATS} repeats per feature)...')\n",
    "print('This may take a few minutes...')\n",
    "\n",
    "# P1A Permutation Importance\n",
    "print('\\n[P1A_82] Computing permutation importance...')\n",
    "p1a_perm = permutation_importance(\n",
    "    p1a_model, X_p1a_val, y_p1a_val,\n",
    "    n_repeats=N_REPEATS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "p1a_perm_df = pd.DataFrame({\n",
    "    'feature': p1a_features,\n",
    "    'importance_mean': -p1a_perm.importances_mean,  # Negate because neg_rmse\n",
    "    'importance_std': p1a_perm.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature: {p1a_perm_df.iloc[0][\"feature\"]} (importance: {p1a_perm_df.iloc[0][\"importance_mean\"]:.2f})')\n",
    "print(f'  Positive importance features: {(p1a_perm_df[\"importance_mean\"] > 0).sum()} / {len(p1a_features)}')\n",
    "\n",
    "# P3A Permutation Importance\n",
    "print('\\n[P3A_82] Computing permutation importance...')\n",
    "p3a_perm = permutation_importance(\n",
    "    p3a_model, X_p3a_val, y_p3a_val,\n",
    "    n_repeats=N_REPEATS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "p3a_perm_df = pd.DataFrame({\n",
    "    'feature': p3a_features,\n",
    "    'importance_mean': -p3a_perm.importances_mean,\n",
    "    'importance_std': p3a_perm.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature: {p3a_perm_df.iloc[0][\"feature\"]} (importance: {p3a_perm_df.iloc[0][\"importance_mean\"]:.2f})')\n",
    "print(f'  Positive importance features: {(p3a_perm_df[\"importance_mean\"] > 0).sum()} / {len(p3a_features)}')\n",
    "\n",
    "# Display results\n",
    "print('\\n' + '='*80)\n",
    "print('PERMUTATION IMPORTANCE RESULTS')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n[P1A_82] Top Features:')\n",
    "print('-'*80)\n",
    "for i, row in p1a_perm_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature'][:50]:50s}: {row['importance_mean']:>8.2f} +/- {row['importance_std']:.2f}\")\n",
    "\n",
    "print('\\n[P3A_82] Top Features:')\n",
    "print('-'*80)\n",
    "for i, row in p3a_perm_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature'][:50]:50s}: {row['importance_mean']:>8.2f} +/- {row['importance_std']:.2f}\")\n",
    "\n",
    "# Critical analysis\n",
    "print('\\n' + '='*80)\n",
    "print('CRITICAL ANALYSIS')\n",
    "print('='*80)\n",
    "\n",
    "p1a_max_importance = p1a_perm_df['importance_mean'].max()\n",
    "p3a_max_importance = p3a_perm_df['importance_mean'].max()\n",
    "p1a_positive_count = (p1a_perm_df['importance_mean'] > 0).sum()\n",
    "p3a_positive_count = (p3a_perm_df['importance_mean'] > 0).sum()\n",
    "\n",
    "print(f'\\nP1A Feature Quality:')\n",
    "print(f'  Maximum importance: {p1a_max_importance:.2f}')\n",
    "print(f'  Features with positive importance: {p1a_positive_count} / {len(p1a_features)}')\n",
    "print(f'  Mean importance (all features): {p1a_perm_df[\"importance_mean\"].mean():.2f}')\n",
    "\n",
    "print(f'\\nP3A Feature Quality:')\n",
    "print(f'  Maximum importance: {p3a_max_importance:.2f}')\n",
    "print(f'  Features with positive importance: {p3a_positive_count} / {len(p3a_features)}')\n",
    "print(f'  Mean importance (all features): {p3a_perm_df[\"importance_mean\"].mean():.2f}')\n",
    "\n",
    "print('\\n' + '-'*80)\n",
    "print('DIAGNOSIS:')\n",
    "if p1a_max_importance > 50:\n",
    "    print('OK P1A features DO have predictive power')\n",
    "    print('   -> Poor performance is due to MARKET UNPREDICTABILITY, not feature quality')\n",
    "    diagnosis = 'Features are good, market is unpredictable'\n",
    "else:\n",
    "    print('WARNING P1A features show weak predictive power')\n",
    "    print('   -> Consider revising feature engineering or adding new features')\n",
    "    diagnosis = 'Features may need improvement'\n",
    "\n",
    "importance_ratio = p3a_max_importance / p1a_max_importance if p1a_max_importance > 0 else float('inf')\n",
    "print(f'\\nP3A vs P1A importance ratio: {importance_ratio:.2f}x')\n",
    "\n",
    "# Plot permutation importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# P1A\n",
    "top_p1a = p1a_perm_df.head(TOP_N_FEATURES)\n",
    "axes[0].barh(range(len(top_p1a)), top_p1a['importance_mean'],\n",
    "             xerr=top_p1a['importance_std'], alpha=0.7, color='darkred')\n",
    "axes[0].set_yticks(range(len(top_p1a)))\n",
    "axes[0].set_yticklabels(top_p1a['feature'], fontsize=9)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance (RMSE increase when permuted)', fontsize=11)\n",
    "axes[0].set_title(f'P1A_82 - Permutation Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# P3A\n",
    "top_p3a = p3a_perm_df.head(TOP_N_FEATURES)\n",
    "axes[1].barh(range(len(top_p3a)), top_p3a['importance_mean'],\n",
    "             xerr=top_p3a['importance_std'], alpha=0.7, color='darkblue')\n",
    "axes[1].set_yticks(range(len(top_p3a)))\n",
    "axes[1].set_yticklabels(top_p3a['feature'], fontsize=9)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Importance (RMSE increase when permuted)', fontsize=11)\n",
    "axes[1].set_title(f'P3A_82 - Permutation Importance', fontsize=12, fontweight='bold')\n",
    "axes[1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}plots/permutation_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'\\n[SAVED] plots/permutation_importance.png')\n",
    "\n",
    "# ==============================================================================\n",
    "# METHOD 2: XGBOOST FEATURE IMPORTANCE\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('METHOD 2: XGBOOST BUILT-IN FEATURE IMPORTANCE')\n",
    "print('='*80)\n",
    "\n",
    "def get_xgb_importance(model, feature_names):\n",
    "    \"\"\"Extract XGBoost importance scores.\"\"\"\n",
    "    gain = model.get_booster().get_score(importance_type='gain')\n",
    "    cover = model.get_booster().get_score(importance_type='cover')\n",
    "    weight = model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'gain': [gain.get(f'f{i}', 0) for i in range(len(feature_names))],\n",
    "        'cover': [cover.get(f'f{i}', 0) for i in range(len(feature_names))],\n",
    "        'weight': [weight.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "    })\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Extract importance\n",
    "print('\\n[P1A_82] Extracting XGBoost importance scores...')\n",
    "p1a_xgb_imp = get_xgb_importance(p1a_model, p1a_features)\n",
    "p1a_xgb_imp_gain = p1a_xgb_imp.sort_values('gain', ascending=False)\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature (by gain): {p1a_xgb_imp_gain.iloc[0][\"feature\"]} ({p1a_xgb_imp_gain.iloc[0][\"gain\"]:.2f})')\n",
    "\n",
    "print('\\n[P3A_82] Extracting XGBoost importance scores...')\n",
    "p3a_xgb_imp = get_xgb_importance(p3a_model, p3a_features)\n",
    "p3a_xgb_imp_gain = p3a_xgb_imp.sort_values('gain', ascending=False)\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature (by gain): {p3a_xgb_imp_gain.iloc[0][\"feature\"]} ({p3a_xgb_imp_gain.iloc[0][\"gain\"]:.2f})')\n",
    "\n",
    "# Display\n",
    "print('\\n[P1A_82] Top Features (by Gain):')\n",
    "print('-'*80)\n",
    "for i, row in p1a_xgb_imp_gain.head(10).iterrows():\n",
    "    print(f\"  {row['feature'][:50]:50s}: Gain={row['gain']:>8.2f}\")\n",
    "\n",
    "print('\\n[P3A_82] Top Features (by Gain):')\n",
    "print('-'*80)\n",
    "for i, row in p3a_xgb_imp_gain.head(10).iterrows():\n",
    "    print(f\"  {row['feature'][:50]:50s}: Gain={row['gain']:>8.2f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# P1A\n",
    "top_p1a_xgb = p1a_xgb_imp_gain.head(TOP_N_FEATURES)\n",
    "axes[0].barh(range(len(top_p1a_xgb)), top_p1a_xgb['gain'], alpha=0.7, color='darkred')\n",
    "axes[0].set_yticks(range(len(top_p1a_xgb)))\n",
    "axes[0].set_yticklabels(top_p1a_xgb['feature'], fontsize=9)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance (Gain)', fontsize=11)\n",
    "axes[0].set_title(f'P1A_82 - XGBoost Gain', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# P3A\n",
    "top_p3a_xgb = p3a_xgb_imp_gain.head(TOP_N_FEATURES)\n",
    "axes[1].barh(range(len(top_p3a_xgb)), top_p3a_xgb['gain'], alpha=0.7, color='darkblue')\n",
    "axes[1].set_yticks(range(len(top_p3a_xgb)))\n",
    "axes[1].set_yticklabels(top_p3a_xgb['feature'], fontsize=9)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Importance (Gain)', fontsize=11)\n",
    "axes[1].set_title(f'P3A_82 - XGBoost Gain', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}plots/xgboost_importance_gain.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'\\n[SAVED] plots/xgboost_importance_gain.png')\n",
    "\n",
    "# ==============================================================================\n",
    "# METHOD 3: SHAP VALUES\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('METHOD 3: SHAP VALUES (Shapley Additive Explanations)')\n",
    "print('='*80)\n",
    "print('Computing SHAP values (this may take several minutes)...')\n",
    "\n",
    "# P1A SHAP\n",
    "print('\\n[P1A_82] Computing SHAP values...')\n",
    "p1a_explainer = shap.TreeExplainer(p1a_model)\n",
    "p1a_shap_values = p1a_explainer.shap_values(X_p1a_val)\n",
    "p1a_shap_importance = pd.DataFrame({\n",
    "    'feature': p1a_features,\n",
    "    'shap_importance': np.abs(p1a_shap_values).mean(axis=0)\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature: {p1a_shap_importance.iloc[0][\"feature\"]} (SHAP: {p1a_shap_importance.iloc[0][\"shap_importance\"]:.2f})')\n",
    "\n",
    "# P3A SHAP\n",
    "print('\\n[P3A_82] Computing SHAP values...')\n",
    "p3a_explainer = shap.TreeExplainer(p3a_model)\n",
    "p3a_shap_values = p3a_explainer.shap_values(X_p3a_val)\n",
    "p3a_shap_importance = pd.DataFrame({\n",
    "    'feature': p3a_features,\n",
    "    'shap_importance': np.abs(p3a_shap_values).mean(axis=0)\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "print(f'  OK Complete')\n",
    "print(f'  Top feature: {p3a_shap_importance.iloc[0][\"feature\"]} (SHAP: {p3a_shap_importance.iloc[0][\"shap_importance\"]:.2f})')\n",
    "\n",
    "# SHAP summary plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(p1a_shap_values, X_p1a_val, feature_names=p1a_features,\n",
    "                  max_display=TOP_N_FEATURES, show=False)\n",
    "plt.title(f'P1A_82 - SHAP Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}plots/shap_summary_p1a.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'[SAVED] plots/shap_summary_p1a.png')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(p3a_shap_values, X_p3a_val, feature_names=p3a_features,\n",
    "                  max_display=TOP_N_FEATURES, show=False)\n",
    "plt.title(f'P3A_82 - SHAP Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}plots/shap_summary_p3a.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'[SAVED] plots/shap_summary_p3a.png')\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('SAVING RESULTS')\n",
    "print('='*80)\n",
    "\n",
    "# Save importance tables\n",
    "p1a_perm_df.to_csv(f'{OUTPUT_DIR}p1a_permutation_importance.csv', index=False)\n",
    "p3a_perm_df.to_csv(f'{OUTPUT_DIR}p3a_permutation_importance.csv', index=False)\n",
    "print(f'[SAVED] Permutation importance CSVs')\n",
    "\n",
    "p1a_xgb_imp_gain.to_csv(f'{OUTPUT_DIR}p1a_xgboost_importance.csv', index=False)\n",
    "p3a_xgb_imp_gain.to_csv(f'{OUTPUT_DIR}p3a_xgboost_importance.csv', index=False)\n",
    "print(f'[SAVED] XGBoost importance CSVs')\n",
    "\n",
    "p1a_shap_importance.to_csv(f'{OUTPUT_DIR}p1a_shap_importance.csv', index=False)\n",
    "p3a_shap_importance.to_csv(f'{OUTPUT_DIR}p3a_shap_importance.csv', index=False)\n",
    "print(f'[SAVED] SHAP importance CSVs')\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'horizon': HORIZON,\n",
    "    'p1a_performance': {\n",
    "        'rmse': float(p1a_perf['rmse']),\n",
    "        'mae': float(p1a_perf['mae']),\n",
    "        'r2': float(p1a_perf['r2'])\n",
    "    },\n",
    "    'p3a_performance': {\n",
    "        'rmse': float(p3a_perf['rmse']),\n",
    "        'mae': float(p3a_perf['mae']),\n",
    "        'r2': float(p3a_perf['r2'])\n",
    "    },\n",
    "    'p1a_max_importance': {\n",
    "        'permutation': float(p1a_max_importance),\n",
    "        'xgboost_gain': float(p1a_xgb_imp_gain['gain'].max()),\n",
    "        'shap': float(p1a_shap_importance['shap_importance'].max())\n",
    "    },\n",
    "    'p3a_max_importance': {\n",
    "        'permutation': float(p3a_max_importance),\n",
    "        'xgboost_gain': float(p3a_xgb_imp_gain['gain'].max()),\n",
    "        'shap': float(p3a_shap_importance['shap_importance'].max())\n",
    "    },\n",
    "    'diagnosis': diagnosis,\n",
    "    'importance_ratio_p3a_vs_p1a': float(importance_ratio)\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}diagnostics_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f'[SAVED] diagnostics_summary.json')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('DIAGNOSTICS COMPLETE!')\n",
    "print('='*80)\n",
    "print(f'\\nAll results saved to: {OUTPUT_DIR}')\n",
    "print('\\nKey files:')\n",
    "print('  - diagnostics_summary.json: Summary of findings')\n",
    "print('  - p1a/p3a_*_importance.csv: Detailed importance scores')\n",
    "print('  - plots/: Visualizations for all three methods')\n",
    "print('\\n' + '='*80)\n",
    "print('\\nFINAL ANSWER TO YOUR QUESTION:')\n",
    "print(f'  \"{diagnosis}\"')\n",
    "if p1a_max_importance > 50:\n",
    "    print('\\n  DO NOT change feature engineering.')\n",
    "    print('  The features are working. The Atlantic market is simply unpredictable.')\n",
    "else:\n",
    "    print('\\n  CONSIDER revising feature engineering.')\n",
    "    print('  Features show weak predictive power.')\n",
    "print('='*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
