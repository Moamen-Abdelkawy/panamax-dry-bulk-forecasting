{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54114a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBOOST TRAINING: CORE FEATURES (12 features per route)\n",
      "================================================================================\n",
      "Strategy: Reduce feature set to minimize overfitting\n",
      "Horizons: [1, 5, 10, 20] business days\n",
      "Hyperparameter search: 50 iterations\n",
      "================================================================================\n",
      "\n",
      "Loading CORE feature datasets...\n",
      "--------------------------------------------------------------------------------\n",
      "[LOADED] P1A CORE: Train=(705, 13), Val=(125, 13), Test=(326, 13)\n",
      "[LOADED] P3A CORE: Train=(705, 13), Val=(125, 13), Test=(326, 13)\n",
      "[LOADED] Targets: Train=(705, 11), Val=(125, 11), Test=(326, 11)\n",
      "\n",
      "Feature counts:\n",
      "  P1A (Atlantic): 12 features\n",
      "  P3A (Pacific):  12 features\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING PHASE: ALL ROUTES AND HORIZONS\n",
      "================================================================================\n",
      "\n",
      "--- P1A_82 (ATLANTIC) ---\n",
      "\n",
      "P1A_82 - Horizon 1 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 705\n",
      "Validation samples: 125\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $2,695.38\n",
      "  Iteration 20/50: Best RMSE = $2,674.91\n",
      "  Iteration 30/50: Best RMSE = $2,623.17\n",
      "  Iteration 40/50: Best RMSE = $2,554.85\n",
      "  Iteration 50/50: Best RMSE = $2,554.85\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 5\n",
      "  learning_rate       : 0.0578\n",
      "  n_estimators        : 794\n",
      "  min_child_weight    : 3\n",
      "  gamma               : 0.4424\n",
      "  subsample           : 0.7788\n",
      "  colsample_bytree    : 0.9781\n",
      "  reg_alpha           : 0.9555\n",
      "  reg_lambda          : 1.4634\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $2,554.85\n",
      "  MAE:  $2,086.38\n",
      "  R²:   -0.3474\n",
      "\n",
      "P1A_82 - Horizon 5 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 701\n",
      "Validation samples: 121\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $2,573.54\n",
      "  Iteration 20/50: Best RMSE = $2,485.89\n",
      "  Iteration 30/50: Best RMSE = $2,485.89\n",
      "  Iteration 40/50: Best RMSE = $2,485.89\n",
      "  Iteration 50/50: Best RMSE = $2,485.89\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 6\n",
      "  learning_rate       : 0.2279\n",
      "  n_estimators        : 400\n",
      "  min_child_weight    : 6\n",
      "  gamma               : 0.1942\n",
      "  subsample           : 0.7695\n",
      "  colsample_bytree    : 0.9343\n",
      "  reg_alpha           : 1.8305\n",
      "  reg_lambda          : 1.4540\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $2,485.89\n",
      "  MAE:  $1,945.17\n",
      "  R²:   -0.4202\n",
      "\n",
      "P1A_82 - Horizon 10 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 696\n",
      "Validation samples: 116\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $2,857.07\n",
      "  Iteration 20/50: Best RMSE = $2,857.07\n",
      "  Iteration 30/50: Best RMSE = $2,762.28\n",
      "  Iteration 40/50: Best RMSE = $2,071.61\n",
      "  Iteration 50/50: Best RMSE = $2,071.61\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 3\n",
      "  learning_rate       : 0.2490\n",
      "  n_estimators        : 682\n",
      "  min_child_weight    : 5\n",
      "  gamma               : 0.0413\n",
      "  subsample           : 0.8521\n",
      "  colsample_bytree    : 0.9978\n",
      "  reg_alpha           : 1.8088\n",
      "  reg_lambda          : 0.5939\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $2,071.61\n",
      "  MAE:  $1,618.42\n",
      "  R²:   0.0430\n",
      "\n",
      "P1A_82 - Horizon 20 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 686\n",
      "Validation samples: 106\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $8,517.86\n",
      "  Iteration 20/50: Best RMSE = $7,243.73\n",
      "  Iteration 30/50: Best RMSE = $6,378.62\n",
      "  Iteration 40/50: Best RMSE = $5,747.03\n",
      "  Iteration 50/50: Best RMSE = $5,747.03\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 3\n",
      "  learning_rate       : 0.2490\n",
      "  n_estimators        : 682\n",
      "  min_child_weight    : 5\n",
      "  gamma               : 0.0413\n",
      "  subsample           : 0.8521\n",
      "  colsample_bytree    : 0.9978\n",
      "  reg_alpha           : 1.8088\n",
      "  reg_lambda          : 0.5939\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $5,747.03\n",
      "  MAE:  $5,294.96\n",
      "  R²:   -5.9445\n",
      "\n",
      "--- P3A_82 (PACIFIC) ---\n",
      "\n",
      "P3A_82 - Horizon 1 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 705\n",
      "Validation samples: 125\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $2,277.37\n",
      "  Iteration 20/50: Best RMSE = $2,104.67\n",
      "  Iteration 30/50: Best RMSE = $2,104.67\n",
      "  Iteration 40/50: Best RMSE = $2,104.67\n",
      "  Iteration 50/50: Best RMSE = $2,104.67\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 3\n",
      "  learning_rate       : 0.1363\n",
      "  n_estimators        : 281\n",
      "  min_child_weight    : 5\n",
      "  gamma               : 0.0290\n",
      "  subsample           : 0.6700\n",
      "  colsample_bytree    : 0.7304\n",
      "  reg_alpha           : 0.9105\n",
      "  reg_lambda          : 1.2757\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $2,104.67\n",
      "  MAE:  $1,800.18\n",
      "  R²:   0.3141\n",
      "\n",
      "P3A_82 - Horizon 5 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 701\n",
      "Validation samples: 121\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $1,749.69\n",
      "  Iteration 20/50: Best RMSE = $1,749.69\n",
      "  Iteration 30/50: Best RMSE = $1,749.69\n",
      "  Iteration 40/50: Best RMSE = $1,749.69\n",
      "  Iteration 50/50: Best RMSE = $1,749.69\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 3\n",
      "  learning_rate       : 0.2226\n",
      "  n_estimators        : 898\n",
      "  min_child_weight    : 1\n",
      "  gamma               : 0.0590\n",
      "  subsample           : 0.6168\n",
      "  colsample_bytree    : 0.7275\n",
      "  reg_alpha           : 1.6775\n",
      "  reg_lambda          : 1.8236\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $1,749.69\n",
      "  MAE:  $1,399.73\n",
      "  R²:   0.5311\n",
      "\n",
      "P3A_82 - Horizon 10 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 696\n",
      "Validation samples: 116\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $1,556.13\n",
      "  Iteration 20/50: Best RMSE = $1,547.33\n",
      "  Iteration 30/50: Best RMSE = $1,547.33\n",
      "  Iteration 40/50: Best RMSE = $1,488.78\n",
      "  Iteration 50/50: Best RMSE = $1,488.78\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 5\n",
      "  learning_rate       : 0.2414\n",
      "  n_estimators        : 216\n",
      "  min_child_weight    : 8\n",
      "  gamma               : 0.4564\n",
      "  subsample           : 0.9417\n",
      "  colsample_bytree    : 0.9871\n",
      "  reg_alpha           : 1.2008\n",
      "  reg_lambda          : 2.3384\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $1,488.78\n",
      "  MAE:  $1,164.35\n",
      "  R²:   0.6318\n",
      "\n",
      "P3A_82 - Horizon 20 - Training XGBoost (CORE features)...\n",
      "--------------------------------------------------------------------------------\n",
      "Training samples: 686\n",
      "Validation samples: 106\n",
      "Features: 12\n",
      "\n",
      "Performing hyperparameter search (50 iterations)...\n",
      "  Iteration 10/50: Best RMSE = $2,262.35\n",
      "  Iteration 20/50: Best RMSE = $2,181.32\n",
      "  Iteration 30/50: Best RMSE = $2,181.32\n",
      "  Iteration 40/50: Best RMSE = $2,181.32\n",
      "  Iteration 50/50: Best RMSE = $2,117.47\n",
      "\n",
      "Best hyperparameters:\n",
      "  max_depth           : 4\n",
      "  learning_rate       : 0.0123\n",
      "  n_estimators        : 113\n",
      "  min_child_weight    : 1\n",
      "  gamma               : 0.0535\n",
      "  subsample           : 0.6643\n",
      "  colsample_bytree    : 0.9952\n",
      "  reg_alpha           : 1.5468\n",
      "  reg_lambda          : 0.5702\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: $2,117.47\n",
      "  MAE:  $1,733.89\n",
      "  R²:   0.0648\n",
      "\n",
      "================================================================================\n",
      "All 8 models trained successfully!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VALIDATION PERFORMANCE SUMMARY (CORE FEATURES)\n",
      "================================================================================\n",
      " route  horizon    val_rmse     val_mae    val_r2\n",
      "P1A_82        1 2554.847815 2086.375648 -0.347359\n",
      "P1A_82        5 2485.890871 1945.170455 -0.420207\n",
      "P1A_82       10 2071.605518 1618.416748  0.042977\n",
      "P1A_82       20 5747.034685 5294.957796 -5.944507\n",
      "P3A_82        1 2104.671316 1800.175031  0.314136\n",
      "P3A_82        5 1749.692006 1399.734197  0.531131\n",
      "P3A_82       10 1488.782863 1164.349256  0.631800\n",
      "P3A_82       20 2117.472720 1733.890450  0.064789\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Performance by Route (averaged across horizons):\n",
      "--------------------------------------------------------------------------------\n",
      "           val_rmse      val_mae    val_r2\n",
      "route                                     \n",
      "P1A_82  3214.844722  2736.230162 -1.667274\n",
      "P3A_82  1865.154726  1524.537234  0.385464\n",
      "\n",
      "================================================================================\n",
      "BASELINE COMPARISON (h=1 forecast)\n",
      "================================================================================\n",
      "\n",
      "Model Performance Comparison (RMSE):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "P1A_82:\n",
      "  XGBoost (CORE 12 feat)        : RMSE $  2,554.85  |  MAE $  2,086.38\n",
      "  XGBoost (ML 59 feat)          : RMSE $  4,284.31  |  MAE $  3,672.78\n",
      "  SARIMAX                       : RMSE $  7,573.66  |  MAE $  7,113.69\n",
      "\n",
      "  → XGBoost CORE vs SARIMAX: +66.3% improvement\n",
      "\n",
      "P3A_82:\n",
      "  XGBoost (ML 61 feat)          : RMSE $  1,098.56  |  MAE $    920.90\n",
      "  XGBoost (CORE 12 feat)        : RMSE $  2,104.67  |  MAE $  1,800.18\n",
      "  SARIMAX                       : RMSE $  2,477.18  |  MAE $  2,082.62\n",
      "\n",
      "  → XGBoost CORE vs SARIMAX: +15.0% improvement\n",
      "\n",
      "================================================================================\n",
      "SAVING MODELS AND RESULTS\n",
      "================================================================================\n",
      "[SAVED] P1A_h1_core_model.json\n",
      "[SAVED] P1A_h5_core_model.json\n",
      "[SAVED] P1A_h10_core_model.json\n",
      "[SAVED] P1A_h20_core_model.json\n",
      "[SAVED] P3A_h1_core_model.json\n",
      "[SAVED] P3A_h5_core_model.json\n",
      "[SAVED] P3A_h10_core_model.json\n",
      "[SAVED] P3A_h20_core_model.json\n",
      "[SAVED] xgboost_core_results.csv\n",
      "[SAVED] model_comparison_with_baselines.csv\n",
      "[SAVED] metadata.json\n",
      "\n",
      "================================================================================\n",
      "SCRIPT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Outputs saved to: data/models/xgboost_core/\n",
      "\n",
      "Next steps:\n",
      "  1. Review model_comparison_with_baselines.csv\n",
      "  2. Check if P1A R² improved (should be positive now)\n",
      "  3. If P1A still negative R², try feature selection (Script 08)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "XGBoost Training on CORE Features (12 features per route)\n",
    "==========================================================\n",
    "\n",
    "Purpose: Train XGBoost models using only the 12 CORE features instead of\n",
    "         the full 59-61 ML feature set. This reduces overfitting risk and\n",
    "         may improve P1A performance.\n",
    "\n",
    "Strategy:\n",
    "- Use p1a_core_train.csv (12 features) instead of p1a_ml_train.csv (59 features)\n",
    "- Same temporal validation as Notebook 06\n",
    "- Manual hyperparameter search (50 iterations)\n",
    "- Compare results to original XGBoost (ML features) and SARIMAX baseline\n",
    "\n",
    "Author: Data Science Pipeline\n",
    "Date: 2025-10-17\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "DATA_DIR = 'data/processed/'\n",
    "OUTPUT_DIR = 'data/models/xgboost_core/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "HORIZONS = [1, 5, 10, 20]\n",
    "N_ITER = 50\n",
    "RANDOM_STATE = 73\n",
    "\n",
    "print('='*80)\n",
    "print('XGBOOST TRAINING: CORE FEATURES (12 features per route)')\n",
    "print('='*80)\n",
    "print(f'Strategy: Reduce feature set to minimize overfitting')\n",
    "print(f'Horizons: {HORIZONS} business days')\n",
    "print(f'Hyperparameter search: {N_ITER} iterations')\n",
    "print('='*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\nLoading CORE feature datasets...')\n",
    "print('-'*80)\n",
    "\n",
    "# Load P1A CORE features (12 features)\n",
    "p1a_train = pd.read_csv(f'{DATA_DIR}p1a_core_train.csv')\n",
    "p1a_val = pd.read_csv(f'{DATA_DIR}p1a_core_val.csv')\n",
    "p1a_test = pd.read_csv(f'{DATA_DIR}p1a_core_test.csv')\n",
    "print(f'[LOADED] P1A CORE: Train={p1a_train.shape}, Val={p1a_val.shape}, Test={p1a_test.shape}')\n",
    "\n",
    "# Load P3A CORE features (12 features)\n",
    "p3a_train = pd.read_csv(f'{DATA_DIR}p3a_core_train.csv')\n",
    "p3a_val = pd.read_csv(f'{DATA_DIR}p3a_core_val.csv')\n",
    "p3a_test = pd.read_csv(f'{DATA_DIR}p3a_core_test.csv')\n",
    "print(f'[LOADED] P3A CORE: Train={p3a_train.shape}, Val={p3a_val.shape}, Test={p3a_test.shape}')\n",
    "\n",
    "# Load targets (multi-horizon)\n",
    "targets_train = pd.read_csv(f'{DATA_DIR}targets_train.csv')\n",
    "targets_val = pd.read_csv(f'{DATA_DIR}targets_val.csv')\n",
    "targets_test = pd.read_csv(f'{DATA_DIR}targets_test.csv')\n",
    "print(f'[LOADED] Targets: Train={targets_train.shape}, Val={targets_val.shape}, Test={targets_test.shape}')\n",
    "\n",
    "# Feature columns (exclude Date)\n",
    "p1a_features = [c for c in p1a_train.columns if c != 'Date']\n",
    "p3a_features = [c for c in p3a_train.columns if c != 'Date']\n",
    "\n",
    "print(f'\\nFeature counts:')\n",
    "print(f'  P1A (Atlantic): {len(p1a_features)} features')\n",
    "print(f'  P3A (Pacific):  {len(p3a_features)} features')\n",
    "print('='*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def train_xgboost_core(X_train, y_train, X_val, y_val, route_name, horizon, n_iter=50):\n",
    "    \"\"\"\n",
    "    Train XGBoost model on CORE features with manual hyperparameter search.\n",
    "\n",
    "    Returns:\n",
    "    - best_model: Trained XGBoost model\n",
    "    - best_params: Best hyperparameters\n",
    "    - val_metrics: Dict with RMSE, MAE, R²\n",
    "    \"\"\"\n",
    "    print(f'\\n{route_name} - Horizon {horizon} - Training XGBoost (CORE features)...')\n",
    "    print('-'*80)\n",
    "\n",
    "    # Remove NaN values\n",
    "    train_mask = ~y_train.isna()\n",
    "    val_mask = ~y_val.isna()\n",
    "\n",
    "    X_train_clean = X_train[train_mask].reset_index(drop=True)\n",
    "    y_train_clean = y_train[train_mask].reset_index(drop=True)\n",
    "    X_val_clean = X_val[val_mask].reset_index(drop=True)\n",
    "    y_val_clean = y_val[val_mask].reset_index(drop=True)\n",
    "\n",
    "    print(f'Training samples: {len(y_train_clean)}')\n",
    "    print(f'Validation samples: {len(y_val_clean)}')\n",
    "    print(f'Features: {X_train_clean.shape[1]}')\n",
    "\n",
    "    # Manual hyperparameter search\n",
    "    print(f'\\nPerforming hyperparameter search ({n_iter} iterations)...')\n",
    "\n",
    "    best_val_rmse = np.inf\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Sample random hyperparameters\n",
    "        params = {\n",
    "            'max_depth': np.random.randint(3, 8),  # Reduced from 10 (less overfitting)\n",
    "            'learning_rate': np.random.uniform(0.01, 0.30),\n",
    "            'n_estimators': np.random.randint(100, 1000),\n",
    "            'min_child_weight': np.random.randint(1, 10),\n",
    "            'gamma': np.random.uniform(0, 0.5),\n",
    "            'subsample': np.random.uniform(0.6, 1.0),\n",
    "            'colsample_bytree': np.random.uniform(0.6, 1.0),\n",
    "            'reg_alpha': np.random.uniform(0, 2),  # Increased regularization\n",
    "            'reg_lambda': np.random.uniform(0, 3),  # Increased regularization\n",
    "            'objective': 'reg:squarederror',\n",
    "            'tree_method': 'hist',\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "        }\n",
    "\n",
    "        # Train model on TRAINING data only\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_clean,\n",
    "            y_train_clean,\n",
    "            eval_set=[(X_val_clean, y_val_clean)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Evaluate on VALIDATION data\n",
    "        y_val_pred = model.predict(X_val_clean)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_clean, y_val_pred))\n",
    "\n",
    "        # Track best model\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_model = model\n",
    "            best_params = params.copy()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'  Iteration {i+1}/{n_iter}: Best RMSE = ${best_val_rmse:,.2f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    y_val_pred = best_model.predict(X_val_clean)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val_clean, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val_clean, y_val_pred)\n",
    "    val_r2 = r2_score(y_val_clean, y_val_pred)\n",
    "\n",
    "    print(f'\\nBest hyperparameters:')\n",
    "    for param in ['max_depth', 'learning_rate', 'n_estimators', 'min_child_weight',\n",
    "                  'gamma', 'subsample', 'colsample_bytree', 'reg_alpha', 'reg_lambda']:\n",
    "        print(f'  {param:20s}: {best_params[param]:.4f}' if isinstance(best_params[param], float)\n",
    "              else f'  {param:20s}: {best_params[param]}')\n",
    "\n",
    "    print(f'\\nValidation Performance:')\n",
    "    print(f'  RMSE: ${val_rmse:,.2f}')\n",
    "    print(f'  MAE:  ${val_mae:,.2f}')\n",
    "    print(f'  R²:   {val_r2:.4f}')\n",
    "\n",
    "    return best_model, best_params, {'rmse': val_rmse, 'mae': val_mae, 'r2': val_r2}\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAIN ALL MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TRAINING PHASE: ALL ROUTES AND HORIZONS')\n",
    "print('='*80)\n",
    "\n",
    "# P1A models\n",
    "print('\\n--- P1A_82 (ATLANTIC) ---')\n",
    "for h in HORIZONS:\n",
    "    target_col = f'P1A_82_h{h}'\n",
    "\n",
    "    model, params, metrics = train_xgboost_core(\n",
    "        X_train=p1a_train[p1a_features],\n",
    "        y_train=targets_train[target_col],\n",
    "        X_val=p1a_val[p1a_features],\n",
    "        y_val=targets_val[target_col],\n",
    "        route_name='P1A_82',\n",
    "        horizon=h,\n",
    "        n_iter=N_ITER\n",
    "    )\n",
    "\n",
    "    models[f'P1A_h{h}'] = model\n",
    "    results.append({\n",
    "        'route': 'P1A_82',\n",
    "        'horizon': h,\n",
    "        'val_rmse': metrics['rmse'],\n",
    "        'val_mae': metrics['mae'],\n",
    "        'val_r2': metrics['r2'],\n",
    "        'best_params': params\n",
    "    })\n",
    "\n",
    "# P3A models\n",
    "print('\\n--- P3A_82 (PACIFIC) ---')\n",
    "for h in HORIZONS:\n",
    "    target_col = f'P3A_82_h{h}'\n",
    "\n",
    "    model, params, metrics = train_xgboost_core(\n",
    "        X_train=p3a_train[p3a_features],\n",
    "        y_train=targets_train[target_col],\n",
    "        X_val=p3a_val[p3a_features],\n",
    "        y_val=targets_val[target_col],\n",
    "        route_name='P3A_82',\n",
    "        horizon=h,\n",
    "        n_iter=N_ITER\n",
    "    )\n",
    "\n",
    "    models[f'P3A_h{h}'] = model\n",
    "    results.append({\n",
    "        'route': 'P3A_82',\n",
    "        'horizon': h,\n",
    "        'val_rmse': metrics['rmse'],\n",
    "        'val_mae': metrics['mae'],\n",
    "        'val_r2': metrics['r2'],\n",
    "        'best_params': params\n",
    "    })\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print(f'All {len(models)} models trained successfully!')\n",
    "print('='*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('VALIDATION PERFORMANCE SUMMARY (CORE FEATURES)')\n",
    "print('='*80)\n",
    "print(results_df[['route', 'horizon', 'val_rmse', 'val_mae', 'val_r2']].to_string(index=False))\n",
    "\n",
    "print('\\n' + '-'*80)\n",
    "print('Performance by Route (averaged across horizons):')\n",
    "print('-'*80)\n",
    "route_summary = results_df.groupby('route')[['val_rmse', 'val_mae', 'val_r2']].mean()\n",
    "print(route_summary.to_string())\n",
    "\n",
    "# ==============================================================================\n",
    "# COMPARISON WITH BASELINES\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('BASELINE COMPARISON (h=1 forecast)')\n",
    "print('='*80)\n",
    "\n",
    "# Extract h=1 results\n",
    "xgb_core_h1 = results_df[results_df['horizon'] == 1]\n",
    "\n",
    "# Expected baselines (from previous runs)\n",
    "baselines = pd.DataFrame([\n",
    "    {'route': 'P1A_82', 'model': 'SARIMAX', 'rmse': 7573.66, 'mae': 7113.69},\n",
    "    {'route': 'P1A_82', 'model': 'XGBoost (ML 59 feat)', 'rmse': 4284.31, 'mae': 3672.78},\n",
    "    {'route': 'P3A_82', 'model': 'SARIMAX', 'rmse': 2477.18, 'mae': 2082.62},\n",
    "    {'route': 'P3A_82', 'model': 'XGBoost (ML 61 feat)', 'rmse': 1098.56, 'mae': 920.90},\n",
    "])\n",
    "\n",
    "# Add XGBoost CORE results\n",
    "for _, row in xgb_core_h1.iterrows():\n",
    "    baselines = pd.concat([baselines, pd.DataFrame([{\n",
    "        'route': row['route'],\n",
    "        'model': 'XGBoost (CORE 12 feat)',\n",
    "        'rmse': row['val_rmse'],\n",
    "        'mae': row['val_mae']\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Display comparison\n",
    "print('\\nModel Performance Comparison (RMSE):')\n",
    "print('-'*80)\n",
    "for route in ['P1A_82', 'P3A_82']:\n",
    "    print(f'\\n{route}:')\n",
    "    route_baselines = baselines[baselines['route'] == route].sort_values('rmse')\n",
    "    for _, row in route_baselines.iterrows():\n",
    "        print(f\"  {row['model']:30s}: RMSE ${row['rmse']:>10,.2f}  |  MAE ${row['mae']:>10,.2f}\")\n",
    "\n",
    "    # Calculate improvement over SARIMAX\n",
    "    sarimax_rmse = baselines[(baselines['route'] == route) & (baselines['model'] == 'SARIMAX')]['rmse'].values[0]\n",
    "    core_rmse = baselines[(baselines['route'] == route) & (baselines['model'] == 'XGBoost (CORE 12 feat)')]['rmse'].values[0]\n",
    "    improvement = ((sarimax_rmse - core_rmse) / sarimax_rmse) * 100\n",
    "    print(f'\\n  → XGBoost CORE vs SARIMAX: {improvement:+.1f}% improvement')\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('SAVING MODELS AND RESULTS')\n",
    "print('='*80)\n",
    "\n",
    "# Save models\n",
    "for model_name, model in models.items():\n",
    "    model.save_model(f'{OUTPUT_DIR}{model_name}_core_model.json')\n",
    "    print(f'[SAVED] {model_name}_core_model.json')\n",
    "\n",
    "# Save results DataFrame\n",
    "results_df.to_csv(f'{OUTPUT_DIR}xgboost_core_results.csv', index=False)\n",
    "print(f'[SAVED] xgboost_core_results.csv')\n",
    "\n",
    "# Save comparison\n",
    "baselines.to_csv(f'{OUTPUT_DIR}model_comparison_with_baselines.csv', index=False)\n",
    "print(f'[SAVED] model_comparison_with_baselines.csv')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'strategy': 'XGBoost on CORE features (12 per route)',\n",
    "    'purpose': 'Reduce overfitting risk compared to full ML feature set',\n",
    "    'routes': ['P1A_82', 'P3A_82'],\n",
    "    'horizons': HORIZONS,\n",
    "    'n_features': {'P1A': len(p1a_features), 'P3A': len(p3a_features)},\n",
    "    'training_samples': len(p1a_train),\n",
    "    'validation_samples': len(p1a_val),\n",
    "    'test_samples': len(p1a_test),\n",
    "    'hyperparameter_search': f'Manual random search, n_iter={N_ITER}',\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f'[SAVED] metadata.json')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('SCRIPT COMPLETE!')\n",
    "print('='*80)\n",
    "print(f'\\nOutputs saved to: {OUTPUT_DIR}')\n",
    "print('\\nNext steps:')\n",
    "print('  1. Review model_comparison_with_baselines.csv')\n",
    "print('  2. Check if P1A R² improved (should be positive now)')\n",
    "print('  3. If P1A still negative R², try feature selection (Script 08)')\n",
    "print('='*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
