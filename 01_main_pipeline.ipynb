{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized successfully.\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 1.26.4\n",
      "\n",
      "Study Period: 2021-03-01 onwards\n",
      "All processed files will be saved to: data/processed/pipeline\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# --- Core Parameters ---\n",
    "STUDY_START_DATE = '2021-03-01'\n",
    "study_start = pd.to_datetime(STUDY_START_DATE) # Define this globally\n",
    "# Define the root directory for raw data input\n",
    "INPUT_ROOT = 'data/raw/'\n",
    "# Define the primary output directory for all processed data and analysis files\n",
    "OUTPUT_DIR = 'data/processed/pipeline'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Environment initialized successfully.\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"\\nStudy Period: {STUDY_START_DATE} onwards\")\n",
    "print(f\"All processed files will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bunker-api-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUNKER DATA PIPELINE: STAGE 1 (OPTIONAL API FETCH)\n",
      "================================================================================\n",
      "\n",
      "API credentials loaded. Fetching data from Socrata...\n",
      "API data fetching complete. Loaded 1189 records.\n",
      "\n",
      "Loading and processing manual CSV files...\n",
      "\n",
      "Processing complete. Consolidated raw data saved to 'data/raw/bunker\\bunker_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.1.1a: Optional - Fetch historical data from Socrata API and create raw consolidated file\n",
    "# Note: This cell is optional. If API credentials are not provided, it will skip to using local CSV files\n",
    "import os\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUNKER DATA PIPELINE: STAGE 1 (OPTIONAL API FETCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Attempt to load credentials from a .env file\n",
    "    # Ensure 'socrata_api_credentials.env' is in the same directory or provide the correct path\n",
    "    if os.path.exists('socrata_api_credentials.env'):\n",
    "        load_dotenv(dotenv_path='socrata_api_credentials.env')\n",
    "        app_token = os.getenv(\"SOCRATA_APP_TOKEN\")\n",
    "        username = os.getenv(\"SOCRATA_USERNAME\")\n",
    "        password = os.getenv(\"SOCRATA_PASSWORD\")\n",
    "    else:\n",
    "        app_token, username, password = None, None, None\n",
    "\n",
    "    if not all([app_token, username, password]):\n",
    "        print(\"\\nWARNING: Missing API credentials. Skipping Socrata API fetch.\")\n",
    "        # As a fallback for the rest of the script to run, create an empty api_df\n",
    "        api_df = pd.DataFrame(columns=['date', 'vlsfo_price', 'mgo_price'])\n",
    "        api_df['date'] = pd.to_datetime(api_df['date'])\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nAPI credentials loaded. Fetching data from Socrata...\")\n",
    "        client = Socrata(\"agtransport.usda.gov\", app_token, username=username, password=password)\n",
    "        start_date = \"2021-03-01T00:00:00.000\"\n",
    "        end_date = \"2025-09-25T00:00:00.000\"\n",
    "        date_filter = f\"day BETWEEN '{start_date}' AND '{end_date}'\"\n",
    "        \n",
    "        # Fetch data from the API\n",
    "        bunker_results = client.get(\"4v3x-mj86\", where=date_filter, limit=20000)\n",
    "        \n",
    "        if bunker_results:\n",
    "            api_df = pd.DataFrame.from_records(bunker_results)\n",
    "            # Select and rename necessary columns\n",
    "            api_df = api_df[['day', 'vlsfo_fuel_oil_imo_2020_grade_0_5', 'marine_gas_oil']].copy()\n",
    "            api_df.rename(columns={'day': 'date', 'vlsfo_fuel_oil_imo_2020_grade_0_5': 'vlsfo_price', 'marine_gas_oil': 'mgo_price'}, inplace=True)\n",
    "            # Convert date column to datetime objects\n",
    "            api_df['date'] = pd.to_datetime(api_df['date'])\n",
    "            print(f\"API data fetching complete. Loaded {len(api_df)} records.\")\n",
    "        else:\n",
    "            print(\"No data returned from API.\")\n",
    "            api_df = pd.DataFrame(columns=['date', 'vlsfo_price', 'mgo_price'])\n",
    "            api_df['date'] = pd.to_datetime(api_df['date'])\n",
    "\n",
    "\n",
    "    # --- This section now runs regardless of API success ---\n",
    "    manual_files = {'vlsfo': 'shipandbunker_vlsfo_past_month.csv', 'mgo': 'shipandbunker_mgo_past_month.csv'}\n",
    "    manual_dataframes = []\n",
    "    \n",
    "    # Create dummy CSV files for demonstration if they don't exist\n",
    "    for filename in manual_files.values():\n",
    "        filepath = os.path.join(INPUT_ROOT, 'bunker', filename)\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Creating dummy file: {filepath}\")\n",
    "            dummy_data = \"ColA,ColB,Date,Price\\nVal1,Val2,2025-10-01,700.50\\nVal3,Val4,2025-10-02,705.75\"\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(dummy_data)\n",
    "\n",
    "    print(\"\\nLoading and processing manual CSV files...\")\n",
    "    for fuel, filename in manual_files.items():\n",
    "        filepath = os.path.join(INPUT_ROOT, 'bunker', filename)\n",
    "        if os.path.exists(filepath):\n",
    "            # FIX: Changed header=None to header=0 to correctly handle the header row.\n",
    "            df = pd.read_csv(filepath, header=0, usecols=[2, 3], names=['date', f'{fuel}_price'])\n",
    "            manual_dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"WARNING: Manual file not found: {filepath}\")\n",
    "\n",
    "    if len(manual_dataframes) == 2:\n",
    "        vlsfo_manual_df, mgo_manual_df = manual_dataframes\n",
    "        vlsfo_manual_df['date'] = pd.to_datetime(vlsfo_manual_df['date'])\n",
    "        mgo_manual_df['date'] = pd.to_datetime(mgo_manual_df['date'])\n",
    "        manual_df = pd.merge(vlsfo_manual_df, mgo_manual_df, on='date', how='outer')\n",
    "\n",
    "        # Combine API and manual data\n",
    "        # This condition handles cases where the API call might have failed and returned an empty DataFrame\n",
    "        if not api_df.empty:\n",
    "            combined_df = pd.concat([api_df, manual_df[manual_df['date'] > api_df['date'].max()]], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = manual_df\n",
    "\n",
    "        # Clean and finalize the dataframe\n",
    "        for col in ['vlsfo_price', 'mgo_price']:\n",
    "            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "        \n",
    "        combined_df.sort_values(by='date', inplace=True)\n",
    "        combined_df.drop_duplicates(subset='date', keep='last', inplace=True)\n",
    "        \n",
    "        # Save the consolidated data\n",
    "        output_dir = os.path.join(INPUT_ROOT, 'bunker')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, \"bunker_data.csv\")\n",
    "        combined_df.to_csv(output_path, index=False, date_format='%Y-%m-%d')\n",
    "        print(f\"\\nProcessing complete. Consolidated raw data saved to '{output_path}'.\")\n",
    "    else:\n",
    "        print(\"\\nCould not process manual files. Skipping consolidation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: An unexpected error occurred in the pipeline: {e}. Proceeding with existing local data if possible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bunker-final-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUNKER DATA: LOADING AND CREATING CANONICAL FILE\n",
      "================================================================================\n",
      "\n",
      "Loading consolidated bunker data from: data/raw/bunker/bunker_data.csv\n",
      "  Loaded 1201 rows, 3 columns\n",
      "  Columns: ['date', 'vlsfo_price', 'mgo_price']\n",
      "\n",
      "Bunker Data Summary:\n",
      "  Date Range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "  VLSFO Mean: $647.69/MT\n",
      "  MGO Mean: $858.33/MT\n",
      "  Missing Values: VLSFO=0, MGO=0\n",
      "\n",
      "Filtered to study period (2021-03-01 onwards): 1201 rows\n",
      "  Saved to: data/processed/pipeline\\bunker_canonical.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.1.1b: Load the consolidated raw data and save the canonical version\n",
    "print(\"=\"*80)\n",
    "print(\"BUNKER DATA: LOADING AND CREATING CANONICAL FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bunker_file = os.path.join(INPUT_ROOT, 'bunker/bunker_data.csv')\n",
    "print(f\"\\nLoading consolidated bunker data from: {bunker_file}\")\n",
    "\n",
    "bunker_df = pd.read_csv(bunker_file)\n",
    "print(f\"  Loaded {len(bunker_df)} rows, {len(bunker_df.columns)} columns\")\n",
    "print(f\"  Columns: {list(bunker_df.columns)}\")\n",
    "\n",
    "date_col = None\n",
    "for col in bunker_df.columns:\n",
    "    if col.lower() == 'date':\n",
    "        date_col = col\n",
    "        break\n",
    "if date_col is None: raise ValueError(\"No date column found\")\n",
    "\n",
    "bunker_df[date_col] = pd.to_datetime(bunker_df[date_col])\n",
    "bunker_df = bunker_df.rename(columns={date_col: 'Date'})\n",
    "\n",
    "print(f\"\\nBunker Data Summary:\")\n",
    "print(f\"  Date Range: {bunker_df['Date'].min()} to {bunker_df['Date'].max()}\")\n",
    "print(f\"  VLSFO Mean: ${bunker_df['vlsfo_price'].mean():.2f}/MT\")\n",
    "print(f\"  MGO Mean: ${bunker_df['mgo_price'].mean():.2f}/MT\")\n",
    "print(f\"  Missing Values: VLSFO={bunker_df['vlsfo_price'].isna().sum()}, MGO={bunker_df['mgo_price'].isna().sum()}\")\n",
    "\n",
    "bunker_df = bunker_df[bunker_df['Date'] >= study_start]\n",
    "print(f\"\\nFiltered to study period ({STUDY_START_DATE} onwards): {len(bunker_df)} rows\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "bunker_output = os.path.join(OUTPUT_DIR, 'bunker_canonical.csv')\n",
    "bunker_df.to_csv(bunker_output, index=False)\n",
    "print(f\"  Saved to: {bunker_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BALTIC EXCHANGE SPOT RATES & INDICES ACQUISITION\n",
      "================================================================================\n",
      "\n",
      "Loading Baltic Exchange data from: data/raw/baltic_exchange/baltic_exchange_historic_data_010118_061025.csv\n",
      "  Successfully loaded 1202 rows\n",
      "  Total columns: 46\n",
      "\n",
      "Pre-cleaning Baltic Exchange data...\n",
      "  Dropped 53 rows where both P1A_82 and P3A_82 were null\n",
      "  Remaining rows: 1149\n",
      "\n",
      "Filtered to study period: 1149 rows\n",
      "\n",
      "Target Variable Coverage:\n",
      "  P1A_82: 100.00% (1149/1149 observations)\n",
      "    Mean: $17156.13, Std: $8110.55\n",
      "    Range: $4458.00 - $45050.00\n",
      "  P3A_82: 100.00% (1149/1149 observations)\n",
      "    Mean: $16725.27, Std: $7525.47\n",
      "    Range: $5434.00 - $40687.00\n",
      "\n",
      "Saved to: data/processed/pipeline\\baltic_spot_canonical.csv\n",
      "  Shape: 1149 rows x 46 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BALTIC EXCHANGE SPOT RATES & INDICES ACQUISITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baltic_file = os.path.join(INPUT_ROOT, 'baltic_exchange/baltic_exchange_historic_data_010118_061025.csv')\n",
    "print(f\"\\nLoading Baltic Exchange data from: {baltic_file}\")\n",
    "\n",
    "baltic_df = pd.read_csv(baltic_file)\n",
    "print(f\"  Successfully loaded {len(baltic_df)} rows\")\n",
    "print(f\"  Total columns: {len(baltic_df.columns)}\")\n",
    "\n",
    "print(\"\\nPre-cleaning Baltic Exchange data...\")\n",
    "initial_rows = len(baltic_df)\n",
    "baltic_df_clean = baltic_df.dropna(subset=['P1A_82', 'P3A_82'], how='all')\n",
    "rows_dropped = initial_rows - len(baltic_df_clean)\n",
    "print(f\"  Dropped {rows_dropped} rows where both P1A_82 and P3A_82 were null\")\n",
    "print(f\"  Remaining rows: {len(baltic_df_clean)}\")\n",
    "\n",
    "date_col = None\n",
    "for col in baltic_df_clean.columns:\n",
    "    if col.lower().strip() == 'date':\n",
    "        date_col = col\n",
    "        break\n",
    "if date_col is None: raise ValueError(\"No date column found\")\n",
    "\n",
    "baltic_df_clean[date_col] = pd.to_datetime(baltic_df_clean[date_col])\n",
    "baltic_df_clean = baltic_df_clean.rename(columns={date_col: 'Date'})\n",
    "\n",
    "baltic_spot = baltic_df_clean[baltic_df_clean['Date'] >= study_start].copy()\n",
    "print(f\"\\nFiltered to study period: {len(baltic_spot)} rows\")\n",
    "\n",
    "print(\"\\nTarget Variable Coverage:\")\n",
    "for target in ['P1A_82', 'P3A_82']:\n",
    "    coverage = (baltic_spot[target].notna().sum() / len(baltic_spot)) * 100\n",
    "    print(f\"  {target}: {coverage:.2f}% ({baltic_spot[target].notna().sum()}/{len(baltic_spot)} observations)\")\n",
    "    print(f\"    Mean: ${baltic_spot[target].mean():.2f}, Std: ${baltic_spot[target].std():.2f}\")\n",
    "    print(f\"    Range: ${baltic_spot[target].min():.2f} - ${baltic_spot[target].max():.2f}\")\n",
    "\n",
    "baltic_output = os.path.join(OUTPUT_DIR, 'baltic_spot_canonical.csv')\n",
    "baltic_spot.to_csv(baltic_output, index=False)\n",
    "print(f\"\\nSaved to: {baltic_output}\")\n",
    "print(f\"  Shape: {baltic_spot.shape[0]} rows x {baltic_spot.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BALTIC FORWARD ASSESSMENTS (BFA) ACQUISITION\n",
      "================================================================================\n",
      "\n",
      "Loading BFA data from: data/raw/baltic_exchange/bfa_panamax_20190923_20251010.csv\n",
      "  Loaded 25022 rows, 5 columns\n",
      "  Columns: ['GroupDesc', 'ArchiveDate', 'RouteIdentifier', 'RouteAverage', 'FFADescription']\n",
      "  Filtered to study period: 25022 rows\n",
      "\n",
      "Pivoting BFA data from long to wide format...\n",
      "  Pivoted to wide format: 1165 rows, 23 columns\n",
      "\n",
      "Forward Curve Structure Analysis:\n",
      "  P1EA_82 (Atlantic) contracts: 11\n",
      "  P3EA_82 (Pacific) contracts: 11\n",
      "  Total forward contracts: 22\n",
      "\n",
      "Saved to: data/processed/pipeline\\bfa_wide_canonical.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BALTIC FORWARD ASSESSMENTS (BFA) ACQUISITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bfa_file = os.path.join(INPUT_ROOT, 'baltic_exchange/bfa_panamax_20190923_20251010.csv')\n",
    "print(f\"\\nLoading BFA data from: {bfa_file}\")\n",
    "\n",
    "bfa_df = pd.read_csv(bfa_file)\n",
    "print(f\"  Loaded {len(bfa_df)} rows, {len(bfa_df.columns)} columns\")\n",
    "print(f\"  Columns: {list(bfa_df.columns)}\")\n",
    "\n",
    "date_col = None\n",
    "for col in bfa_df.columns:\n",
    "    if 'date' in col.lower():\n",
    "        date_col = col\n",
    "        break\n",
    "if date_col: \n",
    "    bfa_df[date_col] = pd.to_datetime(bfa_df[date_col])\n",
    "    bfa_df = bfa_df.rename(columns={date_col: 'Date'})\n",
    "    bfa_df = bfa_df[bfa_df['Date'] >= study_start]\n",
    "    print(f\"  Filtered to study period: {len(bfa_df)} rows\")\n",
    "\n",
    "if 'RouteIdentifier' in bfa_df.columns and 'RouteAverage' in bfa_df.columns:\n",
    "    print(\"\\nPivoting BFA data from long to wide format...\")\n",
    "    bfa_wide = bfa_df.pivot(index='Date', columns='RouteIdentifier', values='RouteAverage').reset_index()\n",
    "    print(f\"  Pivoted to wide format: {len(bfa_wide)} rows, {len(bfa_wide.columns)} columns\")\n",
    "else:\n",
    "    bfa_wide = bfa_df\n",
    "\n",
    "print(\"\\nForward Curve Structure Analysis:\")\n",
    "p1_cols = [col for col in bfa_wide.columns if 'P1EA_82' in str(col)]\n",
    "p3_cols = [col for col in bfa_wide.columns if 'P3EA_82' in str(col)]\n",
    "print(f\"  P1EA_82 (Atlantic) contracts: {len(p1_cols)}\")\n",
    "print(f\"  P3EA_82 (Pacific) contracts: {len(p3_cols)}\")\n",
    "print(f\"  Total forward contracts: {len(p1_cols) + len(p3_cols)}\")\n",
    "\n",
    "bfa_output = os.path.join(OUTPUT_DIR, 'bfa_wide_canonical.csv')\n",
    "bfa_wide.to_csv(bfa_output, index=False)\n",
    "print(f\"\\nSaved to: {bfa_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarksons helper functions defined successfully (WITH SERIES NUMBER EXTRACTION).\n"
     ]
    }
   ],
   "source": [
    "def extract_series_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extract series numbers, indicator names, and units from Clarksons Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of column index to {series_number, indicator, unit}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read rows 4-6 (indices 3-5) to get metadata\n",
    "        df_meta = pd.read_excel(file_path, header=None, nrows=6)\n",
    "        \n",
    "        series_numbers = df_meta.iloc[3, :]  # Row 4: Series numbers\n",
    "        indicator_names = df_meta.iloc[4, :]  # Row 5: Indicator names\n",
    "        units = df_meta.iloc[5, :]           # Row 6: Units/Date\n",
    "        \n",
    "        metadata = {}\n",
    "        for col_idx in range(len(series_numbers)):\n",
    "            # Skip first column (Date)\n",
    "            if col_idx == 0:\n",
    "                continue\n",
    "                \n",
    "            series_num = series_numbers.iloc[col_idx]\n",
    "            indicator = indicator_names.iloc[col_idx]\n",
    "            unit = units.iloc[col_idx]\n",
    "            \n",
    "            # Only include if series number exists\n",
    "            if pd.notna(series_num):\n",
    "                metadata[col_idx] = {\n",
    "                    'series_number': str(int(series_num)) if not pd.isna(series_num) else 'Unknown',\n",
    "                    'indicator': str(indicator) if pd.notna(indicator) else 'Unknown',\n",
    "                    'unit': str(unit) if pd.notna(unit) else 'Unknown'\n",
    "                }\n",
    "        \n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "def detect_frequency_from_format(date_series):\n",
    "    \"\"\"\n",
    "    Detect frequency by examining the format of date values.\n",
    "    \"\"\"\n",
    "    sample = date_series.dropna().head(10).astype(str)\n",
    "    if len(sample) == 0: return 'Unknown'\n",
    "    first_val = sample.iloc[0].strip()\n",
    "    if 'Q' in first_val and '-' in first_val: return 'Quarterly'\n",
    "    if len(first_val) == 4 and first_val.isdigit(): return 'Yearly'\n",
    "    try:\n",
    "        if '-' in first_val and len(sample) >= 2:\n",
    "            date1 = pd.to_datetime(sample.iloc[0], format='%d-%m-%Y', errors='coerce')\n",
    "            date2 = pd.to_datetime(sample.iloc[1], format='%d-%m-%Y', errors='coerce')\n",
    "            if pd.notna(date1) and pd.notna(date2):\n",
    "                diff = (date2 - date1).days\n",
    "                if diff <= 3: return 'Daily'\n",
    "                elif diff <= 10: return 'Weekly'\n",
    "                elif diff <= 45: return 'Monthly'\n",
    "    except:\n",
    "        pass\n",
    "    return 'Unknown'\n",
    "\n",
    "def infer_frequency_from_dates(dates):\n",
    "    \"\"\"\n",
    "    Infer frequency from parsed datetime objects.\n",
    "    \"\"\"\n",
    "    clean_dates = dates.dropna().sort_values()\n",
    "    if len(clean_dates) < 2: return 'Unknown'\n",
    "    diffs = clean_dates.diff().dropna()\n",
    "    median_diff = diffs.median()\n",
    "    if median_diff <= pd.Timedelta(days=3): return 'Daily'\n",
    "    elif median_diff <= pd.Timedelta(days=10): return 'Weekly'\n",
    "    elif median_diff <= pd.Timedelta(days=45): return 'Monthly'\n",
    "    elif median_diff <= pd.Timedelta(days=120): return 'Quarterly'\n",
    "    else: return 'Yearly'\n",
    "\n",
    "def parse_clarksons_dates(date_series, frequency):\n",
    "    \"\"\"\n",
    "    Parse Clarksons dates based on detected frequency.\n",
    "    \"\"\"\n",
    "    if frequency in ['Daily', 'Weekly', 'Monthly', 'Unknown']:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(date_series, format='%d-%m-%Y', errors='coerce')\n",
    "        except:\n",
    "            parsed = pd.to_datetime(date_series, errors='coerce')\n",
    "    elif frequency == 'Quarterly':\n",
    "        def quarter_to_date(q_str):\n",
    "            try:\n",
    "                if pd.isna(q_str): return pd.NaT\n",
    "                q_str = str(q_str).strip()\n",
    "                if 'Q' in q_str and '-' in q_str:\n",
    "                    quarter, year = q_str.split('-')\n",
    "                    quarter_num = int(quarter.replace('Q', ''))\n",
    "                    month = (quarter_num - 1) * 3 + 1\n",
    "                    return pd.Timestamp(int(year), month, 1)\n",
    "                return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        parsed = date_series.apply(quarter_to_date)\n",
    "    elif frequency == 'Yearly':\n",
    "        def year_to_date(year_str):\n",
    "            try:\n",
    "                if pd.isna(year_str): return pd.NaT\n",
    "                year = int(str(year_str).strip())\n",
    "                return pd.Timestamp(year, 1, 1)\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        parsed = date_series.apply(year_to_date)\n",
    "    else:\n",
    "        parsed = pd.to_datetime(date_series, errors='coerce')\n",
    "    return parsed\n",
    "\n",
    "def construct_clarksons_headers(multi_index_cols):\n",
    "    \"\"\"\n",
    "    Construct proper column headers from Clarksons multi-level structure.\n",
    "    \n",
    "    Structure (Excel rows 5-6 read as headers using header=[4, 5]):\n",
    "    - First column: (indicator from row 5 or empty, 'Date' from row 6) -> 'Date'\n",
    "    - Other columns: ('Indicator Name' from row 5, 'Unit' from row 6) -> 'Indicator Name_Unit'\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    for col in multi_index_cols:\n",
    "        level0, level1 = str(col[0]), str(col[1])\n",
    "        \n",
    "        # Check if this is the Date column\n",
    "        if level1.strip().lower() == 'date' or (pd.isna(col[0]) and 'date' in level1.lower()):\n",
    "            new_cols.append('Date')\n",
    "        else:\n",
    "            # For other columns, combine indicator name with unit\n",
    "            # Handle NaN or empty values in level0\n",
    "            if pd.isna(col[0]) or level0.strip().lower() in ('nan', ''):\n",
    "                # If level0 is empty, use only level1\n",
    "                new_cols.append(level1.strip())\n",
    "            else:\n",
    "                # Combine indicator (level0) with unit (level1)\n",
    "                combined = f\"{level0.strip()}_{level1.strip()}\"\n",
    "                new_cols.append(combined.strip('_'))\n",
    "    \n",
    "    return new_cols\n",
    "\n",
    "def get_column_signature(file_path):\n",
    "    \"\"\"\n",
    "    Extract column signature for duplicate detection.\n",
    "    Reads Excel rows 5-6 as headers (pandas header=[4, 5]).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, header=[4, 5], nrows=1)\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            cols = construct_clarksons_headers(df.columns)\n",
    "        else:\n",
    "            cols = list(df.columns)\n",
    "        return tuple(sorted(cols[1:]))  # Exclude Date column\n",
    "    except:\n",
    "        return tuple()\n",
    "\n",
    "print(\"Clarksons helper functions defined successfully (WITH SERIES NUMBER EXTRACTION).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLARKSONS SHIPPING INTELLIGENCE DATA ACQUISITION (WITH SERIES NUMBERS)\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Building Clarksons file manifest...\n",
      "  Scanning directory: data/raw/clarksons/\n",
      "  Found 258 Excel files\n",
      "  Unique economic concepts: 77\n",
      "\n",
      "Phase 2: Selecting best file for each economic concept...\n",
      "\n",
      "  Total files selected: 70\n",
      "\n",
      "Phase 3: Parsing and consolidating selected Clarksons files (WITH SERIES NUMBERS)...\n",
      "\n",
      "Consolidated Clarksons data: 188161 rows\n",
      "  Unique series: 634\n",
      "  Unique series numbers: 632\n",
      "\n",
      "Saved to: data/processed/pipeline\\clarksons_canonical.csv\n",
      "\n",
      "Saved series metadata to: data/processed/pipeline\\clarksons_series_metadata.csv\n",
      "  Total series documented: 634\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CLARKSONS SHIPPING INTELLIGENCE DATA ACQUISITION (WITH SERIES NUMBERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPhase 1: Building Clarksons file manifest...\")\n",
    "clarksons_root = os.path.join(INPUT_ROOT, 'clarksons/')\n",
    "manifest = []\n",
    "print(f\"  Scanning directory: {clarksons_root}\")\n",
    "for root, dirs, files in os.walk(clarksons_root):\n",
    "    for file in files:\n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, clarksons_root)\n",
    "            path_parts = rel_path.replace('\\\\', '/').split('/')\n",
    "            economic_concept = '_'.join(path_parts[:-1]) if len(path_parts) > 1 else 'unknown'\n",
    "            manifest.append({'file_path': file_path, 'economic_concept': economic_concept, 'frequency': 'Unknown'})\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "print(f\"  Found {len(manifest_df)} Excel files\")\n",
    "print(f\"  Unique economic concepts: {manifest_df['economic_concept'].nunique()}\")\n",
    "\n",
    "print(\"\\nPhase 2: Selecting best file for each economic concept...\")\n",
    "frequency_priority = {'Daily': 5, 'Weekly': 4, 'Monthly': 3, 'Quarterly': 2, 'Yearly': 1, 'Unknown': 0}\n",
    "selected_files = []\n",
    "data_selection_log = []\n",
    "\n",
    "for concept in manifest_df['economic_concept'].unique():\n",
    "    concept_files = manifest_df[manifest_df['economic_concept'] == concept].copy()\n",
    "    candidates = []\n",
    "    for _, file_info in concept_files.iterrows():\n",
    "        try:\n",
    "            # Read Excel rows 5-6 as headers (indicator names + Date/units)\n",
    "            df = pd.read_excel(file_info['file_path'], header=[4, 5])\n",
    "            \n",
    "            # Construct proper column headers\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = construct_clarksons_headers(df.columns)\n",
    "            \n",
    "            # Trim data at \"Please note\" footer\n",
    "            date_col_data = df.iloc[:, 0].astype(str)\n",
    "            end_idx = date_col_data[date_col_data.str.contains('Please note', case=False, na=False)].index\n",
    "            if len(end_idx) > 0:\n",
    "                df = df.iloc[:end_idx[0]]\n",
    "            \n",
    "            # Detect frequency and parse dates\n",
    "            frequency = detect_frequency_from_format(df.iloc[:, 0])\n",
    "            df['Date'] = parse_clarksons_dates(df.iloc[:, 0], frequency)\n",
    "            if frequency == 'Unknown': \n",
    "                frequency = infer_frequency_from_dates(df['Date'])\n",
    "            \n",
    "            # Filter to study period\n",
    "            df_study = df[df['Date'] >= study_start]\n",
    "            if len(df_study) == 0: \n",
    "                continue\n",
    "            \n",
    "            # Check data completeness (exclude Date column)\n",
    "            missing_pct = df_study.iloc[:, 1:].isna().mean().mean()\n",
    "            if missing_pct >= 0.2: \n",
    "                continue\n",
    "            \n",
    "            # Store candidate information\n",
    "            candidate_info = file_info.to_dict()\n",
    "            candidate_info.update({\n",
    "                'frequency': frequency,\n",
    "                'freq_score': frequency_priority.get(frequency, 0),\n",
    "                'col_signature': get_column_signature(file_info['file_path'])\n",
    "            })\n",
    "            candidates.append(candidate_info)\n",
    "            data_selection_log.append({\n",
    "                'concept': concept, \n",
    "                'file': os.path.basename(file_info['file_path']), \n",
    "                'selected': False\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # Silently skip files that cannot be processed\n",
    "            pass\n",
    "\n",
    "    # Select best candidate(s) for this concept\n",
    "    if len(candidates) > 0:\n",
    "        best_score = max(c['freq_score'] for c in candidates)\n",
    "        best_freq_candidates = [c for c in candidates if c['freq_score'] == best_score]\n",
    "        \n",
    "        # Remove duplicates based on column signature\n",
    "        unique_signatures = {}\n",
    "        final_selected = []\n",
    "        for candidate in best_freq_candidates:\n",
    "            sig = candidate['col_signature']\n",
    "            if sig not in unique_signatures:\n",
    "                unique_signatures[sig] = candidate\n",
    "                final_selected.append(candidate)\n",
    "        \n",
    "        selected_files.extend(final_selected)\n",
    "        \n",
    "        # Mark selected files in log\n",
    "        selected_filenames = [os.path.basename(sf['file_path']) for sf in final_selected]\n",
    "        for log in data_selection_log:\n",
    "            if log['file'] in selected_filenames and log['concept'] == concept:\n",
    "                log['selected'] = True\n",
    "\n",
    "print(f\"\\n  Total files selected: {len(selected_files)}\")\n",
    "\n",
    "print(\"\\nPhase 3: Parsing and consolidating selected Clarksons files (WITH SERIES NUMBERS)...\")\n",
    "all_clarksons_data = []\n",
    "series_metadata_records = []\n",
    "\n",
    "for file_info in selected_files:\n",
    "    try:\n",
    "        # Extract series metadata (row 4: series numbers)\n",
    "        series_meta = extract_series_metadata(file_info['file_path'])\n",
    "        \n",
    "        # Read Excel rows 5-6 as headers (pandas header=[4, 5])\n",
    "        df = pd.read_excel(file_info['file_path'], header=[4, 5])\n",
    "        \n",
    "        # Construct proper column headers\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = construct_clarksons_headers(df.columns)\n",
    "        \n",
    "        # Trim at \"Please note\" footer\n",
    "        date_col_data = df.iloc[:, 0].astype(str)\n",
    "        end_idx = date_col_data[date_col_data.str.contains('Please note', case=False, na=False)].index\n",
    "        if len(end_idx) > 0:\n",
    "            df = df.iloc[:end_idx[0]]\n",
    "        \n",
    "        # Parse dates\n",
    "        df['Date'] = parse_clarksons_dates(df.iloc[:, 0], file_info['frequency'])\n",
    "        \n",
    "        # Filter to study period\n",
    "        if file_info['frequency'] == 'Yearly': \n",
    "            df = df[df['Date'].dt.year >= study_start.year]\n",
    "        else: \n",
    "            df = df[df['Date'] >= study_start]\n",
    "        \n",
    "        # Process each column with its series number\n",
    "        for col_idx, col_name in enumerate(df.columns):\n",
    "            if col_name == 'Date':\n",
    "                continue\n",
    "            \n",
    "            # Get series metadata for this column\n",
    "            # Note: col_idx in df may not match original Excel column index due to header processing\n",
    "            # We need to map back to original column index\n",
    "            original_col_idx = col_idx  # This needs adjustment based on how pandas read the file\n",
    "            \n",
    "            # Try to find metadata by matching column index\n",
    "            series_number = 'Unknown'\n",
    "            for meta_idx, meta in series_meta.items():\n",
    "                if meta_idx == original_col_idx:\n",
    "                    series_number = meta['series_number']\n",
    "                    break\n",
    "            \n",
    "            # If not found by index, try to infer from metadata\n",
    "            if series_number == 'Unknown' and len(series_meta) > 0:\n",
    "                # Use a fallback: assign based on order\n",
    "                meta_keys = sorted(series_meta.keys())\n",
    "                if col_idx - 1 < len(meta_keys):  # -1 because Date is column 0\n",
    "                    series_number = series_meta[meta_keys[col_idx - 1]]['series_number']\n",
    "            \n",
    "            # Create series name with economic concept prefix\n",
    "            series_name = file_info['economic_concept'] + '_' + str(col_name)\n",
    "            \n",
    "            # Create long-format dataframe for this series\n",
    "            series_df = pd.DataFrame({\n",
    "                'Date': df['Date'],\n",
    "                'SeriesNumber': series_number,\n",
    "                'SeriesName': series_name,\n",
    "                'Value': df[col_name]\n",
    "            })\n",
    "            \n",
    "            # Remove null values\n",
    "            series_df = series_df.dropna(subset=['Value'])\n",
    "            \n",
    "            if len(series_df) > 0:\n",
    "                all_clarksons_data.append(series_df)\n",
    "                \n",
    "                # Record metadata\n",
    "                series_metadata_records.append({\n",
    "                    'SeriesNumber': series_number,\n",
    "                    'SeriesName': series_name,\n",
    "                    'EconomicConcept': file_info['economic_concept'],\n",
    "                    'Indicator': col_name,\n",
    "                    'Frequency': file_info['frequency'],\n",
    "                    'SourceFile': os.path.basename(file_info['file_path'])\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR processing {os.path.basename(file_info['file_path'])}: {e}\")\n",
    "\n",
    "if all_clarksons_data:\n",
    "    clarksons_canonical = pd.concat(all_clarksons_data, ignore_index=True)\n",
    "    print(f\"\\nConsolidated Clarksons data: {len(clarksons_canonical)} rows\")\n",
    "    print(f\"  Unique series: {clarksons_canonical['SeriesName'].nunique()}\")\n",
    "    print(f\"  Unique series numbers: {clarksons_canonical['SeriesNumber'].nunique()}\")\n",
    "    \n",
    "    # Save canonical dataset with series numbers\n",
    "    clarksons_output = os.path.join(OUTPUT_DIR, 'clarksons_canonical.csv')\n",
    "    clarksons_canonical.to_csv(clarksons_output, index=False)\n",
    "    print(f\"\\nSaved to: {clarksons_output}\")\n",
    "    \n",
    "    # Save series metadata\n",
    "    series_metadata_df = pd.DataFrame(series_metadata_records)\n",
    "    series_metadata_df = series_metadata_df.drop_duplicates(subset=['SeriesNumber', 'SeriesName'])\n",
    "    metadata_output = os.path.join(OUTPUT_DIR, 'clarksons_series_metadata.csv')\n",
    "    series_metadata_df.to_csv(metadata_output, index=False)\n",
    "    print(f\"\\nSaved series metadata to: {metadata_output}\")\n",
    "    print(f\"  Total series documented: {len(series_metadata_df)}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No Clarksons data was successfully processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLARKSONS SHIPPING INTELLIGENCE DATA ACQUISITION (WITH SERIES NUMBERS)\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Building Clarksons file manifest...\n",
      "  Scanning directory: data/raw/clarksons/\n",
      "  Found 258 Excel files\n",
      "  Unique economic concepts: 77\n",
      "\n",
      "Phase 2: Selecting best file for each economic concept...\n",
      "\n",
      "  Total files selected: 70\n",
      "\n",
      "Phase 3: Parsing and consolidating selected Clarksons files (WITH SERIES NUMBERS)...\n",
      "\n",
      "Consolidated Clarksons data: 188161 rows\n",
      "  Unique series: 634\n",
      "  Unique series numbers: 632\n",
      "\n",
      "Saved to: data/processed/pipeline\\clarksons_canonical.csv\n",
      "\n",
      "Saved series metadata to: data/processed/pipeline\\clarksons_series_metadata.csv\n",
      "  Total series documented: 634\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CLARKSONS SHIPPING INTELLIGENCE DATA ACQUISITION (WITH SERIES NUMBERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPhase 1: Building Clarksons file manifest...\")\n",
    "clarksons_root = os.path.join(INPUT_ROOT, 'clarksons/')\n",
    "manifest = []\n",
    "print(f\"  Scanning directory: {clarksons_root}\")\n",
    "for root, dirs, files in os.walk(clarksons_root):\n",
    "    for file in files:\n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(file_path, clarksons_root)\n",
    "            path_parts = rel_path.replace('\\\\', '/').split('/')\n",
    "            economic_concept = '_'.join(path_parts[:-1]) if len(path_parts) > 1 else 'unknown'\n",
    "            manifest.append({'file_path': file_path, 'economic_concept': economic_concept, 'frequency': 'Unknown'})\n",
    "\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "print(f\"  Found {len(manifest_df)} Excel files\")\n",
    "print(f\"  Unique economic concepts: {manifest_df['economic_concept'].nunique()}\")\n",
    "\n",
    "print(\"\\nPhase 2: Selecting best file for each economic concept...\")\n",
    "frequency_priority = {'Daily': 5, 'Weekly': 4, 'Monthly': 3, 'Quarterly': 2, 'Yearly': 1, 'Unknown': 0}\n",
    "selected_files = []\n",
    "data_selection_log = []\n",
    "\n",
    "for concept in manifest_df['economic_concept'].unique():\n",
    "    concept_files = manifest_df[manifest_df['economic_concept'] == concept].copy()\n",
    "    candidates = []\n",
    "    for _, file_info in concept_files.iterrows():\n",
    "        try:\n",
    "            # Read Excel rows 5-6 as headers (indicator names + Date/units)\n",
    "            df = pd.read_excel(file_info['file_path'], header=[4, 5])\n",
    "            \n",
    "            # Construct proper column headers\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = construct_clarksons_headers(df.columns)\n",
    "            \n",
    "            # Trim data at \"Please note\" footer\n",
    "            date_col_data = df.iloc[:, 0].astype(str)\n",
    "            end_idx = date_col_data[date_col_data.str.contains('Please note', case=False, na=False)].index\n",
    "            if len(end_idx) > 0:\n",
    "                df = df.iloc[:end_idx[0]]\n",
    "            \n",
    "            # Detect frequency and parse dates\n",
    "            frequency = detect_frequency_from_format(df.iloc[:, 0])\n",
    "            df['Date'] = parse_clarksons_dates(df.iloc[:, 0], frequency)\n",
    "            if frequency == 'Unknown': \n",
    "                frequency = infer_frequency_from_dates(df['Date'])\n",
    "            \n",
    "            # Filter to study period\n",
    "            df_study = df[df['Date'] >= study_start]\n",
    "            if len(df_study) == 0: \n",
    "                continue\n",
    "            \n",
    "            # Check data completeness (exclude Date column)\n",
    "            missing_pct = df_study.iloc[:, 1:].isna().mean().mean()\n",
    "            if missing_pct >= 0.2: \n",
    "                continue\n",
    "            \n",
    "            # Store candidate information\n",
    "            candidate_info = file_info.to_dict()\n",
    "            candidate_info.update({\n",
    "                'frequency': frequency,\n",
    "                'freq_score': frequency_priority.get(frequency, 0),\n",
    "                'col_signature': get_column_signature(file_info['file_path'])\n",
    "            })\n",
    "            candidates.append(candidate_info)\n",
    "            data_selection_log.append({\n",
    "                'concept': concept, \n",
    "                'file': os.path.basename(file_info['file_path']), \n",
    "                'selected': False\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # Silently skip files that cannot be processed\n",
    "            pass\n",
    "\n",
    "    # Select best candidate(s) for this concept\n",
    "    if len(candidates) > 0:\n",
    "        best_score = max(c['freq_score'] for c in candidates)\n",
    "        best_freq_candidates = [c for c in candidates if c['freq_score'] == best_score]\n",
    "        \n",
    "        # Remove duplicates based on column signature\n",
    "        unique_signatures = {}\n",
    "        final_selected = []\n",
    "        for candidate in best_freq_candidates:\n",
    "            sig = candidate['col_signature']\n",
    "            if sig not in unique_signatures:\n",
    "                unique_signatures[sig] = candidate\n",
    "                final_selected.append(candidate)\n",
    "        \n",
    "        selected_files.extend(final_selected)\n",
    "        \n",
    "        # Mark selected files in log\n",
    "        selected_filenames = [os.path.basename(sf['file_path']) for sf in final_selected]\n",
    "        for log in data_selection_log:\n",
    "            if log['file'] in selected_filenames and log['concept'] == concept:\n",
    "                log['selected'] = True\n",
    "\n",
    "print(f\"\\n  Total files selected: {len(selected_files)}\")\n",
    "\n",
    "print(\"\\nPhase 3: Parsing and consolidating selected Clarksons files (WITH SERIES NUMBERS)...\")\n",
    "all_clarksons_data = []\n",
    "series_metadata_records = []\n",
    "\n",
    "for file_info in selected_files:\n",
    "    try:\n",
    "        # Extract series metadata (row 4: series numbers)\n",
    "        series_meta = extract_series_metadata(file_info['file_path'])\n",
    "        \n",
    "        # Read Excel rows 5-6 as headers (pandas header=[4, 5])\n",
    "        df = pd.read_excel(file_info['file_path'], header=[4, 5])\n",
    "        \n",
    "        # Construct proper column headers\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = construct_clarksons_headers(df.columns)\n",
    "        \n",
    "        # Trim at \"Please note\" footer\n",
    "        date_col_data = df.iloc[:, 0].astype(str)\n",
    "        end_idx = date_col_data[date_col_data.str.contains('Please note', case=False, na=False)].index\n",
    "        if len(end_idx) > 0:\n",
    "            df = df.iloc[:end_idx[0]]\n",
    "        \n",
    "        # Parse dates\n",
    "        df['Date'] = parse_clarksons_dates(df.iloc[:, 0], file_info['frequency'])\n",
    "        \n",
    "        # Filter to study period\n",
    "        if file_info['frequency'] == 'Yearly': \n",
    "            df = df[df['Date'].dt.year >= study_start.year]\n",
    "        else: \n",
    "            df = df[df['Date'] >= study_start]\n",
    "        \n",
    "        # Process each column with its series number\n",
    "        for col_idx, col_name in enumerate(df.columns):\n",
    "            if col_name == 'Date':\n",
    "                continue\n",
    "            \n",
    "            # Get series metadata for this column\n",
    "            # Note: col_idx in df may not match original Excel column index due to header processing\n",
    "            # We need to map back to original column index\n",
    "            original_col_idx = col_idx  # This needs adjustment based on how pandas read the file\n",
    "            \n",
    "            # Try to find metadata by matching column index\n",
    "            series_number = 'Unknown'\n",
    "            for meta_idx, meta in series_meta.items():\n",
    "                if meta_idx == original_col_idx:\n",
    "                    series_number = meta['series_number']\n",
    "                    break\n",
    "            \n",
    "            # If not found by index, try to infer from metadata\n",
    "            if series_number == 'Unknown' and len(series_meta) > 0:\n",
    "                # Use a fallback: assign based on order\n",
    "                meta_keys = sorted(series_meta.keys())\n",
    "                if col_idx - 1 < len(meta_keys):  # -1 because Date is column 0\n",
    "                    series_number = series_meta[meta_keys[col_idx - 1]]['series_number']\n",
    "            \n",
    "            # Create series name with economic concept prefix\n",
    "            series_name = file_info['economic_concept'] + '_' + str(col_name)\n",
    "            \n",
    "            # Create long-format dataframe for this series\n",
    "            series_df = pd.DataFrame({\n",
    "                'Date': df['Date'],\n",
    "                'SeriesNumber': series_number,\n",
    "                'SeriesName': series_name,\n",
    "                'Value': df[col_name]\n",
    "            })\n",
    "            \n",
    "            # Remove null values\n",
    "            series_df = series_df.dropna(subset=['Value'])\n",
    "            \n",
    "            if len(series_df) > 0:\n",
    "                all_clarksons_data.append(series_df)\n",
    "                \n",
    "                # Record metadata\n",
    "                series_metadata_records.append({\n",
    "                    'SeriesNumber': series_number,\n",
    "                    'SeriesName': series_name,\n",
    "                    'EconomicConcept': file_info['economic_concept'],\n",
    "                    'Indicator': col_name,\n",
    "                    'Frequency': file_info['frequency'],\n",
    "                    'SourceFile': os.path.basename(file_info['file_path'])\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR processing {os.path.basename(file_info['file_path'])}: {e}\")\n",
    "\n",
    "if all_clarksons_data:\n",
    "    clarksons_canonical = pd.concat(all_clarksons_data, ignore_index=True)\n",
    "    print(f\"\\nConsolidated Clarksons data: {len(clarksons_canonical)} rows\")\n",
    "    print(f\"  Unique series: {clarksons_canonical['SeriesName'].nunique()}\")\n",
    "    print(f\"  Unique series numbers: {clarksons_canonical['SeriesNumber'].nunique()}\")\n",
    "    \n",
    "    # Save canonical dataset with series numbers\n",
    "    clarksons_output = os.path.join(OUTPUT_DIR, 'clarksons_canonical.csv')\n",
    "    clarksons_canonical.to_csv(clarksons_output, index=False)\n",
    "    print(f\"\\nSaved to: {clarksons_output}\")\n",
    "    \n",
    "    # Save series metadata\n",
    "    series_metadata_df = pd.DataFrame(series_metadata_records)\n",
    "    series_metadata_df = series_metadata_df.drop_duplicates(subset=['SeriesNumber', 'SeriesName'])\n",
    "    metadata_output = os.path.join(OUTPUT_DIR, 'clarksons_series_metadata.csv')\n",
    "    series_metadata_df.to_csv(metadata_output, index=False)\n",
    "    print(f\"\\nSaved series metadata to: {metadata_output}\")\n",
    "    print(f\"  Total series documented: {len(series_metadata_df)}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No Clarksons data was successfully processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLARKSONS SERIES METADATA VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Total series documented: 634\n",
      "Series with valid numbers (not 'Unknown'): 634\n",
      "Series with 'Unknown' numbers: 0\n",
      "\n",
      "Frequency distribution:\n",
      "Frequency\n",
      "Weekly     475\n",
      "Monthly     91\n",
      "Daily       68\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample records (first 10):\n",
      " SeriesNumber                                                                  Indicator Frequency\n",
      "        30235                                         Capesize Bulkcarrier Deliveries_No   Monthly\n",
      "        30236                                        Capesize Bulkcarrier Deliveries_DWT   Monthly\n",
      "       554898                          Average Capesize 2020s-built (Eco) Earnings_$/day    Weekly\n",
      "       554906              Capesize 2020s-built (Bauxite) Kamsar - Yantai Earnings_$/day    Weekly\n",
      "       554914        Capesize 2020s-built (Eco) (Coal) Baltimore - Kandla Earnings_$/day    Weekly\n",
      "       554918     Capesize 2020s-built (Eco) (Coal) Baltimore - Rotterdam Earnings_$/day    Weekly\n",
      "       554922       Capesize 2020s-built (Eco) (Coal) Bolivar - Rotterdam Earnings_$/day    Weekly\n",
      "       554926 Capesize 2020s-built (Eco) (Coal) Hay Point - Gwangyang 17m Earnings_$/day    Weekly\n",
      "       554930       Capesize 2020s-built (Eco) (Coal) Hay Point - Qingdao Earnings_$/day    Weekly\n",
      "       554934     Capesize 2020s-built (Eco) (Coal) Hay Point - Rotterdam Earnings_$/day    Weekly\n",
      "\n",
      "WARNING: Found 4 records with duplicate series numbers\n",
      "This may indicate series measured in different units/frequencies.\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the metadata\n",
    "if os.path.exists(os.path.join(OUTPUT_DIR, 'clarksons_series_metadata.csv')):\n",
    "    metadata = pd.read_csv(os.path.join(OUTPUT_DIR, 'clarksons_series_metadata.csv'))\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CLARKSONS SERIES METADATA VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal series documented: {len(metadata)}\")\n",
    "    print(f\"Series with valid numbers (not 'Unknown'): {(metadata['SeriesNumber'] != 'Unknown').sum()}\")\n",
    "    print(f\"Series with 'Unknown' numbers: {(metadata['SeriesNumber'] == 'Unknown').sum()}\")\n",
    "    \n",
    "    print(\"\\nFrequency distribution:\")\n",
    "    print(metadata['Frequency'].value_counts())\n",
    "    \n",
    "    print(\"\\nSample records (first 10):\")\n",
    "    print(metadata[['SeriesNumber', 'Indicator', 'Frequency']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Check for duplicate series numbers\n",
    "    dup_series = metadata[metadata.duplicated(subset=['SeriesNumber'], keep=False)]\n",
    "    if len(dup_series) > 0:\n",
    "        print(f\"\\nWARNING: Found {len(dup_series)} records with duplicate series numbers\")\n",
    "        print(\"This may indicate series measured in different units/frequencies.\")\n",
    "    else:\n",
    "        print(\"\\nAll series numbers are unique (GOOD).\")\n",
    "else:\n",
    "    print(\"Metadata file not found - series number extraction may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MASTER CALENDAR CREATION\n",
      "================================================================================\n",
      "\n",
      "Master Calendar created with 1149 unique business dates\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MASTER CALENDAR CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "master_dates = baltic_spot['Date'].unique()\n",
    "master_calendar = pd.DataFrame({'Date': sorted(master_dates)})\n",
    "\n",
    "print(f\"\\nMaster Calendar created with {len(master_calendar)} unique business dates\")\n",
    "print(f\"  Date range: {master_calendar['Date'].min()} to {master_calendar['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY ASSURANCE\n",
      "================================================================================\n",
      "\n",
      "1. Missing Value Analysis:\n",
      "   Bunker: 0.00% missing values\n",
      "   Baltic: 25.34% missing values\n",
      "   BFA: 2.27% missing values\n",
      "   Clarksons: 82.31% average missing (across 634 series)\n",
      "              Max missing in any series: 97.97%\n",
      "\n",
      "2. Duplicate Date Check:\n",
      "   Bunker: 0 duplicate dates\n",
      "   Baltic: 0 duplicate dates\n",
      "   BFA: 0 duplicate dates\n",
      "\n",
      "3. Study Period Compliance (Start Date: 2021-03-01):\n",
      "   Bunker: 2021-03-01 [PASS]\n",
      "   Baltic: 2021-03-01 [PASS]\n",
      "   BFA: 2021-03-01 [PASS]\n",
      "   Clarksons: 2021-03-01 [PASS]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSURANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bunker_canonical = pd.read_csv(os.path.join(OUTPUT_DIR, 'bunker_canonical.csv'), parse_dates=['Date'])\n",
    "baltic_canonical = pd.read_csv(os.path.join(OUTPUT_DIR, 'baltic_spot_canonical.csv'), parse_dates=['Date'])\n",
    "bfa_canonical = pd.read_csv(os.path.join(OUTPUT_DIR, 'bfa_wide_canonical.csv'), parse_dates=['Date'])\n",
    "clarksons_canonical = pd.read_csv(os.path.join(OUTPUT_DIR, 'clarksons_canonical.csv'), parse_dates=['Date'])\n",
    "\n",
    "print(\"\\n1. Missing Value Analysis:\")\n",
    "for name, df in [(\"Bunker\", bunker_canonical), (\"Baltic\", baltic_canonical), (\"BFA\", bfa_canonical)]:\n",
    "    missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"   {name}: {missing_pct:.2f}% missing values\")\n",
    "\n",
    "# Handle duplicate (Date, SeriesName) combinations before pivoting\n",
    "# Keep last value for each duplicate (most recent data if multiple files have same series)\n",
    "clarksons_canonical_deduplicated = clarksons_canonical.drop_duplicates(subset=['Date', 'SeriesName'], keep='last')\n",
    "duplicates_removed = len(clarksons_canonical) - len(clarksons_canonical_deduplicated)\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"\\n   Note: Removed {duplicates_removed} duplicate (Date, SeriesName) entries from Clarksons data\")\n",
    "\n",
    "clarksons_wide = clarksons_canonical_deduplicated.pivot(index='Date', columns='SeriesName', values='Value')\n",
    "clarksons_missing = (clarksons_wide.isnull().sum() / len(clarksons_wide)) * 100\n",
    "print(f\"   Clarksons: {clarksons_missing.mean():.2f}% average missing (across {len(clarksons_missing)} series)\")\n",
    "print(f\"              Max missing in any series: {clarksons_missing.max():.2f}%\")\n",
    "\n",
    "print(\"\\n2. Duplicate Date Check:\")\n",
    "for name, df in [(\"Bunker\", bunker_canonical), (\"Baltic\", baltic_canonical), (\"BFA\", bfa_canonical)]:\n",
    "    duplicates = df['Date'].duplicated().sum()\n",
    "    print(f\"   {name}: {duplicates} duplicate dates\")\n",
    "\n",
    "print(\"\\n3. Study Period Compliance (Start Date: 2021-03-01):\")\n",
    "for name, df in [(\"Bunker\", bunker_canonical), (\"Baltic\", baltic_canonical), (\"BFA\", bfa_canonical), (\"Clarksons\", clarksons_canonical)]:\n",
    "    min_date = df['Date'].min()\n",
    "    compliant = \"PASS\" if min_date >= pd.Timestamp('2021-03-01') else \"FAIL\"\n",
    "    print(f\"   {name}: {min_date.strftime('%Y-%m-%d')} [{compliant}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CANONICAL DATASETS SUMMARY\n",
      "================================================================================\n",
      "              Dataset   Rows  Columns/Series Date Range Start Date Range End\n",
      "     Bunker Canonical   1201               2       2021-03-01     2025-10-10\n",
      "Baltic Spot Canonical   1149              45       2021-03-01     2025-10-06\n",
      "   BFA Wide Canonical   1165              22       2021-03-01     2025-10-10\n",
      "  Clarksons Canonical 188161             634       2021-03-01     2025-10-03\n",
      "\n",
      "================================================================================\n",
      "DATA ACQUISITION COMPLETE\n",
      "All datasets loaded, processed, and verified successfully.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Dataset': ['Bunker Canonical', 'Baltic Spot Canonical', 'BFA Wide Canonical', 'Clarksons Canonical'],\n",
    "    'Rows': [len(bunker_canonical), len(baltic_canonical), len(bfa_canonical), len(clarksons_canonical)],\n",
    "    'Columns/Series': [\n",
    "        bunker_canonical.shape[1]-1, \n",
    "        baltic_canonical.shape[1]-1, \n",
    "        bfa_canonical.shape[1]-1, \n",
    "        clarksons_canonical['SeriesName'].nunique()\n",
    "    ],\n",
    "    'Date Range Start': [\n",
    "        bunker_canonical['Date'].min().strftime('%Y-%m-%d'),\n",
    "        baltic_canonical['Date'].min().strftime('%Y-%m-%d'),\n",
    "        bfa_canonical['Date'].min().strftime('%Y-%m-%d'),\n",
    "        clarksons_canonical['Date'].min().strftime('%Y-%m-%d')\n",
    "    ],\n",
    "    'Date Range End': [\n",
    "        bunker_canonical['Date'].max().strftime('%Y-%m-%d'),\n",
    "        baltic_canonical['Date'].max().strftime('%Y-%m-%d'),\n",
    "        bfa_canonical['Date'].max().strftime('%Y-%m-%d'),\n",
    "        clarksons_canonical['Date'].max().strftime('%Y-%m-%d')\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CANONICAL DATASETS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA ACQUISITION COMPLETE\")\n",
    "print(\"All datasets loaded, processed, and verified successfully.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VIF-INFORMED CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Correlation Thresholds:\n",
      "  High (drop one feature): r > 0.95\n",
      "  Medium (review): 0.85 < r  0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VIF-INFORMED CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "CORR_THRESHOLD_HIGH = 0.95\n",
    "CORR_THRESHOLD_MEDIUM = 0.85\n",
    "\n",
    "print(f\"\\nCorrelation Thresholds:\")\n",
    "print(f\"  High (drop one feature): r > {CORR_THRESHOLD_HIGH}\")\n",
    "print(f\"  Medium (review): {CORR_THRESHOLD_MEDIUM} < r  {CORR_THRESHOLD_HIGH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] BFA Forward Curve Features - Term Structure Exemption\n",
      "================================================================================\n",
      "  P1EA_82 tenors: 11 contracts\n",
      "  P3EA_82 tenors: 11 contracts\n",
      "  Total BFA features: 22\n",
      "\n",
      "  ALL BFA features retained (term structure exemption).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] BFA Forward Curve Features - Term Structure Exemption\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bfa_features = [col for col in bfa_canonical.columns if col != 'Date']\n",
    "p1_bfa_features = [col for col in bfa_features if col.startswith('P1EA_82')]\n",
    "p3_bfa_features = [col for col in bfa_features if col.startswith('P3EA_82')]\n",
    "\n",
    "print(f\"  P1EA_82 tenors: {len(p1_bfa_features)} contracts\")\n",
    "print(f\"  P3EA_82 tenors: {len(p3_bfa_features)} contracts\")\n",
    "print(f\"  Total BFA features: {len(bfa_features)}\")\n",
    "print(\"\\n  ALL BFA features retained (term structure exemption).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Baltic Exchange feature correlations...\n",
      "  Analyzing 43 Baltic features (labels excluded)...\n",
      "  Found 148 feature pairs with correlation > 0.85\n",
      "  Identified 71 pairs with correlation > 0.95 for VIF reduction.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing Baltic Exchange feature correlations...\")\n",
    "\n",
    "baltic_features = [col for col in baltic_canonical.columns if col not in ['Date', 'P1A_82', 'P3A_82']]\n",
    "baltic_feature_df = baltic_canonical[baltic_features].dropna(how='all', axis=1)\n",
    "print(f\"  Analyzing {len(baltic_feature_df.columns)} Baltic features (labels excluded)...\")\n",
    "\n",
    "baltic_corr = baltic_feature_df.corr()\n",
    "\n",
    "def compare_null_counts(df, feature1, feature2):\n",
    "    \"\"\"Compare nulls and apply tie-breaker for Panamax-specific features.\"\"\"\n",
    "    null1 = df[feature1].isnull().sum()\n",
    "    null2 = df[feature2].isnull().sum()\n",
    "    if null1 < null2: return (feature1, feature2, null1, null2)\n",
    "    if null2 < null1: return (feature2, feature1, null2, null1)\n",
    "    \n",
    "    # Tie-breaker logic\n",
    "    f1_is_panamax = feature1.startswith('P') or feature1 == 'BPI'\n",
    "    f2_is_panamax = feature2.startswith('P') or feature2 == 'BPI'\n",
    "    if f1_is_panamax and not f2_is_panamax: return (feature1, feature2, null1, null2)\n",
    "    if f2_is_panamax and not f1_is_panamax: return (feature2, feature1, null2, null1)\n",
    "    \n",
    "    return (feature1, feature2, null1, null2) # Default tie-breaker\n",
    "\n",
    "baltic_high_corr_pairs = []\n",
    "baltic_null_comparison_results = []\n",
    "for i in range(len(baltic_corr.columns)):\n",
    "    for j in range(i+1, len(baltic_corr.columns)):\n",
    "        corr_val = abs(baltic_corr.iloc[i, j])\n",
    "        if corr_val > CORR_THRESHOLD_MEDIUM:\n",
    "            feature1 = baltic_corr.columns[i]\n",
    "            feature2 = baltic_corr.columns[j]\n",
    "            action = 'DROP_ONE' if corr_val > CORR_THRESHOLD_HIGH else 'REVIEW'\n",
    "            if corr_val > CORR_THRESHOLD_HIGH:\n",
    "                kept, dropped, nulls_kept, nulls_dropped = compare_null_counts(baltic_canonical, feature1, feature2)\n",
    "                baltic_null_comparison_results.append({\n",
    "                    'Feature_Kept': kept, 'Feature_Dropped': dropped, 'Correlation': corr_val,\n",
    "                    'Nulls_Kept': nulls_kept, 'Nulls_Dropped': nulls_dropped\n",
    "                })\n",
    "            baltic_high_corr_pairs.append({'Feature_1': feature1, 'Feature_2': feature2, 'Correlation': corr_val, 'Action': action})\n",
    "\n",
    "baltic_high_corr_df = pd.DataFrame(baltic_high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "baltic_null_comparison_df = pd.DataFrame(baltic_null_comparison_results).sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(f\"  Found {len(baltic_high_corr_df)} feature pairs with correlation > {CORR_THRESHOLD_MEDIUM}\")\n",
    "print(f\"  Identified {len(baltic_null_comparison_df)} pairs with correlation > {CORR_THRESHOLD_HIGH} for VIF reduction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BALTIC: CLOSED-FORM FEATURE DECISIONS\n",
      "================================================================================\n",
      "\n",
      "Expected (Baltic features excluding labels): 43\n",
      "Retained: 20 | Dropped_Correlation: 23\n",
      "Check sum: 43 == 43\n",
      "\n",
      "[SAVED] Baltic pairs to: data/processed/pipeline\\correlation_baltic_pairs_full.csv\n",
      "[SAVED] Baltic retained to: data/processed/pipeline\\features_baltic_retained.csv\n",
      "[SAVED] Baltic dropped to: data/processed/pipeline\\features_baltic_dropped.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== BALTIC: Closed-form retained/dropped with reconciliation ====\n",
    "print(\"=\"*80)\n",
    "print(\"BALTIC: CLOSED-FORM FEATURE DECISIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Universe (exclude Date and labels)\n",
    "baltic_universe = [c for c in baltic_canonical.columns if c not in ['Date','P1A_82','P3A_82']]\n",
    "expected_baltic = len(baltic_universe)  # should be 43\n",
    "\n",
    "# Rebuild pair list with overlap counts (guard against spurious r)\n",
    "pairs = []\n",
    "X = baltic_canonical[baltic_universe]\n",
    "cols = X.columns.tolist()\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        a, b = cols[i], cols[j]\n",
    "        tmp = X[[a,b]].dropna()\n",
    "        if len(tmp) < 10:\n",
    "            continue\n",
    "        r = tmp[a].corr(tmp[b])\n",
    "        if pd.notna(r) and abs(r) > CORR_THRESHOLD_MEDIUM:\n",
    "            pairs.append({'Feature_1': a, 'Feature_2': b,\n",
    "                          'Correlation': abs(r), 'N_overlap': len(tmp),\n",
    "                          'Action': 'DROP_ONE' if abs(r) > CORR_THRESHOLD_HIGH else 'REVIEW'})\n",
    "\n",
    "baltic_pairs_df = pd.DataFrame(pairs).sort_values(['Correlation','N_overlap'], ascending=[False, False])\n",
    "baltic_pairs_df.to_csv(os.path.join(OUTPUT_DIR, \"correlation_baltic_pairs_full.csv\"), index=False)\n",
    "\n",
    "def tie_break_baltic(df, f1, f2):\n",
    "    \"\"\"\n",
    "    Determine which feature to keep when both are highly correlated.\n",
    "    Priority:\n",
    "    1. Fewer null values\n",
    "    2. Panamax-specific features (starts with 'P' or is 'BPI') when nulls are equal\n",
    "    3. Alphabetical order as final fallback\n",
    "    \"\"\"\n",
    "    n1, n2 = df[f1].isna().sum(), df[f2].isna().sum()\n",
    "    \n",
    "    # Primary criterion: data completeness\n",
    "    if n1 < n2:\n",
    "        return f1, f2  # Keep f1 (fewer nulls), drop f2\n",
    "    if n2 < n1:\n",
    "        return f2, f1  # Keep f2 (fewer nulls), drop f1\n",
    "    \n",
    "    # Secondary criterion: Panamax priority (only when nulls are equal)\n",
    "    f1_is_panamax = f1.startswith('P') or f1 == 'BPI'\n",
    "    f2_is_panamax = f2.startswith('P') or f2 == 'BPI'\n",
    "    \n",
    "    if f1_is_panamax and not f2_is_panamax:\n",
    "        return f1, f2  # Keep Panamax feature f1\n",
    "    if f2_is_panamax and not f1_is_panamax:\n",
    "        return f2, f1  # Keep Panamax feature f2\n",
    "    \n",
    "    # Tertiary criterion: deterministic alphabetical fallback\n",
    "    return (min(f1, f2), max(f1, f2))\n",
    "\n",
    "to_drop, kept = set(), set()\n",
    "for _, r in baltic_pairs_df[baltic_pairs_df['Action']=='DROP_ONE'].iterrows():\n",
    "    a, b = r['Feature_1'], r['Feature_2']\n",
    "    if a in to_drop or b in to_drop: \n",
    "        continue\n",
    "    if a in kept and b not in kept: \n",
    "        to_drop.add(b); continue\n",
    "    if b in kept and a not in kept: \n",
    "        to_drop.add(a); continue\n",
    "    if a in kept and b in kept:\n",
    "        k, d = tie_break_baltic(baltic_canonical, a, b)\n",
    "        if d in kept: kept.remove(d)\n",
    "        to_drop.add(d); continue\n",
    "    k, d = tie_break_baltic(baltic_canonical, a, b)\n",
    "    kept.add(k); to_drop.add(d)\n",
    "\n",
    "baltic_dropped = sorted(to_drop)\n",
    "baltic_retained = sorted(set(baltic_universe) - to_drop)\n",
    "\n",
    "pd.DataFrame({'Feature': baltic_retained}).to_csv(os.path.join(OUTPUT_DIR,\"features_baltic_retained.csv\"), index=False)\n",
    "pd.DataFrame({'Feature': baltic_dropped}).to_csv(os.path.join(OUTPUT_DIR,\"features_baltic_dropped.csv\"), index=False)\n",
    "\n",
    "print(f\"\\nExpected (Baltic features excluding labels): {expected_baltic}\")\n",
    "print(f\"Retained: {len(baltic_retained)} | Dropped_Correlation: {len(baltic_dropped)}\")\n",
    "print(f\"Check sum: {len(baltic_retained) + len(baltic_dropped)} == {expected_baltic}\")\n",
    "print(f\"\\n[SAVED] Baltic pairs to: {os.path.join(OUTPUT_DIR, 'correlation_baltic_pairs_full.csv')}\")\n",
    "print(f\"[SAVED] Baltic retained to: {os.path.join(OUTPUT_DIR, 'features_baltic_retained.csv')}\")\n",
    "print(f\"[SAVED] Baltic dropped to: {os.path.join(OUTPUT_DIR, 'features_baltic_dropped.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Clarksons feature correlations by category...\n",
      "  Analyzing 20 Clarksons categories...\n",
      "  Category correlation analysis complete.\n",
      "\n",
      "  Top 5 categories by redundancy rate:\n",
      "                     Category  Series_Count  High_Corr_Pairs  Redundancy_Rate\n",
      "      port_congestion_turkish             3                3         1.000000\n",
      "      economic_indicators_bfi             7               13         0.619048\n",
      "economic_indicators_commodity             3                1         0.333333\n",
      "         bulkers_data_panamax             9                7         0.194444\n",
      "        bulkers_data_capesize            13               11         0.141026\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing Clarksons feature correlations by category...\")\n",
    "\n",
    "def clean_clarksons_feature_name(feature_name):\n",
    "    import re\n",
    "    return re.sub(r'_Unnamed:.*$', '', feature_name)\n",
    "\n",
    "def extract_category(series_name):\n",
    "    parts = series_name.split('_')\n",
    "    if len(parts) >= 3: return f\"{parts[0]}_{parts[1]}_{parts[2]}\"\n",
    "    return series_name\n",
    "\n",
    "series_inventory = clarksons_canonical.groupby('SeriesName').agg({'Date': ['min', 'max', 'count'], 'Value': lambda x: x.notna().sum()}).reset_index()\n",
    "series_inventory.columns = ['SeriesName', 'First_Date', 'Last_Date', 'Total_Rows', 'Non_Null_Count']\n",
    "series_inventory['SeriesName_Clean'] = series_inventory['SeriesName'].apply(clean_clarksons_feature_name)\n",
    "series_inventory['Category'] = series_inventory['SeriesName'].apply(extract_category)\n",
    "\n",
    "categories = series_inventory['Category'].unique()\n",
    "print(f\"  Analyzing {len(categories)} Clarksons categories...\")\n",
    "clarksons_correlation_summary = []\n",
    "\n",
    "for category in categories:\n",
    "    category_series = series_inventory[series_inventory['Category'] == category]['SeriesName'].tolist()\n",
    "    if len(category_series) < 2: continue\n",
    "    category_data = clarksons_canonical[clarksons_canonical['SeriesName'].isin(category_series)]\n",
    "    category_wide = category_data.pivot(index='Date', columns='SeriesName', values='Value')\n",
    "    valid_cols = category_wide.columns[category_wide.isnull().mean() < 0.5]\n",
    "    category_wide = category_wide[valid_cols]\n",
    "    if len(category_wide.columns) < 2: continue\n",
    "    category_corr = category_wide.corr()\n",
    "    high_corr_count = 0\n",
    "    for i in range(len(category_corr.columns)):\n",
    "        for j in range(i+1, len(category_corr.columns)):\n",
    "            if abs(category_corr.iloc[i, j]) > CORR_THRESHOLD_HIGH: high_corr_count += 1\n",
    "    clarksons_correlation_summary.append({\n",
    "        'Category': category, 'Series_Count': len(category_wide.columns), 'High_Corr_Pairs': high_corr_count,\n",
    "        'Redundancy_Rate': high_corr_count / (len(category_wide.columns) * (len(category_wide.columns) - 1) / 2) if len(category_wide.columns) > 1 else 0\n",
    "    })\n",
    "\n",
    "clarksons_summary_df = pd.DataFrame(clarksons_correlation_summary).sort_values('Redundancy_Rate', ascending=False)\n",
    "print(f\"  Category correlation analysis complete.\")\n",
    "print(\"\\n  Top 5 categories by redundancy rate:\")\n",
    "if len(clarksons_summary_df) > 0:\n",
    "    print(clarksons_summary_df.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLARKSONS: CLOSED-FORM SERIES DECISIONS\n",
      "================================================================================\n",
      "\n",
      "Inferring frequency for each series...\n",
      "  Frequency distribution:\n",
      "Frequency\n",
      "Daily       68\n",
      "Monthly     91\n",
      "Weekly     475\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Excluded due to low completeness (<0.5): 0\n",
      "  Candidates for correlation analysis: 634\n",
      "\n",
      "  Performing category-wise correlation analysis...\n",
      "\n",
      "================================================================================\n",
      "Expected (Clarksons series): 634\n",
      "\n",
      "Decision Breakdown:\n",
      "Decision\n",
      "Dropped_Correlation    492\n",
      "Retained               142\n",
      "Name: Count, dtype: int64\n",
      "\n",
      "Check total: 634 == 634\n",
      "\n",
      "[SAVED] Clarksons pairs to: data/processed/pipeline\\correlation_clarksons_pairs_full.csv\n",
      "[SAVED] Clarksons decisions to: data/processed/pipeline\\features_clarksons_decisions.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== CLARKSONS: Closed-form decisions with frequency-aware sparsity ====\n",
    "print(\"=\"*80)\n",
    "print(\"CLARKSONS: CLOSED-FORM SERIES DECISIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "COMPLETENESS_CUTOFF = 0.50  # Require at least 50% of expected observations\n",
    "\n",
    "# Build series inventory with frequency inference\n",
    "series_inventory = clarksons_canonical.groupby('SeriesName').agg(\n",
    "    First_Date=('Date','min'), \n",
    "    Last_Date=('Date','max'),\n",
    "    Actual_Obs=('Date','count')\n",
    ").reset_index()\n",
    "\n",
    "series_inventory['Category'] = series_inventory['SeriesName'].apply(\n",
    "    lambda s: '_'.join(s.split('_')[:3]) if len(s.split('_'))>=3 else s\n",
    ")\n",
    "\n",
    "expected_clx = len(series_inventory)  # should be 634\n",
    "\n",
    "# Infer frequency for each series from actual observation dates\n",
    "def infer_series_frequency(series_name):\n",
    "    dates = clarksons_canonical[clarksons_canonical['SeriesName']==series_name]['Date'].sort_values()\n",
    "    if len(dates) < 2:\n",
    "        return 'Unknown'\n",
    "    diffs = dates.diff().dropna()\n",
    "    median_diff = diffs.median()\n",
    "    \n",
    "    if median_diff <= pd.Timedelta(days=3): \n",
    "        return 'Daily'\n",
    "    elif median_diff <= pd.Timedelta(days=10): \n",
    "        return 'Weekly'\n",
    "    elif median_diff <= pd.Timedelta(days=45): \n",
    "        return 'Monthly'\n",
    "    elif median_diff <= pd.Timedelta(days=120): \n",
    "        return 'Quarterly'\n",
    "    else: \n",
    "        return 'Yearly'\n",
    "\n",
    "print(\"\\nInferring frequency for each series...\")\n",
    "series_inventory['Frequency'] = series_inventory['SeriesName'].apply(infer_series_frequency)\n",
    "\n",
    "# Calculate expected observations based on frequency and date range\n",
    "def calc_expected_obs(first_date, last_date, frequency):\n",
    "    days = (last_date - first_date).days\n",
    "    if frequency == 'Daily': \n",
    "        return max(days, 1)\n",
    "    elif frequency == 'Weekly': \n",
    "        return max(days // 7, 1)\n",
    "    elif frequency == 'Monthly': \n",
    "        return max(days // 30, 1)\n",
    "    elif frequency == 'Quarterly': \n",
    "        return max(days // 90, 1)\n",
    "    elif frequency == 'Yearly': \n",
    "        return max(days // 365, 1)\n",
    "    else: \n",
    "        return 1\n",
    "\n",
    "series_inventory['Expected_Obs'] = series_inventory.apply(\n",
    "    lambda row: calc_expected_obs(row['First_Date'], row['Last_Date'], row['Frequency']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate completeness rate (actual vs expected for this frequency)\n",
    "series_inventory['Completeness'] = series_inventory['Actual_Obs'] / series_inventory['Expected_Obs']\n",
    "# Cap at 1.0 for cases where actual > expected (e.g., due to rounding)\n",
    "series_inventory['Completeness'] = series_inventory['Completeness'].clip(upper=1.0)\n",
    "\n",
    "print(f\"  Frequency distribution:\")\n",
    "print(series_inventory['Frequency'].value_counts().sort_index())\n",
    "\n",
    "# Make decision based on completeness (not absolute missing rate)\n",
    "universe = series_inventory[['SeriesName','Category','Frequency','Completeness']].copy()\n",
    "universe['Decision'] = np.where(\n",
    "    universe['Completeness'] < COMPLETENESS_CUTOFF, \n",
    "    'Excluded_Sparsity', \n",
    "    'Candidate'\n",
    ")\n",
    "\n",
    "print(f\"\\n  Excluded due to low completeness (<{COMPLETENESS_CUTOFF}): {(universe['Decision']=='Excluded_Sparsity').sum()}\")\n",
    "print(f\"  Candidates for correlation analysis: {(universe['Decision']=='Candidate').sum()}\")\n",
    "\n",
    "# Category-wise correlation only on Candidates\n",
    "candidates = universe[universe['Decision']=='Candidate'].copy()\n",
    "dropped_rows, kept_rows, pair_rows = [], [], []\n",
    "\n",
    "# Build wide format only for correlation analysis (not for sparsity assessment)\n",
    "clx_wide = clarksons_canonical.pivot(index='Date', columns='SeriesName', values='Value')\n",
    "\n",
    "def resolve_category(cat, cand_df):\n",
    "    ser_list = cand_df['SeriesName'].tolist()\n",
    "    wide = clx_wide[ser_list]\n",
    "    # Drop columns with all-null within this slice\n",
    "    wide = wide.loc[:, wide.isnull().mean() < 1.0]\n",
    "    if wide.shape[1] <= 1:\n",
    "        for s in wide.columns:\n",
    "            kept_rows.append({'SeriesName': s, 'Category': cat, 'Decision': 'Retained'})\n",
    "        return\n",
    "\n",
    "    # Build pairs\n",
    "    pairs = []\n",
    "    cols = list(wide.columns)\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            a, b = cols[i], cols[j]\n",
    "            tmp = wide[[a,b]].dropna()\n",
    "            if len(tmp) < 10:  # insufficient overlap\n",
    "                continue\n",
    "            r = tmp[a].corr(tmp[b])\n",
    "            if pd.notna(r) and abs(r) > CORR_THRESHOLD_MEDIUM:\n",
    "                pairs.append({'Category': cat, 'Series_1': a, 'Series_2': b,\n",
    "                              'Correlation': abs(r), 'N_overlap': len(tmp),\n",
    "                              'Action': 'DROP_ONE' if abs(r) > CORR_THRESHOLD_HIGH else 'REVIEW'})\n",
    "    \n",
    "    if pairs:\n",
    "        dfp = pd.DataFrame(pairs).sort_values(['Correlation','N_overlap'], ascending=[False, False])\n",
    "        pair_rows.append(dfp)\n",
    "        # Greedy drop for > high threshold\n",
    "        to_drop_cat, kept_cat = set(), set()\n",
    "        \n",
    "        def tie_break(a, b):\n",
    "            # Use completeness instead of null count for tie-breaking\n",
    "            comp_a = universe[universe['SeriesName']==a]['Completeness'].iloc[0]\n",
    "            comp_b = universe[universe['SeriesName']==b]['Completeness'].iloc[0]\n",
    "            if comp_a > comp_b: \n",
    "                return a, b\n",
    "            if comp_b > comp_a: \n",
    "                return b, a\n",
    "            return (min(a,b), max(a,b))\n",
    "        \n",
    "        for _, r in dfp[dfp['Action']=='DROP_ONE'].iterrows():\n",
    "            a, b = r['Series_1'], r['Series_2']\n",
    "            if a in to_drop_cat or b in to_drop_cat: \n",
    "                continue\n",
    "            if a in kept_cat and b not in kept_cat: \n",
    "                to_drop_cat.add(b); continue\n",
    "            if b in kept_cat and a not in kept_cat: \n",
    "                to_drop_cat.add(a); continue\n",
    "            if a in kept_cat and b in kept_cat:\n",
    "                k, d = tie_break(a,b)\n",
    "                if d in kept_cat: kept_cat.remove(d)\n",
    "                to_drop_cat.add(d); continue\n",
    "            k, d = tie_break(a,b)\n",
    "            kept_cat.add(k); to_drop_cat.add(d)\n",
    "        \n",
    "        final_keep = sorted(set(cols) - to_drop_cat)\n",
    "        kept_rows.extend([{'SeriesName': s, 'Category': cat, 'Decision':'Retained'} for s in final_keep])\n",
    "        dropped_rows.extend([{'SeriesName': s, 'Category': cat, 'Decision':'Dropped_Correlation'} for s in sorted(to_drop_cat)])\n",
    "    else:\n",
    "        # No high-corr pairs; retain all candidates in this category\n",
    "        kept_rows.extend([{'SeriesName': s, 'Category': cat, 'Decision':'Retained'} for s in cols])\n",
    "\n",
    "print(\"\\n  Performing category-wise correlation analysis...\")\n",
    "for cat in candidates['Category'].unique():\n",
    "    sub = candidates[candidates['Category']==cat]\n",
    "    resolve_category(cat, sub)\n",
    "\n",
    "# Save correlation pairs\n",
    "clx_pairs_df = pd.concat(pair_rows, ignore_index=True) if len(pair_rows) else pd.DataFrame(\n",
    "    columns=['Category','Series_1','Series_2','Correlation','N_overlap','Action']\n",
    ")\n",
    "clx_pairs_df.to_csv(os.path.join(OUTPUT_DIR,\"correlation_clarksons_pairs_full.csv\"), index=False)\n",
    "\n",
    "clx_kept_df = pd.DataFrame(kept_rows)\n",
    "clx_drop_df = pd.DataFrame(dropped_rows)\n",
    "\n",
    "# Assemble final decisions (every one of the 634 gets a status)\n",
    "final_decisions = pd.concat([\n",
    "    universe[universe['Decision']=='Excluded_Sparsity'][['SeriesName','Category','Decision','Frequency','Completeness']],\n",
    "    clx_kept_df.merge(universe[['SeriesName','Frequency','Completeness']], on='SeriesName', how='left'),\n",
    "    clx_drop_df.merge(universe[['SeriesName','Frequency','Completeness']], on='SeriesName', how='left')\n",
    "], ignore_index=True)\n",
    "\n",
    "final_decisions = final_decisions[['SeriesName','Category','Decision','Frequency','Completeness']].drop_duplicates()\n",
    "\n",
    "# Persist\n",
    "final_path = os.path.join(OUTPUT_DIR, \"features_clarksons_decisions.csv\")\n",
    "final_decisions.to_csv(final_path, index=False)\n",
    "\n",
    "# Reconciliation audit\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Expected (Clarksons series): {expected_clx}\")\n",
    "print(f\"\\nDecision Breakdown:\")\n",
    "print(final_decisions['Decision'].value_counts().rename('Count'))\n",
    "print(f\"\\nCheck total: {len(final_decisions)} == {expected_clx}\")\n",
    "print(f\"\\n[SAVED] Clarksons pairs to: {os.path.join(OUTPUT_DIR, 'correlation_clarksons_pairs_full.csv')}\")\n",
    "print(f\"[SAVED] Clarksons decisions to: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING CORRELATION ANALYSIS RESULTS\n",
      "================================================================================\n",
      "[SAVED] Baltic redundancy report to: data/processed/pipeline\\correlation_baltic_redundancy_report.csv\n",
      "[SAVED] Clarksons category summary to: data/processed/pipeline\\correlation_clarksons_category_summary.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CORRELATION ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baltic_null_comparison_df.to_csv(os.path.join(OUTPUT_DIR, \"correlation_baltic_redundancy_report.csv\"), index=False)\n",
    "print(f\"[SAVED] Baltic redundancy report to: {os.path.join(OUTPUT_DIR, 'correlation_baltic_redundancy_report.csv')}\")\n",
    "\n",
    "clarksons_summary_df.to_csv(os.path.join(OUTPUT_DIR, \"correlation_clarksons_category_summary.csv\"), index=False)\n",
    "print(f\"[SAVED] Clarksons category summary to: {os.path.join(OUTPUT_DIR, 'correlation_clarksons_category_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTATIONAL ENVIRONMENT\n",
      "================================================================================\n",
      "Execution Date: 2025-10-18 16:36:27\n",
      "Python Version: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26100-SP0\n",
      "\n",
      "Package Versions:\n",
      "  Pandas: 2.3.2\n",
      "  NumPy: 1.26.4\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "END OF NOTEBOOK\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPUTATIONAL ENVIRONMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"\\nPackage Versions:\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF NOTEBOOK\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
