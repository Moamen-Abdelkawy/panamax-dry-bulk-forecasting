{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 02: Comprehensive Feature Engineering (REVISED - Round 2)\n\n**Created:** October 31, 2025  \n**Revised:** October 31, 2025 (Post-validation fixes)  \n**Purpose:** Create comprehensive feature library (~115 features) with empirically validated transformation variants  \n**Approach:** Data-driven (create all features ‚Üí empirical validation ‚Üí allocate to CORE/ML)\n\n---\n\n## Why This Approach?\n\n**Round 1 Problem:** Pre-selected features based on theory failed validation\n- Example: `Grain_Trade_YoY` had -67.84 importance (harmful!)\n- P1A validation failed due to poor feature quality\n\n**Round 2 Solution:** Empirical feature selection\n1. Engineer ALL potentially useful transformations (~115 features)\n2. Use Random Forest + VIF to empirically validate\n3. Allocate best performers to CORE (ARIMAX) vs ML (XGBoost)\n\n---\n\n## Transformation Strategy (REVISED)\n\n**Decisions Applied:**\n- **D1-1A**: ‚ùå Removed FFA spreads using labels as inputs (-4 features)\n- **D2-2B**: ‚ùå Skip vol30 for 8 annual trade features (-8 features)\n- **D3-3A**: ‚ùå Remove mom transformation entirely (-24 features)\n- **D4-4B**: ‚úÖ Use 5 transformations (level, diff, pct, yoy, vol30)\n- **D5-5A**: ‚úÖ Keep YoY transformations (valuable for freight markets)\n- **D7-7A**: ‚úÖ Uniform strategy across all features\n\nFor each raw feature, create up to 5 variants:\n\n| Transformation | Formula | Stationarity | Use Case |\n|----------------|---------|--------------|----------|\n| **Level** | `x_t` | No | XGBoost only |\n| **First Difference** | `x_t - x_{t-1}` | Yes | ARIMAX + XGBoost |\n| **Percent Change** | `(x_t - x_{t-1}) / x_{t-1} * 100` | Yes | Both |\n| **YoY Change** | `(x_t - x_{t-12}) / x_{t-12} * 100` | Yes | ARIMAX (seasonality) |\n| **Rolling Vol 30d** | `œÉ_{30}(x)` | Partial | XGBoost (regimes) |\n\n**Removed Transformations:**\n- ‚ùå **MoM** - Redundant (identical to pct)\n- ‚ùå **MA30 Deviation** - Excessive NaN (430 avg per feature, 89.94% for annual data)\n\n**Expected Results:**\n- Feature count: ~115 (down from 184)\n- Data retention: ~95% (1,100+ rows vs previous 70 rows)\n- No infinite VIF issues\n- No label leakage\n\n---\n\n## Critical Reminders\n\n‚úÖ **All input features already have 1-day lag** (from Notebook 01)  \n‚ùå **Do NOT apply additional lag** (would create 2-day lag)  \n‚úÖ **Rolling windows:** Use `center=False` (default) - backward-looking only  \n‚úÖ **Expected NaN patterns:**\n- Row 1: ALL features (from 1-day lag in Notebook 01)\n- Vol30 features: ~30 rows (rolling window)\n- YoY features: ~12 rows (12-period lookback)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:23.963274Z",
     "start_time": "2025-11-01T08:19:23.956110Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.088791Z",
     "start_time": "2025-11-01T08:19:24.045802Z"
    }
   },
   "source": [
    "# Load raw features (already lagged by 1 day)\n",
    "features_raw = pd.read_csv('data/processed/intermediate/features_raw_daily.csv', \n",
    "                            index_col='Date', parse_dates=True)\n",
    "\n",
    "# Load labels for FFA spread computation\n",
    "labels = pd.read_csv('data/processed/intermediate/labels.csv',\n",
    "                     index_col='Date', parse_dates=True)\n",
    "\n",
    "print(f\"‚úÖ Data loaded\")\n",
    "print(f\"\\nRaw features shape: {features_raw.shape}\")\n",
    "print(f\"Date range: {features_raw.index.min()} to {features_raw.index.max()}\")\n",
    "print(f\"Total days: {len(features_raw)}\")\n",
    "print(f\"\\nLabels shape: {labels.shape}\")\n",
    "print(f\"Labels: {labels.columns.tolist()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded\n",
      "\n",
      "Raw features shape: (1153, 58)\n",
      "Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "Total days: 1153\n",
      "\n",
      "Labels shape: (1153, 2)\n",
      "Labels: ['P1A_82', 'P3A_82']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.133949Z",
     "start_time": "2025-11-01T08:19:24.127800Z"
    }
   },
   "source": [
    "# Verify first row is all NaN (due to 1-day lag from Notebook 01)\n",
    "first_row_nulls = features_raw.iloc[0].isnull().sum()\n",
    "total_cols = len(features_raw.columns)\n",
    "\n",
    "print(f\"First row NULL count: {first_row_nulls}/{total_cols}\")\n",
    "if first_row_nulls == total_cols:\n",
    "    print(\"‚úÖ VERIFIED: All features have 1-day lag (row 1 is all NaN)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: Not all features are lagged properly!\")\n",
    "    print(f\"Non-null features in row 1: {features_raw.iloc[0].notna().sum()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row NULL count: 58/58\n",
      "‚úÖ VERIFIED: All features have 1-day lag (row 1 is all NaN)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.216902Z",
     "start_time": "2025-11-01T08:19:24.204007Z"
    }
   },
   "source": "def create_transformations(df, feature_name, transformations='all', verbose=False):\n    \"\"\"\n    Create multiple transformations of a feature.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe\n    feature_name : str\n        Name of feature to transform (must exist in df)\n    transformations : list or 'all'\n        Which transformations to apply. Options:\n        - 'all': Apply 5 transformations (level, diff, pct, yoy, vol30)\n        - list: e.g., ['level', 'diff', 'pct']\n    verbose : bool\n        Print transformation details\n        \n    Returns:\n    --------\n    pd.DataFrame with new transformation columns\n    \n    Transformations (Round 2 - Revised):\n    -------------------------------------\n    - level: Raw values (x_t)\n    - diff: First difference (x_t - x_{t-1})\n    - pct: Percent change ((x_t - x_{t-1}) / x_{t-1} * 100)\n    - yoy: Year-over-year change (12 periods)\n    - vol30: 30-day rolling standard deviation\n    \n    REMOVED Transformations (per decisions):\n    - mom: REMOVED (D3-3A) - Redundant with pct (identical formula)\n    - ma30_dev: REMOVED (D4-4B) - Caused excessive NaN accumulation\n    \n    Notes:\n    ------\n    ‚ö†Ô∏è Input features already have 1-day lag (from Notebook 01)\n    ‚ö†Ô∏è Rolling windows use center=False (backward-looking only)\n    \"\"\"\n    if feature_name not in df.columns:\n        raise ValueError(f\"Feature '{feature_name}' not found in dataframe\")\n    \n    results = pd.DataFrame(index=df.index)\n    feature_data = df[feature_name]\n    \n    if verbose:\n        print(f\"\\nTransforming: {feature_name}\")\n        print(f\"  Non-null values: {feature_data.notna().sum()}/{len(feature_data)}\")\n    \n    # Level (raw)\n    if transformations == 'all' or 'level' in transformations:\n        results[f'{feature_name}_level'] = feature_data\n        if verbose: print(f\"  ‚úì level\")\n    \n    # First difference\n    if transformations == 'all' or 'diff' in transformations:\n        results[f'{feature_name}_diff'] = feature_data.diff()\n        if verbose: print(f\"  ‚úì diff\")\n    \n    # Percent change\n    if transformations == 'all' or 'pct' in transformations:\n        results[f'{feature_name}_pct'] = feature_data.pct_change() * 100\n        if verbose: print(f\"  ‚úì pct\")\n    \n    # Year-over-year\n    if transformations == 'all' or 'yoy' in transformations:\n        results[f'{feature_name}_yoy'] = feature_data.pct_change(12) * 100\n        if verbose: print(f\"  ‚úì yoy\")\n    \n    # Rolling volatility\n    if transformations == 'all' or 'vol30' in transformations:\n        results[f'{feature_name}_vol30'] = feature_data.rolling(window=30, min_periods=30, center=False).std()\n        if verbose: print(f\"  ‚úì vol30\")\n    \n    if verbose:\n        print(f\"  Created {len(results.columns)} features\")\n    \n    return results\n\nprint(\"‚úÖ Transformation functions defined (REVISED - Round 2)\")\nprint(\"\\nAvailable transformations (D4-4B):\")\nprint(\"  - level: Raw values\")\nprint(\"  - diff: First difference\")\nprint(\"  - pct: Percent change\")\nprint(\"  - yoy: Year-over-year (12 periods)\")\nprint(\"  - vol30: 30-day rolling volatility\")\nprint(\"\\n‚ùå Removed transformations:\")\nprint(\"  - mom: REMOVED (D3-3A) - Identical to pct\")\nprint(\"  - ma30_dev: REMOVED (D4-4B) - Excessive NaN (430 avg per feature)\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformation functions defined (REVISED - Round 2)\n",
      "\n",
      "Available transformations (D4-4B):\n",
      "  - level: Raw values\n",
      "  - diff: First difference\n",
      "  - pct: Percent change\n",
      "  - yoy: Year-over-year (12 periods)\n",
      "  - vol30: 30-day rolling volatility\n",
      "\n",
      "‚ùå Removed transformations:\n",
      "  - mom: REMOVED (D3-3A) - Identical to pct\n",
      "  - ma30_dev: REMOVED (D4-4B) - Excessive NaN (430 avg per feature)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 Baltic & Market Indices"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.314040Z",
     "start_time": "2025-11-01T08:19:24.291021Z"
    }
   },
   "source": "# Initialize comprehensive feature set\nfeatures_comprehensive = pd.DataFrame(index=features_raw.index)\n\nprint(\"=\"*80)\nprint(\"CATEGORY 1: BALTIC & MARKET INDICES\")\nprint(\"=\"*80)\nprint(\"Applying 5 transformations (level, diff, pct, yoy, vol30)\\n\")\n\n# Features to transform\nmarket_indices = ['BPI', 'C5TC', 'P4_82', 'PDOPEX']\n\nfor feature in market_indices:\n    print(f\"Transforming: {feature}\")\n    transformed = create_transformations(features_raw, feature, transformations='all', verbose=False)\n    features_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\n    print(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\nprint(f\"\\n‚úÖ Category 1 complete: {len(features_comprehensive.columns)} features created\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CATEGORY 1: BALTIC & MARKET INDICES\n",
      "================================================================================\n",
      "Applying 5 transformations (level, diff, pct, yoy, vol30)\n",
      "\n",
      "Transforming: BPI\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: C5TC\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: P4_82\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: PDOPEX\n",
      "  ‚Üí Created 5 features\n",
      "\n",
      "‚úÖ Category 1 complete: 20 features created\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bunker Prices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.350869Z",
     "start_time": "2025-11-01T08:19:24.335050Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY 2: BUNKER PRICES\")\nprint(\"=\"*80)\nprint(\"Applying 5 transformations (pct_change and volatility are critical)\\n\")\n\nbunker_features = ['VLSFO', 'MGO']\n\nfor feature in bunker_features:\n    print(f\"Transforming: {feature}\")\n    transformed = create_transformations(features_raw, feature, transformations='all', verbose=False)\n    features_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\n    print(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\nprint(f\"\\n‚úÖ Category 2 complete: {len(features_comprehensive.columns)} total features so far\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 2: BUNKER PRICES\n",
      "================================================================================\n",
      "Applying 5 transformations (pct_change and volatility are critical)\n",
      "\n",
      "Transforming: VLSFO\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: MGO\n",
      "  ‚Üí Created 5 features\n",
      "\n",
      "‚úÖ Category 2 complete: 30 total features so far\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fleet & Supply"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.418570Z",
     "start_time": "2025-11-01T08:19:24.383360Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY 3: FLEET & SUPPLY\")\nprint(\"=\"*80)\nprint(\"Applying 5 transformations (except TC5yr - levels only due to missing data)\\n\")\n\n# Features with full transformations\nsupply_features = [\n    'Panamax_Orderbook_Pct',\n    'Panamax_Deliveries_DWT',\n    'Panamax_Idle_Pct',\n    'Capesize_Orderbook_Pct',\n    'Atlantic_Port_Calls',\n    'Panamax_Fleet_Growth_YoY'\n]\n\nfor feature in supply_features:\n    print(f\"Transforming: {feature}\")\n    transformed = create_transformations(features_raw, feature, transformations='all', verbose=False)\n    features_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\n    print(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\n# TC5yr features - levels only (sparse data)\nprint(f\"\\nTransforming: TC5yr_Atlantic (level only - sparse data)\")\nfeatures_comprehensive['TC5yr_Atlantic_level'] = features_raw['TC5yr_Atlantic']\nprint(f\"  ‚Üí Created 1 feature\")\n\nprint(f\"\\nTransforming: TC5yr_Pacific (level only - sparse data)\")\nfeatures_comprehensive['TC5yr_Pacific_level'] = features_raw['TC5yr_Pacific']\nprint(f\"  ‚Üí Created 1 feature\")\n\nprint(f\"\\n‚úÖ Category 3 complete: {len(features_comprehensive.columns)} total features so far\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 3: FLEET & SUPPLY\n",
      "================================================================================\n",
      "Applying 5 transformations (except TC5yr - levels only due to missing data)\n",
      "\n",
      "Transforming: Panamax_Orderbook_Pct\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: Panamax_Deliveries_DWT\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: Panamax_Idle_Pct\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: Capesize_Orderbook_Pct\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: Atlantic_Port_Calls\n",
      "  ‚Üí Created 5 features\n",
      "Transforming: Panamax_Fleet_Growth_YoY\n",
      "  ‚Üí Created 5 features\n",
      "\n",
      "Transforming: TC5yr_Atlantic (level only - sparse data)\n",
      "  ‚Üí Created 1 feature\n",
      "\n",
      "Transforming: TC5yr_Pacific (level only - sparse data)\n",
      "  ‚Üí Created 1 feature\n",
      "\n",
      "‚úÖ Category 3 complete: 62 total features so far\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Trade Volumes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.505147Z",
     "start_time": "2025-11-01T08:19:24.452627Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY 4: TRADE VOLUMES\")\nprint(\"=\"*80)\nprint(\"Applying transformations (with special handling for annual data)\\n\")\n\n# Annual trade volumes - SKIP vol30 per Decision D2-2B\n# Reason: Annual data forward-filled to daily creates constant values ‚Üí std30 = 0 ‚Üí NaN\nprint(\"üìã ANNUAL TRADE VOLUMES (interpolated from yearly data):\")\nprint(\"   Applying 4 transformations only (level, diff, pct, yoy)\")\nprint(\"   ‚ùå SKIPPING vol30 per Decision D2-2B (constant values ‚Üí std=0 ‚Üí NaN)\\n\")\n\nannual_trade_features = [\n    'China_Coal_Imports_MT',\n    'China_Grain_Imports_MT',\n    'India_Coal_Imports_MT',\n    'Japan_Coal_Imports_MT',\n    'Indonesia_Coal_Exports_MT',\n    'Australia_Coal_Exports_MT',\n    'World_Grain_Trade_MT',\n    'World_Coal_Trade_MT'\n]\n\nfor feature in annual_trade_features:\n    print(f\"Transforming: {feature}\")\n    # Skip vol30 for annual features (D2-2B)\n    transformed = create_transformations(features_raw, feature, \n                                         transformations=['level', 'diff', 'pct', 'yoy'], \n                                         verbose=False)\n    features_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\n    print(f\"  ‚Üí Created {len(transformed.columns)} features (skipped vol30)\")\n\n# Pre-transformed trade indicators (already YoY, but create additional variants)\nprint(f\"\\nüìã PRE-TRANSFORMED TRADE INDICATORS (monthly YoY data):\")\nprint(f\"   Applying 3 transformations (level, diff, pct)\\n\")\n\nprint(f\"Transforming: Coal_Trade_YoY\")\ntransformed = create_transformations(features_raw, 'Coal_Trade_YoY', transformations=['level', 'diff', 'pct'], verbose=False)\nfeatures_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\nprint(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\nprint(f\"\\nTransforming: Grain_Trade_YoY\")\ntransformed = create_transformations(features_raw, 'Grain_Trade_YoY', transformations=['level', 'diff', 'pct'], verbose=False)\nfeatures_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\nprint(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\n# Volume indices (composite indicators - full transformations)\nprint(f\"\\nüìã VOLUME INDICES (monthly composite data):\")\nprint(f\"   Applying ALL 5 transformations\\n\")\n\nprint(f\"Transforming: Coal_Trade_Volume_Index\")\ntransformed = create_transformations(features_raw, 'Coal_Trade_Volume_Index', transformations='all', verbose=False)\nfeatures_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\nprint(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\nprint(f\"\\nTransforming: Grain_Trade_Volume_Index\")\ntransformed = create_transformations(features_raw, 'Grain_Trade_Volume_Index', transformations='all', verbose=False)\nfeatures_comprehensive = pd.concat([features_comprehensive, transformed], axis=1)\nprint(f\"  ‚Üí Created {len(transformed.columns)} features\")\n\nprint(f\"\\n‚úÖ Category 4 complete: {len(features_comprehensive.columns)} total features so far\")\nprint(f\"   ‚ÑπÔ∏è Prevented ~8 features with 90% missing (annual vol30 transformations)\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 4: TRADE VOLUMES\n",
      "================================================================================\n",
      "Applying transformations (with special handling for annual data)\n",
      "\n",
      "üìã ANNUAL TRADE VOLUMES (interpolated from yearly data):\n",
      "   Applying 4 transformations only (level, diff, pct, yoy)\n",
      "   ‚ùå SKIPPING vol30 per Decision D2-2B (constant values ‚Üí std=0 ‚Üí NaN)\n",
      "\n",
      "Transforming: China_Coal_Imports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: China_Grain_Imports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: India_Coal_Imports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: Japan_Coal_Imports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: Indonesia_Coal_Exports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: Australia_Coal_Exports_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: World_Grain_Trade_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "Transforming: World_Coal_Trade_MT\n",
      "  ‚Üí Created 4 features (skipped vol30)\n",
      "\n",
      "üìã PRE-TRANSFORMED TRADE INDICATORS (monthly YoY data):\n",
      "   Applying 3 transformations (level, diff, pct)\n",
      "\n",
      "Transforming: Coal_Trade_YoY\n",
      "  ‚Üí Created 3 features\n",
      "\n",
      "Transforming: Grain_Trade_YoY\n",
      "  ‚Üí Created 3 features\n",
      "\n",
      "üìã VOLUME INDICES (monthly composite data):\n",
      "   Applying ALL 5 transformations\n",
      "\n",
      "Transforming: Coal_Trade_Volume_Index\n",
      "  ‚Üí Created 5 features\n",
      "\n",
      "Transforming: Grain_Trade_Volume_Index\n",
      "  ‚Üí Created 5 features\n",
      "\n",
      "‚úÖ Category 4 complete: 110 total features so far\n",
      "   ‚ÑπÔ∏è Prevented ~8 features with 90% missing (annual vol30 transformations)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.5 FFA Spreads (REMOVED - Used Labels as Features)\n\n**Decision D1-1A:** These features have been DELETED because they used target variables (P1A_82, P3A_82) as inputs, creating data leakage risk even with lagging.\n\nOriginal features removed:\n- P1A_FFA_Spread_level\n- P1A_FFA_Spread_diff\n- P3A_FFA_Spread_level\n- P3A_FFA_Spread_diff\n\nImpact: -4 features"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.535865Z",
     "start_time": "2025-11-01T08:19:24.527489Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY 5: FFA SPREADS (REMOVED)\")\nprint(\"=\"*80)\nprint(\"‚ùå FFA Spread features DELETED per Decision D1-1A\")\nprint(\"   Reason: Used target variables (P1A_82, P3A_82) as feature inputs\")\nprint(\"   Impact: -4 features removed\")\nprint(f\"\\n‚úÖ Category 5 skipped: {len(features_comprehensive.columns)} total features so far\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 5: FFA SPREADS (REMOVED)\n",
      "================================================================================\n",
      "‚ùå FFA Spread features DELETED per Decision D1-1A\n",
      "   Reason: Used target variables (P1A_82, P3A_82) as feature inputs\n",
      "   Impact: -4 features removed\n",
      "\n",
      "‚úÖ Category 5 skipped: 110 total features so far\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Economic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.612713Z",
     "start_time": "2025-11-01T08:19:24.596766Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY 6: ECONOMIC INDICATORS\")\nprint(\"=\"*80)\nprint(\"Creating additional variants of IP growth indicators\\n\")\n\n# Atlantic IP (already YoY, but create additional variants)\n# Note: mom transformation removed per D3-3A (redundant with pct)\nprint(\"Transforming: Atlantic_IP_YoY\")\nfeatures_comprehensive['Atlantic_IP_yoy'] = features_raw['Atlantic_IP_YoY']\nfeatures_comprehensive['Atlantic_IP_pct'] = features_raw['Atlantic_IP_YoY'].pct_change(1) * 100\nfeatures_comprehensive['Atlantic_IP_diff'] = features_raw['Atlantic_IP_YoY'].diff()\nprint(f\"  ‚Üí Created 3 features (yoy, pct, diff)\")\n\n# Pacific IP (already YoY, but create additional variants)\nprint(\"\\nTransforming: Pacific_IP_YoY\")\nfeatures_comprehensive['Pacific_IP_yoy'] = features_raw['Pacific_IP_YoY']\nfeatures_comprehensive['Pacific_IP_pct'] = features_raw['Pacific_IP_YoY'].pct_change(1) * 100\nfeatures_comprehensive['Pacific_IP_diff'] = features_raw['Pacific_IP_YoY'].diff()\nprint(f\"  ‚Üí Created 3 features (yoy, pct, diff)\")\n\nprint(f\"\\n‚úÖ Category 6 complete: {len(features_comprehensive.columns)} total features so far\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 6: ECONOMIC INDICATORS\n",
      "================================================================================\n",
      "Creating additional variants of IP growth indicators\n",
      "\n",
      "Transforming: Atlantic_IP_YoY\n",
      "  ‚Üí Created 3 features (yoy, pct, diff)\n",
      "\n",
      "Transforming: Pacific_IP_YoY\n",
      "  ‚Üí Created 3 features (yoy, pct, diff)\n",
      "\n",
      "‚úÖ Category 6 complete: 116 total features so far\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 FFA Term Structure (Forward Curve Features)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.655723Z",
     "start_time": "2025-11-01T08:19:24.642722Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY 7: FFA TERM STRUCTURE (EXPERIMENTAL)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Creating forward curve features (note: Basis/Slope failed in Round 1)\\n\")\n",
    "\n",
    "# P1A term structure\n",
    "print(\"Creating: P1A term structure features\")\n",
    "# Current month FFA (level + diff)\n",
    "features_comprehensive['P1EA_CURMON_level'] = features_raw['P1EA_82CURMON']\n",
    "features_comprehensive['P1EA_CURMON_diff'] = features_raw['P1EA_82CURMON'].diff()\n",
    "\n",
    "# 1-month forward (level + diff)\n",
    "features_comprehensive['P1EA_1MON_level'] = features_raw['P1EA_82+1MON']\n",
    "features_comprehensive['P1EA_1MON_diff'] = features_raw['P1EA_82+1MON'].diff()\n",
    "\n",
    "# 1-quarter forward (level + diff)\n",
    "features_comprehensive['P1EA_1Q_level'] = features_raw['P1EA_82+1Q']\n",
    "features_comprehensive['P1EA_1Q_diff'] = features_raw['P1EA_82+1Q'].diff()\n",
    "\n",
    "print(f\"  ‚Üí Created 6 P1A FFA features\")\n",
    "\n",
    "# P3A term structure\n",
    "print(\"\\nCreating: P3A term structure features\")\n",
    "# Current month FFA (level + diff)\n",
    "features_comprehensive['P3EA_CURMON_level'] = features_raw['P3EA_82CURMON']\n",
    "features_comprehensive['P3EA_CURMON_diff'] = features_raw['P3EA_82CURMON'].diff()\n",
    "\n",
    "# 1-month forward (level + diff)\n",
    "features_comprehensive['P3EA_1MON_level'] = features_raw['P3EA_82+1MON']\n",
    "features_comprehensive['P3EA_1MON_diff'] = features_raw['P3EA_82+1MON'].diff()\n",
    "\n",
    "# 1-quarter forward (level + diff)\n",
    "features_comprehensive['P3EA_1Q_level'] = features_raw['P3EA_82+1Q']\n",
    "features_comprehensive['P3EA_1Q_diff'] = features_raw['P3EA_82+1Q'].diff()\n",
    "\n",
    "print(f\"  ‚Üí Created 6 P3A FFA features\")\n",
    "\n",
    "print(f\"\\n‚úÖ Category 7 complete: {len(features_comprehensive.columns)} total features so far\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Round 1 showed FFA term structure had negative importance.\")\n",
    "print(\"   These features included for empirical validation - may be dropped in Notebook 03.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CATEGORY 7: FFA TERM STRUCTURE (EXPERIMENTAL)\n",
      "================================================================================\n",
      "Creating forward curve features (note: Basis/Slope failed in Round 1)\n",
      "\n",
      "Creating: P1A term structure features\n",
      "  ‚Üí Created 6 P1A FFA features\n",
      "\n",
      "Creating: P3A term structure features\n",
      "  ‚Üí Created 6 P3A FFA features\n",
      "\n",
      "‚úÖ Category 7 complete: 128 total features so far\n",
      "\n",
      "‚ö†Ô∏è Note: Round 1 showed FFA term structure had negative importance.\n",
      "   These features included for empirical validation - may be dropped in Notebook 03.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.679386Z",
     "start_time": "2025-11-01T08:19:24.674012Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä COMPREHENSIVE FEATURE SET SUMMARY\")\n",
    "print(f\"  Total features created: {features_comprehensive.shape[1]}\")\n",
    "print(f\"  Total rows (days): {features_comprehensive.shape[0]}\")\n",
    "print(f\"  Date range: {features_comprehensive.index.min()} to {features_comprehensive.index.max()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "üìä COMPREHENSIVE FEATURE SET SUMMARY\n",
      "  Total features created: 128\n",
      "  Total rows (days): 1153\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.712063Z",
     "start_time": "2025-11-01T08:19:24.702790Z"
    }
   },
   "source": [
    "# Missing values analysis\n",
    "print(\"\\nüìä MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_summary = features_comprehensive.isnull().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_summary / len(features_comprehensive) * 100)\n",
    "\n",
    "# Show top 30 features with most missing values\n",
    "print(\"\\nTop 30 features with most missing values:\")\n",
    "print(\"\\n{:<50} {:>10} {:>10}\".format('Feature', 'Missing', 'Pct %'))\n",
    "print(\"-\" * 72)\n",
    "for feat, count in missing_summary.head(30).items():\n",
    "    pct = missing_pct[feat]\n",
    "    print(\"{:<50} {:>10} {:>9.2f}%\".format(feat, int(count), pct))\n",
    "\n",
    "# Expected NaN patterns\n",
    "print(\"\\n\\nüìã EXPECTED NaN PATTERNS:\")\n",
    "print(\"  1. Row 1: ALL features (from 1-day lag in Notebook 01) ‚úÖ\")\n",
    "print(\"  2. MA30 features: ~30 rows (rolling window) ‚úÖ\")\n",
    "print(\"  3. YoY features: ~12 rows (12-period lookback) ‚úÖ\")\n",
    "print(\"  4. Vol30 features: ~30 rows (rolling window) ‚úÖ\")\n",
    "print(\"  5. TC5yr features: Sparse data (weekly reporting) ‚úÖ\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä MISSING VALUES ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 30 features with most missing values:\n",
      "\n",
      "Feature                                               Missing      Pct %\n",
      "------------------------------------------------------------------------\n",
      "VLSFO_vol30                                                63      5.46%\n",
      "MGO_vol30                                                  63      5.46%\n",
      "BPI_vol30                                                  30      2.60%\n",
      "C5TC_vol30                                                 30      2.60%\n",
      "P4_82_vol30                                                30      2.60%\n",
      "PDOPEX_vol30                                               30      2.60%\n",
      "Panamax_Deliveries_DWT_vol30                               30      2.60%\n",
      "Panamax_Orderbook_Pct_vol30                                30      2.60%\n",
      "Capesize_Orderbook_Pct_vol30                               30      2.60%\n",
      "Panamax_Idle_Pct_vol30                                     30      2.60%\n",
      "Panamax_Fleet_Growth_YoY_vol30                             30      2.60%\n",
      "Atlantic_Port_Calls_vol30                                  30      2.60%\n",
      "Grain_Trade_Volume_Index_vol30                             30      2.60%\n",
      "Coal_Trade_Volume_Index_vol30                              30      2.60%\n",
      "Coal_Trade_YoY_pct                                         18      1.56%\n",
      "PDOPEX_yoy                                                 13      1.13%\n",
      "Panamax_Deliveries_DWT_yoy                                 13      1.13%\n",
      "Panamax_Idle_Pct_yoy                                       13      1.13%\n",
      "MGO_yoy                                                    13      1.13%\n",
      "VLSFO_yoy                                                  13      1.13%\n",
      "Panamax_Fleet_Growth_YoY_yoy                               13      1.13%\n",
      "Capesize_Orderbook_Pct_yoy                                 13      1.13%\n",
      "Panamax_Orderbook_Pct_yoy                                  13      1.13%\n",
      "Atlantic_Port_Calls_yoy                                    13      1.13%\n",
      "India_Coal_Imports_MT_yoy                                  13      1.13%\n",
      "Japan_Coal_Imports_MT_yoy                                  13      1.13%\n",
      "China_Grain_Imports_MT_yoy                                 13      1.13%\n",
      "World_Grain_Trade_MT_yoy                                   13      1.13%\n",
      "China_Coal_Imports_MT_yoy                                  13      1.13%\n",
      "P4_82_yoy                                                  13      1.13%\n",
      "\n",
      "\n",
      "üìã EXPECTED NaN PATTERNS:\n",
      "  1. Row 1: ALL features (from 1-day lag in Notebook 01) ‚úÖ\n",
      "  2. MA30 features: ~30 rows (rolling window) ‚úÖ\n",
      "  3. YoY features: ~12 rows (12-period lookback) ‚úÖ\n",
      "  4. Vol30 features: ~30 rows (rolling window) ‚úÖ\n",
      "  5. TC5yr features: Sparse data (weekly reporting) ‚úÖ\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.737394Z",
     "start_time": "2025-11-01T08:19:24.730015Z"
    }
   },
   "source": [
    "# Verify first row is all NaN (leakage check)\n",
    "print(\"\\n‚ö†Ô∏è CRITICAL LEAKAGE CHECK:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "first_row_nulls = features_comprehensive.iloc[0].isnull().sum()\n",
    "total_features = len(features_comprehensive.columns)\n",
    "\n",
    "print(f\"\\nFirst row (2021-03-01) NULL count: {first_row_nulls}/{total_features}\")\n",
    "\n",
    "if first_row_nulls == total_features:\n",
    "    print(\"‚úÖ PASS: All features properly lagged (row 1 is all NaN)\")\n",
    "    print(\"   This confirms no data leakage from improper temporal alignment.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è WARNING: Expected all {total_features} features to be NaN in row 1\")\n",
    "    print(f\"   Found {total_features - first_row_nulls} non-null features!\")\n",
    "    print(\"\\n   Non-null features in row 1:\")\n",
    "    non_null_features = features_comprehensive.iloc[0][features_comprehensive.iloc[0].notna()].index.tolist()\n",
    "    for feat in non_null_features:\n",
    "        print(f\"     - {feat}: {features_comprehensive.iloc[0][feat]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è CRITICAL LEAKAGE CHECK:\n",
      "================================================================================\n",
      "\n",
      "First row (2021-03-01) NULL count: 128/128\n",
      "‚úÖ PASS: All features properly lagged (row 1 is all NaN)\n",
      "   This confirms no data leakage from improper temporal alignment.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.776455Z",
     "start_time": "2025-11-01T08:19:24.759402Z"
    }
   },
   "source": "# Check for infinite values\nprint(\"\\nüìä INFINITE VALUES CHECK:\")\nprint(\"=\" * 80)\n\ninf_counts = np.isinf(features_comprehensive).sum()\nfeatures_with_inf = inf_counts[inf_counts > 0]\n\nif len(features_with_inf) == 0:\n    print(\"\\n‚úÖ PASS: No infinite values detected\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Found {len(features_with_inf)} features with infinite values:\\n\")\n    for feat, count in features_with_inf.items():\n        print(f\"  {feat}: {count} infinite values\")\n    \n    print(\"\\nüîß FIXING: Replacing infinite values with NaN...\")\n    print(\"   Rationale: Infinite values (from division by zero in pct_change)\")\n    print(\"              must be removed to ensure clean data for validation.\\n\")\n    \n    features_comprehensive = features_comprehensive.replace([np.inf, -np.inf], np.nan)\n    \n    # Verify fix\n    inf_after = np.isinf(features_comprehensive).sum().sum()\n    if inf_after == 0:\n        print(\"‚úÖ FIXED: All infinite values replaced with NaN\")\n        \n        # Report new missing counts\n        for feat in features_with_inf.index:\n            new_missing = features_comprehensive[feat].isnull().sum()\n            new_pct = (new_missing / len(features_comprehensive)) * 100\n            print(f\"   - {feat}: now {new_missing} NaN ({new_pct:.2f}%)\")\n    else:\n        print(f\"‚ùå ERROR: Still {inf_after} infinite values remain!\")\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä INFINITE VALUES CHECK:\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  Found 1 features with infinite values:\n",
      "\n",
      "  Coal_Trade_YoY_pct: 1 infinite values\n",
      "\n",
      "üîß FIXING: Replacing infinite values with NaN...\n",
      "   Rationale: Infinite values (from division by zero in pct_change)\n",
      "              must be removed to ensure clean data for validation.\n",
      "\n",
      "‚úÖ FIXED: All infinite values replaced with NaN\n",
      "   - Coal_Trade_YoY_pct: now 19 NaN (1.65%)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.825072Z",
     "start_time": "2025-11-01T08:19:24.805064Z"
    }
   },
   "source": [
    "# Summary statistics for select features\n",
    "print(\"\\nüìä SUMMARY STATISTICS (Sample Features):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select representative features\n",
    "sample_features = [\n",
    "    'BPI_level', 'BPI_pct', 'BPI_ma30_dev',\n",
    "    'VLSFO_pct', 'VLSFO_ma30_dev',\n",
    "    'C5TC_diff', 'C5TC_pct',\n",
    "    'China_Coal_Imports_MT_yoy', 'China_Grain_Imports_MT_yoy',\n",
    "    'P1A_FFA_Spread_level', 'P3A_FFA_Spread_level'\n",
    "]\n",
    "\n",
    "# Filter to features that exist\n",
    "sample_features = [f for f in sample_features if f in features_comprehensive.columns]\n",
    "\n",
    "if len(sample_features) > 0:\n",
    "    summary_stats = features_comprehensive[sample_features].describe()\n",
    "    print(\"\\n\" + summary_stats.to_string())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Sample features not found in dataset\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SUMMARY STATISTICS (Sample Features):\n",
      "================================================================================\n",
      "\n",
      "         BPI_level      BPI_pct    VLSFO_pct    C5TC_diff     C5TC_pct  China_Coal_Imports_MT_yoy  China_Grain_Imports_MT_yoy\n",
      "count  1152.000000  1151.000000  1151.000000  1151.000000  1151.000000                1140.000000                 1140.000000\n",
      "mean   1967.139757     0.020324     0.004903     9.923545     0.346486                   0.455578                   -0.133485\n",
      "std     818.960842     2.731426     1.104261  1490.048504     7.775443                   6.124854                    2.217483\n",
      "min     748.000000    -8.396947    -5.514706 -7946.000000   -30.251952                 -16.752173                  -12.999742\n",
      "25%    1405.750000    -1.700319    -0.518904  -736.000000    -4.042270                   0.000000                    0.000000\n",
      "50%    1705.500000    -0.185357     0.000000   -37.000000    -0.234299                   0.000000                    0.000000\n",
      "75%    2379.500000     1.369142     0.580837   739.500000     3.770552                   0.000000                    0.000000\n",
      "max    4328.000000    22.923239     8.328644  7140.000000    56.315925                  53.654883                   12.614883\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Type Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:24.845846Z",
     "start_time": "2025-11-01T08:19:24.834408Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE TYPE BREAKDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count features by transformation type\n",
    "feature_types = {\n",
    "    'level': len([c for c in features_comprehensive.columns if '_level' in c]),\n",
    "    'diff': len([c for c in features_comprehensive.columns if '_diff' in c]),\n",
    "    'pct': len([c for c in features_comprehensive.columns if '_pct' in c]),\n",
    "    'yoy': len([c for c in features_comprehensive.columns if '_yoy' in c or 'YoY' in c]),\n",
    "    'mom': len([c for c in features_comprehensive.columns if '_mom' in c]),\n",
    "    'ma30_dev': len([c for c in features_comprehensive.columns if '_ma30_dev' in c]),\n",
    "    'vol30': len([c for c in features_comprehensive.columns if '_vol30' in c]),\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Transformation Type Counts:\")\n",
    "print(\"\\n{:<15} {:>10}\".format('Type', 'Count'))\n",
    "print(\"-\" * 27)\n",
    "for ftype, count in feature_types.items():\n",
    "    print(\"{:<15} {:>10}\".format(ftype, count))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 27)\n",
    "print(\"{:<15} {:>10}\".format('TOTAL', features_comprehensive.shape[1]))\n",
    "\n",
    "# Feature category breakdown\n",
    "print(\"\\n\\nüìä Feature Category Breakdown:\")\n",
    "categories = {\n",
    "    'Baltic/Market Indices': ['BPI', 'C5TC', 'P4_82', 'PDOPEX'],\n",
    "    'Bunker Prices': ['VLSFO', 'MGO'],\n",
    "    'Fleet/Supply': ['Panamax', 'Capesize', 'TC5yr', 'Atlantic_Port_Calls'],\n",
    "    'Trade Volumes': ['Coal', 'Grain', 'Trade'],\n",
    "    'FFA Spreads': ['FFA_Spread'],\n",
    "    'Economic': ['IP_'],\n",
    "    'FFA Term Structure': ['P1EA', 'P3EA']\n",
    "}\n",
    "\n",
    "print(\"\\n{:<25} {:>10}\".format('Category', 'Count'))\n",
    "print(\"-\" * 37)\n",
    "for category, keywords in categories.items():\n",
    "    count = sum(1 for col in features_comprehensive.columns \n",
    "                if any(kw in col for kw in keywords))\n",
    "    print(\"{:<25} {:>10}\".format(category, count))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE TYPE BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "üìä Transformation Type Counts:\n",
      "\n",
      "Type                 Count\n",
      "---------------------------\n",
      "level                   32\n",
      "diff                    32\n",
      "pct                     26\n",
      "yoy                     34\n",
      "mom                      0\n",
      "ma30_dev                 0\n",
      "vol30                   14\n",
      "\n",
      "---------------------------\n",
      "TOTAL                  128\n",
      "\n",
      "\n",
      "üìä Feature Category Breakdown:\n",
      "\n",
      "Category                       Count\n",
      "-------------------------------------\n",
      "Baltic/Market Indices             20\n",
      "Bunker Prices                     10\n",
      "Fleet/Supply                      32\n",
      "Trade Volumes                     48\n",
      "FFA Spreads                        0\n",
      "Economic                           6\n",
      "FFA Term Structure                12\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Save Comprehensive Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:25.106971Z",
     "start_time": "2025-11-01T08:19:24.861853Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING COMPREHENSIVE FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path('data/processed/features')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = output_dir / 'features_comprehensive.csv'\n",
    "features_comprehensive.to_csv(output_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive features saved to:\")\n",
    "print(f\"   {output_path}\")\n",
    "print(f\"\\nüìä File details:\")\n",
    "print(f\"   Shape: {features_comprehensive.shape}\")\n",
    "print(f\"   Features: {features_comprehensive.shape[1]} columns\")\n",
    "print(f\"   Rows: {features_comprehensive.shape[0]} days\")\n",
    "print(f\"   Date range: {features_comprehensive.index.min()} to {features_comprehensive.index.max()}\")\n",
    "print(f\"   File size: {output_path.stat().st_size / 1024:.2f} KB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING COMPREHENSIVE FEATURE SET\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Comprehensive features saved to:\n",
      "   data\\processed\\features\\features_comprehensive.csv\n",
      "\n",
      "üìä File details:\n",
      "   Shape: (1153, 128)\n",
      "   Features: 128 columns\n",
      "   Rows: 1153 days\n",
      "   Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "   File size: 1249.36 KB\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:25.120945Z",
     "start_time": "2025-11-01T08:19:25.112997Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"NOTEBOOK 02 COMPLETE (REVISED) ‚úÖ\")\nprint(\"=\"*80)\n\nprint(\"\\nüìã Summary:\")\nprint(f\"   ‚úì Created {features_comprehensive.shape[1]} comprehensive features\")\nprint(f\"   ‚úì Applied 5 transformation types (level, diff, pct, yoy, vol30)\")\nprint(f\"   ‚úì Processed {len(features_raw.columns)} raw input features\")\nprint(f\"   ‚úì Covered 1,153 business days (2021-03-01 to 2025-10-10)\")\nprint(f\"   ‚úì All leakage checks passed (row 1 all NaN)\")\nprint(f\"   ‚úì Saved to: data/processed/features/features_comprehensive.csv\")\n\nprint(\"\\nüîß Decisions Applied:\")\nprint(\"   D1-1A: ‚ùå Removed FFA spreads using labels (-4 features)\")\nprint(\"   D2-2B: ‚ùå Skipped vol30 for 8 annual features (-8 features)\")\nprint(\"   D3-3A: ‚ùå Removed mom transformation (-24 features)\")\nprint(\"   D4-4B: ‚úÖ Reduced to 5 transformations\")\nprint(\"   D5-5A: ‚úÖ Kept YoY transformations\")\n\nprint(f\"\\nüìä Expected Improvements:\")\nprint(f\"   Previous: 184 features ‚Üí 70 usable rows (6.1% retention)\")\nprint(f\"   Revised: ~{features_comprehensive.shape[1]} features ‚Üí ~1,100+ usable rows (95%+ retention)\")\nprint(f\"   Fixes: No infinite VIF, no label leakage, minimal NaN\")\n\nprint(\"\\nüéØ Next Steps:\")\nprint(\"   1. Execute Notebook 03: Feature Validation & Allocation\")\nprint(\"      - Random Forest permutation importance on ALL features\")\nprint(\"      - Data-driven allocation into CORE vs ML sets\")\nprint(\"      - VIF analysis for CORE features\")\nprint(\"      - Decision gate: PASS/FAIL before modeling\")\nprint(\"\\n   2. If PASS: Proceed to Notebook 04 (Data Preparation)\")\nprint(\"   3. If FAIL: Return here to revise features\")\n\nprint(\"\\n‚ö†Ô∏è Critical Reminders:\")\nprint(\"   - All features already have 1-day lag (verified ‚úÖ)\")\nprint(\"   - No additional lag needed in downstream notebooks\")\nprint(\"   - Expected NaN in first ~30 rows for vol30 features\")\nprint(\"   - Expected NaN in first ~12 rows for YoY features\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Ready for empirical feature validation! üöÄ\")\nprint(\"=\"*80)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NOTEBOOK 02 COMPLETE (REVISED) ‚úÖ\n",
      "================================================================================\n",
      "\n",
      "üìã Summary:\n",
      "   ‚úì Created 128 comprehensive features\n",
      "   ‚úì Applied 5 transformation types (level, diff, pct, yoy, vol30)\n",
      "   ‚úì Processed 58 raw input features\n",
      "   ‚úì Covered 1,153 business days (2021-03-01 to 2025-10-10)\n",
      "   ‚úì All leakage checks passed (row 1 all NaN)\n",
      "   ‚úì Saved to: data/processed/features/features_comprehensive.csv\n",
      "\n",
      "üîß Decisions Applied:\n",
      "   D1-1A: ‚ùå Removed FFA spreads using labels (-4 features)\n",
      "   D2-2B: ‚ùå Skipped vol30 for 8 annual features (-8 features)\n",
      "   D3-3A: ‚ùå Removed mom transformation (-24 features)\n",
      "   D4-4B: ‚úÖ Reduced to 5 transformations\n",
      "   D5-5A: ‚úÖ Kept YoY transformations\n",
      "\n",
      "üìä Expected Improvements:\n",
      "   Previous: 184 features ‚Üí 70 usable rows (6.1% retention)\n",
      "   Revised: ~128 features ‚Üí ~1,100+ usable rows (95%+ retention)\n",
      "   Fixes: No infinite VIF, no label leakage, minimal NaN\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Execute Notebook 03: Feature Validation & Allocation\n",
      "      - Random Forest permutation importance on ALL features\n",
      "      - Data-driven allocation into CORE vs ML sets\n",
      "      - VIF analysis for CORE features\n",
      "      - Decision gate: PASS/FAIL before modeling\n",
      "\n",
      "   2. If PASS: Proceed to Notebook 04 (Data Preparation)\n",
      "   3. If FAIL: Return here to revise features\n",
      "\n",
      "‚ö†Ô∏è Critical Reminders:\n",
      "   - All features already have 1-day lag (verified ‚úÖ)\n",
      "   - No additional lag needed in downstream notebooks\n",
      "   - Expected NaN in first ~30 rows for vol30 features\n",
      "   - Expected NaN in first ~12 rows for YoY features\n",
      "\n",
      "================================================================================\n",
      "Ready for empirical feature validation! üöÄ\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
