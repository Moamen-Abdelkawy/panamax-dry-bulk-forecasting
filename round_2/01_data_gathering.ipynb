{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Round 2 - Data Gathering\n\n**Purpose:** Load raw data files, create master calendar, pivot BFA data, and merge all features to daily frequency.\n\n**Key Steps:**\n1. Load all raw data files from `round_2/data/raw/`\n2. Create master calendar based on label availability (P1A_82, P3A_82)\n3. Pivot BFA data from tall to wide format\n4. Align all features to master calendar (forward-fill from weekly/monthly/annual frequencies)\n5. **Apply 1-day lag to ALL features (data leakage prevention)**\n6. Save intermediate files for feature engineering\n\n**Critical Notes:**\n- Study period: 2021-03-01 onwards\n- Labels (P1A_82, P3A_82) are in `baltic_exchange_010321_101025.csv`\n- Master calendar ensures all features align to label availability\n- Date format: yyyy-mm-dd (kept as column, NOT index)\n- **All data stays within round_2/ directory structure**\n- **Cross-platform compatible:** Works on Windows, Linux, macOS, Cloud environments\n- **Data leakage prevention:** ALL features are lagged by 1 day using `.shift(1)` to ensure predictions only use information available BEFORE the prediction date (matching Round 1 approach)\n\n**Path Detection Strategy:**\n- Automatically detects `round_2/` directory location\n- Uses `pathlib.Path` for OS-agnostic path handling\n- Validates directory structure before execution\n- Works regardless of execution directory\n\n**Outputs:**\n- `round_2/data/processed/intermediate/master_calendar.csv`\n- `round_2/data/processed/intermediate/bfa_wide.csv`\n- `round_2/data/processed/intermediate/labels.csv`\n- `round_2/data/processed/intermediate/features_raw_daily.csv` (with 1-day lag applied)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.287344Z",
     "start_time": "2025-11-01T08:18:56.128533Z"
    }
   },
   "source": "import pandas as pd\nimport numpy as np\nimport os\nimport sys\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"ROUND 2 - DATA GATHERING\")\nprint(\"=\"*80)\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROUND 2 - DATA GATHERING\n",
      "================================================================================\n",
      "Python version: 3.11.14 | packaged by conda-forge | (main, Oct 13 2025, 14:00:26) [MSC v.1944 64 bit (AMD64)]\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.330806Z",
     "start_time": "2025-11-01T08:18:57.312296Z"
    }
   },
   "source": "# Round 2 root (this round's working directory)\n# This works regardless of where the notebook is executed from\nROUND2_ROOT = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n\n# If running as notebook, detect round_2 directory robustly\nif 'ipykernel' in sys.modules or 'IPython' in sys.modules:\n    # Running in Jupyter - use notebook's actual location\n    try:\n        from IPython import get_ipython\n        notebook_path = Path(get_ipython().run_line_magic('pwd', ''))\n        # Find round_2 directory by walking up\n        current = notebook_path\n        while current.name != 'round_2' and current != current.parent:\n            current = current.parent\n        if current.name == 'round_2':\n            ROUND2_ROOT = current\n        else:\n            # Fallback: assume we're already in round_2\n            ROUND2_ROOT = notebook_path\n    except:\n        # Fallback for other notebook environments\n        ROUND2_ROOT = Path.cwd()\n\nprint(f\"Round 2 Root: {ROUND2_ROOT}\")\n\n# Input directories (within round_2/)\nRAW_DATA_DIR = ROUND2_ROOT / 'data' / 'raw'\nBALTIC_DIR = RAW_DATA_DIR / 'baltic_exchange'\nBUNKER_DIR = RAW_DATA_DIR / 'bunker'\nCLARKSONS_DIR = RAW_DATA_DIR / 'clarksons'\n\n# Output directories (within round_2/)\nPROCESSED_DIR = ROUND2_ROOT / 'data' / 'processed'\nINTERMEDIATE_DIR = PROCESSED_DIR / 'intermediate'\nMODELS_DIR = ROUND2_ROOT / 'data' / 'models'\nDIAGNOSTICS_DIR = ROUND2_ROOT / 'data' / 'diagnostics'\n\n# Create output directories if they don't exist\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\nINTERMEDIATE_DIR.mkdir(parents=True, exist_ok=True)\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\nDIAGNOSTICS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nInput Directories:\")\nprint(f\"  Raw Data: {RAW_DATA_DIR}\")\nprint(f\"  Baltic Exchange: {BALTIC_DIR}\")\nprint(f\"  Bunker: {BUNKER_DIR}\")\nprint(f\"  Clarksons: {CLARKSONS_DIR}\")\nprint(f\"\\nOutput Directories:\")\nprint(f\"  Processed: {PROCESSED_DIR}\")\nprint(f\"  Intermediate: {INTERMEDIATE_DIR}\")\nprint(f\"  Models: {MODELS_DIR}\")\nprint(f\"  Diagnostics: {DIAGNOSTICS_DIR}\")\n\n# Study parameters\nSTUDY_START = '2021-03-01'\nprint(f\"\\nStudy Start Date: {STUDY_START}\")\n\n# Validate that raw data directory exists\nif not RAW_DATA_DIR.exists():\n    raise FileNotFoundError(\n        f\"Raw data directory not found: {RAW_DATA_DIR}\\n\"\n        f\"Please ensure you're running this notebook from the correct location.\\n\"\n        f\"Expected structure: round_2/data/raw/\"\n    )\nelse:\n    print(f\"\\n✅ Raw data directory validated: {RAW_DATA_DIR}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 Root: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\n",
      "\n",
      "Input Directories:\n",
      "  Raw Data: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\n",
      "  Baltic Exchange: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\baltic_exchange\n",
      "  Bunker: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\bunker\n",
      "  Clarksons: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\clarksons\n",
      "\n",
      "Output Directories:\n",
      "  Processed: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\n",
      "  Intermediate: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\n",
      "  Models: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\models\n",
      "  Diagnostics: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\diagnostics\n",
      "\n",
      "Study Start Date: 2021-03-01\n",
      "\n",
      "✅ Raw data directory validated: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Labels (P1A_82, P3A_82) and Create Master Calendar"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.420900Z",
     "start_time": "2025-11-01T08:18:57.395512Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LOAD LABELS AND CREATE MASTER CALENDAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Baltic Exchange data (contains labels)\n",
    "baltic_file = BALTIC_DIR / 'baltic_exchange_010321_101025.csv'\n",
    "print(f\"\\nLoading Baltic Exchange data from: {baltic_file}\")\n",
    "\n",
    "baltic_df = pd.read_csv(baltic_file, parse_dates=['Date'])\n",
    "print(f\"  Loaded {len(baltic_df)} rows, {len(baltic_df.columns)} columns\")\n",
    "print(f\"  Columns: {list(baltic_df.columns)}\")\n",
    "print(f\"  Date range: {baltic_df['Date'].min()} to {baltic_df['Date'].max()}\")\n",
    "\n",
    "# Filter to study period\n",
    "baltic_df = baltic_df[baltic_df['Date'] >= STUDY_START].copy()\n",
    "print(f\"\\nFiltered to study period (>= {STUDY_START}): {len(baltic_df)} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LOAD LABELS AND CREATE MASTER CALENDAR\n",
      "================================================================================\n",
      "\n",
      "Loading Baltic Exchange data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\baltic_exchange\\baltic_exchange_010321_101025.csv\n",
      "  Loaded 1153 rows, 7 columns\n",
      "  Columns: ['Date', 'BPI', 'C5TC', 'P1A_82', 'P3A_82', 'P4_82', 'PDOPEX']\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Filtered to study period (>= 2021-03-01): 1153 rows\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.502930Z",
     "start_time": "2025-11-01T08:18:57.480914Z"
    }
   },
   "source": [
    "# Extract labels (P1A_82, P3A_82)\n",
    "print(\"\\nExtracting labels (P1A_82, P3A_82)...\")\n",
    "labels_df = baltic_df[['Date', 'P1A_82', 'P3A_82']].copy()\n",
    "\n",
    "print(f\"\\nLabel Statistics BEFORE filtering:\")\n",
    "print(f\"  Total rows: {len(labels_df)}\")\n",
    "print(f\"  P1A_82 non-null: {labels_df['P1A_82'].notna().sum()}\")\n",
    "print(f\"  P3A_82 non-null: {labels_df['P3A_82'].notna().sum()}\")\n",
    "print(f\"  Both non-null: {(labels_df['P1A_82'].notna() & labels_df['P3A_82'].notna()).sum()}\")\n",
    "\n",
    "# Filter to dates where BOTH labels are available\n",
    "labels_df = labels_df.dropna(subset=['P1A_82', 'P3A_82']).copy()\n",
    "\n",
    "print(f\"\\nLabel Statistics AFTER filtering (both non-null):\")\n",
    "print(f\"  Total rows: {len(labels_df)}\")\n",
    "print(f\"  Date range: {labels_df['Date'].min()} to {labels_df['Date'].max()}\")\n",
    "print(f\"\\nLabel Summary Statistics:\")\n",
    "print(labels_df[['P1A_82', 'P3A_82']].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting labels (P1A_82, P3A_82)...\n",
      "\n",
      "Label Statistics BEFORE filtering:\n",
      "  Total rows: 1153\n",
      "  P1A_82 non-null: 1153\n",
      "  P3A_82 non-null: 1153\n",
      "  Both non-null: 1153\n",
      "\n",
      "Label Statistics AFTER filtering (both non-null):\n",
      "  Total rows: 1153\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Label Summary Statistics:\n",
      "             P1A_82        P3A_82\n",
      "count   1153.000000   1153.000000\n",
      "mean   17151.208153  16720.982654\n",
      "std     8096.960712   7512.756643\n",
      "min     4458.000000   5434.000000\n",
      "25%    11165.000000  11669.000000\n",
      "50%    15225.000000  14373.000000\n",
      "75%    21275.000000  20273.000000\n",
      "max    45050.000000  40687.000000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.566127Z",
     "start_time": "2025-11-01T08:18:57.545223Z"
    }
   },
   "source": [
    "# Create Master Calendar\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING MASTER CALENDAR\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "master_calendar = labels_df[['Date']].copy()\n",
    "print(f\"\\nMaster Calendar:\")\n",
    "print(f\"  Total days: {len(master_calendar)}\")\n",
    "print(f\"  Start date: {master_calendar['Date'].min()}\")\n",
    "print(f\"  End date: {master_calendar['Date'].max()}\")\n",
    "\n",
    "# Validate no gaps (should be consecutive business days)\n",
    "date_diffs = master_calendar['Date'].diff()[1:]\n",
    "max_gap = date_diffs.max()\n",
    "print(f\"\\nCalendar Validation:\")\n",
    "print(f\"  Maximum gap between dates: {max_gap}\")\n",
    "\n",
    "if max_gap > pd.Timedelta(days=3):\n",
    "    print(\"  ⚠️ WARNING: Large gaps detected (>3 days). This may indicate missing data.\")\n",
    "    large_gaps = date_diffs[date_diffs > pd.Timedelta(days=3)]\n",
    "    print(f\"  Number of large gaps: {len(large_gaps)}\")\n",
    "else:\n",
    "    print(\"  ✅ Calendar validated: No unexpected gaps detected\")\n",
    "\n",
    "# Save master calendar\n",
    "master_calendar_file = INTERMEDIATE_DIR / 'master_calendar.csv'\n",
    "master_calendar.to_csv(master_calendar_file, index=False)\n",
    "print(f\"\\n✅ Master calendar saved to: {master_calendar_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CREATING MASTER CALENDAR\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Master Calendar:\n",
      "  Total days: 1153\n",
      "  Start date: 2021-03-01 00:00:00\n",
      "  End date: 2025-10-10 00:00:00\n",
      "\n",
      "Calendar Validation:\n",
      "  Maximum gap between dates: 11 days 00:00:00\n",
      "  ⚠️ WARNING: Large gaps detected (>3 days). This may indicate missing data.\n",
      "  Number of large gaps: 26\n",
      "\n",
      "✅ Master calendar saved to: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\master_calendar.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.638390Z",
     "start_time": "2025-11-01T08:18:57.626114Z"
    }
   },
   "source": [
    "# Save labels (aligned to master calendar)\n",
    "labels_file = INTERMEDIATE_DIR / 'labels.csv'\n",
    "labels_df.to_csv(labels_file, index=False)\n",
    "print(f\"✅ Labels saved to: {labels_file}\")\n",
    "print(f\"\\nLabel file contains {len(labels_df)} rows with columns: {list(labels_df.columns)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labels saved to: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\labels.csv\n",
      "\n",
      "Label file contains 1153 rows with columns: ['Date', 'P1A_82', 'P3A_82']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load and Process BFA Data (Tall to Wide)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.715467Z",
     "start_time": "2025-11-01T08:18:57.672462Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: LOAD AND PIVOT BFA DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load BFA data (tall format)\n",
    "bfa_file = BALTIC_DIR / 'bfa_panamax_010321_101025.csv'\n",
    "print(f\"\\nLoading BFA data from: {bfa_file}\")\n",
    "\n",
    "bfa_df = pd.read_csv(bfa_file, parse_dates=['ArchiveDate'])\n",
    "print(f\"  Loaded {len(bfa_df)} rows, {len(bfa_df.columns)} columns\")\n",
    "print(f\"  Columns: {list(bfa_df.columns)}\")\n",
    "\n",
    "# Rename ArchiveDate to Date for consistency\n",
    "bfa_df = bfa_df.rename(columns={'ArchiveDate': 'Date'})\n",
    "\n",
    "# Filter to study period\n",
    "bfa_df = bfa_df[bfa_df['Date'] >= STUDY_START].copy()\n",
    "print(f\"\\nFiltered to study period: {len(bfa_df)} rows\")\n",
    "print(f\"  Date range: {bfa_df['Date'].min()} to {bfa_df['Date'].max()}\")\n",
    "\n",
    "# Check unique route identifiers\n",
    "print(f\"\\nUnique RouteIdentifiers: {bfa_df['RouteIdentifier'].nunique()}\")\n",
    "print(f\"  Sample routes: {list(bfa_df['RouteIdentifier'].unique()[:10])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: LOAD AND PIVOT BFA DATA\n",
      "================================================================================\n",
      "\n",
      "Loading BFA data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\baltic_exchange\\bfa_panamax_010321_101025.csv\n",
      "  Loaded 25022 rows, 5 columns\n",
      "  Columns: ['GroupDesc', 'ArchiveDate', 'RouteIdentifier', 'RouteAverage', 'FFADescription']\n",
      "\n",
      "Filtered to study period: 25022 rows\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Unique RouteIdentifiers: 22\n",
      "  Sample routes: ['P1EA_82CURMON', 'P1EA_82+1MON', 'P1EA_82+2MON', 'P1EA_82+3MON', 'P1EA_82+4MON', 'P1EA_82+5MON', 'P1EA_82CURQ', 'P1EA_82+1Q', 'P1EA_82+2Q', 'P1EA_82+3Q']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.762911Z",
     "start_time": "2025-11-01T08:18:57.741501Z"
    }
   },
   "source": [
    "# Pivot BFA data from tall to wide format\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PIVOTING BFA DATA (TALL → WIDE)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nPivoting on: Date (index) × RouteIdentifier (columns) → RouteAverage (values)\")\n",
    "bfa_wide = bfa_df.pivot(index='Date', columns='RouteIdentifier', values='RouteAverage').reset_index()\n",
    "\n",
    "print(f\"\\nPivoted BFA data:\")\n",
    "print(f\"  Rows: {len(bfa_wide)}\")\n",
    "print(f\"  Columns: {len(bfa_wide.columns)} (including Date)\")\n",
    "print(f\"  Date range: {bfa_wide['Date'].min()} to {bfa_wide['Date'].max()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PIVOTING BFA DATA (TALL → WIDE)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pivoting on: Date (index) × RouteIdentifier (columns) → RouteAverage (values)\n",
      "\n",
      "Pivoted BFA data:\n",
      "  Rows: 1165\n",
      "  Columns: 23 (including Date)\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.801078Z",
     "start_time": "2025-11-01T08:18:57.793705Z"
    }
   },
   "source": [
    "# Identify P1EA and P3EA forward contracts\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FORWARD CURVE STRUCTURE ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "p1_cols = [col for col in bfa_wide.columns if 'P1EA_82' in str(col)]\n",
    "p3_cols = [col for col in bfa_wide.columns if 'P3EA_82' in str(col)]\n",
    "\n",
    "print(f\"\\nP1EA_82 (Atlantic) Forward Contracts: {len(p1_cols)}\")\n",
    "print(f\"  Key contracts: {p1_cols[:6]}\")\n",
    "\n",
    "print(f\"\\nP3EA_82 (Pacific) Forward Contracts: {len(p3_cols)}\")\n",
    "print(f\"  Key contracts: {p3_cols[:6]}\")\n",
    "\n",
    "print(f\"\\nTotal forward contracts: {len(p1_cols) + len(p3_cols)}\")\n",
    "\n",
    "# Check for CURMON (current month FFA) - needed for FFA spread\n",
    "if 'P1EA_82CURMON' in bfa_wide.columns:\n",
    "    print(f\"\\n✅ P1EA_82CURMON found (for P1A FFA spread calculation)\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ WARNING: P1EA_82CURMON not found!\")\n",
    "\n",
    "if 'P3EA_82CURMON' in bfa_wide.columns:\n",
    "    print(f\"✅ P3EA_82CURMON found (for P3A FFA spread calculation)\")\n",
    "else:\n",
    "    print(f\"⚠️ WARNING: P3EA_82CURMON not found!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FORWARD CURVE STRUCTURE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "P1EA_82 (Atlantic) Forward Contracts: 11\n",
      "  Key contracts: ['P1EA_82+1MON', 'P1EA_82+1Q', 'P1EA_82+2MON', 'P1EA_82+2Q', 'P1EA_82+3MON', 'P1EA_82+3Q']\n",
      "\n",
      "P3EA_82 (Pacific) Forward Contracts: 11\n",
      "  Key contracts: ['P3EA_82+1MON', 'P3EA_82+1Q', 'P3EA_82+2MON', 'P3EA_82+2Q', 'P3EA_82+3MON', 'P3EA_82+3Q']\n",
      "\n",
      "Total forward contracts: 22\n",
      "\n",
      "✅ P1EA_82CURMON found (for P1A FFA spread calculation)\n",
      "✅ P3EA_82CURMON found (for P3A FFA spread calculation)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.870867Z",
     "start_time": "2025-11-01T08:18:57.841682Z"
    }
   },
   "source": [
    "# Save BFA wide format\n",
    "bfa_wide_file = INTERMEDIATE_DIR / 'bfa_wide.csv'\n",
    "bfa_wide.to_csv(bfa_wide_file, index=False)\n",
    "print(f\"\\n✅ BFA wide format saved to: {bfa_wide_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BFA wide format saved to: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\bfa_wide.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Load Other Baltic Exchange Data (BPI, C5TC, P4_82, PDOPEX)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.914225Z",
     "start_time": "2025-11-01T08:18:57.904908Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: LOAD BALTIC EXCHANGE INDICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract non-label columns from Baltic Exchange data\n",
    "print(\"\\nExtracting Baltic indices (BPI, C5TC, P4_82, PDOPEX)...\")\n",
    "baltic_features = baltic_df[['Date', 'BPI', 'C5TC', 'P4_82', 'PDOPEX']].copy()\n",
    "\n",
    "print(f\"\\nBaltic Features:\")\n",
    "print(f\"  Rows: {len(baltic_features)}\")\n",
    "print(f\"  Columns: {list(baltic_features.columns)}\")\n",
    "print(f\"  Date range: {baltic_features['Date'].min()} to {baltic_features['Date'].max()}\")\n",
    "\n",
    "print(f\"\\nMissing Value Summary:\")\n",
    "print(baltic_features.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: LOAD BALTIC EXCHANGE INDICES\n",
      "================================================================================\n",
      "\n",
      "Extracting Baltic indices (BPI, C5TC, P4_82, PDOPEX)...\n",
      "\n",
      "Baltic Features:\n",
      "  Rows: 1153\n",
      "  Columns: ['Date', 'BPI', 'C5TC', 'P4_82', 'PDOPEX']\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Missing Value Summary:\n",
      "Date         0\n",
      "BPI          0\n",
      "C5TC         0\n",
      "P4_82        0\n",
      "PDOPEX    1134\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Bunker Data (VLSFO, MGO)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:57.985009Z",
     "start_time": "2025-11-01T08:18:57.965006Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: LOAD BUNKER DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "bunker_file = BUNKER_DIR / 'bunker_data_010321_101025.csv'\n",
    "print(f\"\\nLoading bunker data from: {bunker_file}\")\n",
    "\n",
    "bunker_df = pd.read_csv(bunker_file, parse_dates=['Date'])\n",
    "print(f\"  Loaded {len(bunker_df)} rows, {len(bunker_df.columns)} columns\")\n",
    "print(f\"  Columns: {list(bunker_df.columns)}\")\n",
    "\n",
    "# Filter to study period\n",
    "bunker_df = bunker_df[bunker_df['Date'] >= STUDY_START].copy()\n",
    "print(f\"\\nFiltered to study period: {len(bunker_df)} rows\")\n",
    "print(f\"  Date range: {bunker_df['Date'].min()} to {bunker_df['Date'].max()}\")\n",
    "\n",
    "print(f\"\\nBunker Statistics:\")\n",
    "print(bunker_df[['VLSFO', 'MGO']].describe())\n",
    "\n",
    "print(f\"\\nMissing Value Summary:\")\n",
    "print(bunker_df.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: LOAD BUNKER DATA\n",
      "================================================================================\n",
      "\n",
      "Loading bunker data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\bunker\\bunker_data_010321_101025.csv\n",
      "  Loaded 1201 rows, 3 columns\n",
      "  Columns: ['Date', 'VLSFO', 'MGO']\n",
      "\n",
      "Filtered to study period: 1201 rows\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Bunker Statistics:\n",
      "             VLSFO          MGO\n",
      "count  1201.000000  1201.000000\n",
      "mean    647.689425   858.333888\n",
      "std     123.301596   189.088041\n",
      "min     493.500000   556.000000\n",
      "25%     571.000000   746.500000\n",
      "50%     623.000000   804.000000\n",
      "75%     671.000000   947.500000\n",
      "max    1125.500000  1431.500000\n",
      "\n",
      "Missing Value Summary:\n",
      "Date     0\n",
      "VLSFO    0\n",
      "MGO      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Load Clarksons Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.082313Z",
     "start_time": "2025-11-01T08:18:58.053060Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: LOAD CLARKSONS DAILY DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "clarksons_daily_file = CLARKSONS_DIR / 'clarksons_daily_data.csv'\n",
    "print(f\"\\nLoading Clarksons daily data from: {clarksons_daily_file}\")\n",
    "\n",
    "clarksons_daily = pd.read_csv(clarksons_daily_file, parse_dates=['Date'])\n",
    "print(f\"  Loaded {len(clarksons_daily)} rows, {len(clarksons_daily.columns)} columns\")\n",
    "print(f\"  Columns: {list(clarksons_daily.columns)}\")\n",
    "\n",
    "# Filter to study period\n",
    "clarksons_daily = clarksons_daily[clarksons_daily['Date'] >= STUDY_START].copy()\n",
    "print(f\"\\nFiltered to study period: {len(clarksons_daily)} rows\")\n",
    "print(f\"  Date range: {clarksons_daily['Date'].min()} to {clarksons_daily['Date'].max()}\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "clarksons_daily = clarksons_daily.rename(columns={\n",
    "    'Panamax Bulker - % Idle, DWT million': 'Panamax_Idle_Pct',\n",
    "    'Atlantic Region Port Calls - Deep Sea Cargo Vessels, 7 day avg., Number': 'Atlantic_Port_Calls'\n",
    "})\n",
    "\n",
    "print(f\"\\nRenamed columns: {list(clarksons_daily.columns)}\")\n",
    "\n",
    "print(f\"\\nDaily Features Statistics:\")\n",
    "print(clarksons_daily[['Panamax_Idle_Pct', 'Atlantic_Port_Calls']].describe())\n",
    "\n",
    "print(f\"\\nMissing Value Summary:\")\n",
    "print(clarksons_daily.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: LOAD CLARKSONS DAILY DATA\n",
      "================================================================================\n",
      "\n",
      "Loading Clarksons daily data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\clarksons\\clarksons_daily_data.csv\n",
      "  Loaded 1685 rows, 3 columns\n",
      "  Columns: ['Date', 'Panamax Bulker - % Idle, DWT million', 'Atlantic Region Port Calls - Deep Sea Cargo Vessels, 7 day avg., Number']\n",
      "\n",
      "Filtered to study period: 1685 rows\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Renamed columns: ['Date', 'Panamax_Idle_Pct', 'Atlantic_Port_Calls']\n",
      "\n",
      "Daily Features Statistics:\n",
      "       Panamax_Idle_Pct  Atlantic_Port_Calls\n",
      "count       1685.000000          1685.000000\n",
      "mean           6.345519           446.032047\n",
      "std            1.094566            13.994491\n",
      "min            3.900000           400.000000\n",
      "25%            5.500000           436.000000\n",
      "50%            6.200000           446.000000\n",
      "75%            7.100000           457.000000\n",
      "max            8.900000           482.000000\n",
      "\n",
      "Missing Value Summary:\n",
      "Date                   0\n",
      "Panamax_Idle_Pct       0\n",
      "Atlantic_Port_Calls    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Load Clarksons Weekly Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.149136Z",
     "start_time": "2025-11-01T08:18:58.125870Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: LOAD CLARKSONS WEEKLY DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "clarksons_weekly_file = CLARKSONS_DIR / 'clarksons_weekly_data.csv'\n",
    "print(f\"\\nLoading Clarksons weekly data from: {clarksons_weekly_file}\")\n",
    "\n",
    "clarksons_weekly = pd.read_csv(clarksons_weekly_file, parse_dates=['Date'])\n",
    "print(f\"  Loaded {len(clarksons_weekly)} rows, {len(clarksons_weekly.columns)} columns\")\n",
    "print(f\"  Columns: {list(clarksons_weekly.columns)}\")\n",
    "\n",
    "# NOTE: Weekly data includes pre-study period (2021-02-26) for forward-fill initialization\n",
    "print(f\"\\nDate range (includes pre-study period): {clarksons_weekly['Date'].min()} to {clarksons_weekly['Date'].max()}\")\n",
    "\n",
    "# Remove thousand separators from numeric columns\n",
    "for col in clarksons_weekly.columns:\n",
    "    if col != 'Date':\n",
    "        clarksons_weekly[col] = clarksons_weekly[col].str.replace(',', '').astype(float)\n",
    "\n",
    "# Rename columns for clarity\n",
    "clarksons_weekly = clarksons_weekly.rename(columns={\n",
    "    '5 Year Timecharter Rate 82,000 dwt Bulkcarrier (Atlantic Region), $/day': 'TC5yr_Atlantic',\n",
    "    '5 Year Timecharter Rate 82,000 dwt Bulkcarrier (Pacific Region), $/day': 'TC5yr_Pacific'\n",
    "})\n",
    "\n",
    "print(f\"\\nRenamed columns: {list(clarksons_weekly.columns)}\")\n",
    "\n",
    "print(f\"\\nWeekly Features Statistics:\")\n",
    "print(clarksons_weekly[['TC5yr_Atlantic', 'TC5yr_Pacific']].describe())\n",
    "\n",
    "print(f\"\\nMissing Value Summary:\")\n",
    "print(clarksons_weekly.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: LOAD CLARKSONS WEEKLY DATA\n",
      "================================================================================\n",
      "\n",
      "Loading Clarksons weekly data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\clarksons\\clarksons_weekly_data.csv\n",
      "  Loaded 244 rows, 3 columns\n",
      "  Columns: ['Date', '5 Year Timecharter Rate 82,000 dwt Bulkcarrier (Atlantic Region), $/day', '5 Year Timecharter Rate 82,000 dwt Bulkcarrier (Pacific Region), $/day']\n",
      "\n",
      "Date range (includes pre-study period): 2021-02-26 00:00:00 to 2025-10-24 00:00:00\n",
      "\n",
      "Renamed columns: ['Date', 'TC5yr_Atlantic', 'TC5yr_Pacific']\n",
      "\n",
      "Weekly Features Statistics:\n",
      "       TC5yr_Atlantic  TC5yr_Pacific\n",
      "count      244.000000     244.000000\n",
      "mean     14148.971311   14016.372951\n",
      "std       1512.014086    1634.788486\n",
      "min      11000.000000   11750.000000\n",
      "25%      13000.000000   12650.000000\n",
      "50%      13750.000000   13700.000000\n",
      "75%      15400.000000   14800.000000\n",
      "max      17500.000000   18000.000000\n",
      "\n",
      "Missing Value Summary:\n",
      "Date              0\n",
      "TC5yr_Atlantic    0\n",
      "TC5yr_Pacific     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Load Clarksons Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.229886Z",
     "start_time": "2025-11-01T08:18:58.189146Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"STEP 7: LOAD CLARKSONS MONTHLY DATA\")\nprint(\"=\"*80)\n\nclarksons_monthly_file = CLARKSONS_DIR / 'clarksons_monthly_data.csv'\nprint(f\"\\nLoading Clarksons monthly data from: {clarksons_monthly_file}\")\n\nclarksons_monthly = pd.read_csv(clarksons_monthly_file, parse_dates=['Date'])\nprint(f\"  Loaded {len(clarksons_monthly)} rows, {len(clarksons_monthly.columns)} columns\")\nprint(f\"  Columns: {list(clarksons_monthly.columns)}\")\n\n# Filter to study period (include pre-study period for forward-fill)\n# Keep 2021-02 for forward-fill initialization\nclarksons_monthly = clarksons_monthly[clarksons_monthly['Date'] >= '2021-02-01'].copy()\nprint(f\"\\nFiltered to include pre-study period (>= 2021-02-01): {len(clarksons_monthly)} rows\")\nprint(f\"  Date range: {clarksons_monthly['Date'].min()} to {clarksons_monthly['Date'].max()}\")\n\n# Remove thousand separators from numeric columns\nfor col in clarksons_monthly.columns:\n    if col != 'Date' and clarksons_monthly[col].dtype == 'object':\n        clarksons_monthly[col] = clarksons_monthly[col].str.replace(',', '').astype(float)\n\n# Rename columns for clarity\nclarksons_monthly = clarksons_monthly.rename(columns={\n    'Atlantic Region Industrial Production Growth, % Yr/Yr': 'Atlantic_IP_YoY',\n    'Pacific Region Industrial Production Growth, % Yr/Yr': 'Pacific_IP_YoY',\n    'Panamax Bulkcarrier Deliveries, DWT': 'Panamax_Deliveries_DWT',\n    'Capesize Orderbook % Fleet': 'Capesize_Orderbook_Pct',\n    'Panamax Fleet Growth, % Yr/Yr': 'Panamax_Fleet_Growth_YoY',\n    'Panamax Orderbook % Fleet': 'Panamax_Orderbook_Pct',\n    'Monthly Global Seaborne Coal Trade Indicator, % Yr/Yr': 'Coal_Trade_YoY',\n    'Monthly Global Seaborne Coal Trade Indicator, % Yr/Yr 3mma': 'Coal_Trade_YoY_3mma',\n    'Monthly Global Seaborne Coal Trade Indicator, Volume Index': 'Coal_Trade_Volume_Index',\n    'Monthly Global Seaborne Grain Trade Indicator, % Yr/Yr': 'Grain_Trade_YoY',\n    'Monthly Global Seaborne Grain Trade Indicator, % Yr/Yr 3mma': 'Grain_Trade_YoY_3mma',\n    'Monthly Global Seaborne Grain Trade Indicator, Volume Index': 'Grain_Trade_Volume_Index'\n})\n\nprint(f\"\\nRenamed columns: {list(clarksons_monthly.columns)}\")\n\nprint(f\"\\nMonthly Features Statistics:\")\nprint(clarksons_monthly.describe())\n\nprint(f\"\\nMissing Value Summary:\")\nprint(clarksons_monthly.isnull().sum())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: LOAD CLARKSONS MONTHLY DATA\n",
      "================================================================================\n",
      "\n",
      "Loading Clarksons monthly data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\clarksons\\clarksons_monthly_data.csv\n",
      "  Loaded 57 rows, 13 columns\n",
      "  Columns: ['Date', 'Atlantic Region Industrial Production Growth, % Yr/Yr', 'Pacific Region Industrial Production Growth, % Yr/Yr', 'Panamax Bulkcarrier Deliveries, DWT', 'Capesize Orderbook % Fleet', 'Panamax Fleet Growth, % Yr/Yr', 'Panamax Orderbook % Fleet', 'Monthly Global Seaborne Coal Trade Indicator, % Yr/Yr', 'Monthly Global Seaborne Coal Trade Indicator, % Yr/Yr 3mma', 'Monthly Global Seaborne Coal Trade Indicator, Volume Index', 'Monthly Global Seaborne Grain Trade Indicator, % Yr/Yr', 'Monthly Global Seaborne Grain Trade Indicator, % Yr/Yr 3mma', 'Monthly Global Seaborne Grain Trade Indicator, Volume Index']\n",
      "\n",
      "Filtered to include pre-study period (>= 2021-02-01): 57 rows\n",
      "  Date range: 2021-02-01 00:00:00 to 2025-10-01 00:00:00\n",
      "\n",
      "Renamed columns: ['Date', 'Atlantic_IP_YoY', 'Pacific_IP_YoY', 'Panamax_Deliveries_DWT', 'Capesize_Orderbook_Pct', 'Panamax_Fleet_Growth_YoY', 'Panamax_Orderbook_Pct', 'Coal_Trade_YoY', 'Coal_Trade_YoY_3mma', 'Coal_Trade_Volume_Index', 'Grain_Trade_YoY', 'Grain_Trade_YoY_3mma', 'Grain_Trade_Volume_Index']\n",
      "\n",
      "Monthly Features Statistics:\n",
      "                                Date  Atlantic_IP_YoY  Pacific_IP_YoY  \\\n",
      "count                             57        53.000000       53.000000   \n",
      "mean   2023-06-01 14:44:12.631578880         2.026226        5.767736   \n",
      "min              2021-02-01 00:00:00        -3.930000       -0.930000   \n",
      "25%              2022-04-01 00:00:00        -0.650000        2.710000   \n",
      "50%              2023-06-01 00:00:00         0.450000        4.030000   \n",
      "75%              2024-08-01 00:00:00         3.400000        5.130000   \n",
      "max              2025-10-01 00:00:00        27.020000       31.200000   \n",
      "std                              NaN         4.937867        6.528519   \n",
      "\n",
      "       Panamax_Deliveries_DWT  Capesize_Orderbook_Pct  \\\n",
      "count            5.600000e+01               57.000000   \n",
      "mean             8.251457e+05                7.199649   \n",
      "min              2.422700e+05                5.150000   \n",
      "25%              6.555335e+05                6.310000   \n",
      "50%              8.233250e+05                7.110000   \n",
      "75%              9.839368e+05                8.170000   \n",
      "max              1.487515e+06                9.520000   \n",
      "std              2.922754e+05                1.193297   \n",
      "\n",
      "       Panamax_Fleet_Growth_YoY  Panamax_Orderbook_Pct  Coal_Trade_YoY  \\\n",
      "count                 57.000000              57.000000       56.000000   \n",
      "mean                   3.519649              11.907544        2.730893   \n",
      "min                    2.880000               7.530000      -23.790000   \n",
      "25%                    3.270000              10.160000       -2.547500   \n",
      "50%                    3.490000              10.970000        3.120000   \n",
      "75%                    3.700000              14.320000        7.852500   \n",
      "max                    4.650000              15.880000       31.740000   \n",
      "std                    0.392201               2.465755        8.540416   \n",
      "\n",
      "       Coal_Trade_YoY_3mma  Coal_Trade_Volume_Index  Grain_Trade_YoY  \\\n",
      "count            56.000000                56.000000        56.000000   \n",
      "mean              2.666607               107.056607         0.189464   \n",
      "min             -13.830000                75.260000       -17.640000   \n",
      "25%              -1.057500               103.345000        -5.147500   \n",
      "50%               2.535000               107.565000        -0.665000   \n",
      "75%               7.392500               112.815000         6.507500   \n",
      "max              15.670000               121.900000        18.690000   \n",
      "std               6.854272                 8.426757         8.502324   \n",
      "\n",
      "       Grain_Trade_YoY_3mma  Grain_Trade_Volume_Index  \n",
      "count             56.000000                 56.000000  \n",
      "mean               0.148393                130.743036  \n",
      "min              -16.330000                109.510000  \n",
      "25%               -4.117500                122.025000  \n",
      "50%               -0.150000                129.190000  \n",
      "75%                4.560000                138.457500  \n",
      "max               14.500000                155.070000  \n",
      "std                6.495092                 11.166943  \n",
      "\n",
      "Missing Value Summary:\n",
      "Date                        0\n",
      "Atlantic_IP_YoY             4\n",
      "Pacific_IP_YoY              4\n",
      "Panamax_Deliveries_DWT      1\n",
      "Capesize_Orderbook_Pct      0\n",
      "Panamax_Fleet_Growth_YoY    0\n",
      "Panamax_Orderbook_Pct       0\n",
      "Coal_Trade_YoY              1\n",
      "Coal_Trade_YoY_3mma         1\n",
      "Coal_Trade_Volume_Index     1\n",
      "Grain_Trade_YoY             1\n",
      "Grain_Trade_YoY_3mma        1\n",
      "Grain_Trade_Volume_Index    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Load Clarksons Annual Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.302521Z",
     "start_time": "2025-11-01T08:18:58.247402Z"
    }
   },
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"STEP 8: LOAD CLARKSONS ANNUAL DATA\")\nprint(\"=\"*80)\n\nclarksons_annual_file = CLARKSONS_DIR / 'clarksons_annual_data.csv'\nprint(f\"\\nLoading Clarksons annual data from: {clarksons_annual_file}\")\n\nclarksons_annual = pd.read_csv(clarksons_annual_file)\nprint(f\"  Loaded {len(clarksons_annual)} rows, {len(clarksons_annual.columns)} columns\")\nprint(f\"  Columns: {list(clarksons_annual.columns)}\")\n\n# Convert Date to datetime (year format)\nclarksons_annual['Date'] = pd.to_datetime(clarksons_annual['Date'], format='%Y')\n\n# Display all available years\nprint(f\"\\nAll available years in annual data: {sorted(clarksons_annual['Date'].dt.year.unique())}\")\n\n# Include 2020 and 2021 for forward-fill initialization (pre-study period)\n# No filtering needed - use all available years\nprint(f\"\\nUsing all {len(clarksons_annual)} rows (no year filtering)\")\nprint(f\"  Date range: {clarksons_annual['Date'].min().year} to {clarksons_annual['Date'].max().year}\")\n\n# Remove thousand separators from numeric columns\nfor col in clarksons_annual.columns:\n    if col != 'Date' and clarksons_annual[col].dtype == 'object':\n        clarksons_annual[col] = clarksons_annual[col].str.replace(',', '').astype(float)\n\n# Rename columns for clarity\nclarksons_annual = clarksons_annual.rename(columns={\n    'China Seaborne Coal Imports, Million Tonnes': 'China_Coal_Imports_MT',\n    'China Seaborne Grain Imports, Million Tonnes': 'China_Grain_Imports_MT',\n    'World Seaborne Grain Trade (including Soybeans), Million Tonnes': 'World_Grain_Trade_MT',\n    'World Seaborne Grain Trade (including Soybeans), Billion Tonne-miles': 'World_Grain_Trade_BTM',\n    'World Seaborne Grain Trade (including Soybeans), % Yr/Yr (tonnes)': 'World_Grain_Trade_YoY_MT',\n    'World Seaborne Grain Trade (including Soybeans), % Yr/Yr (tonne-miles)': 'World_Grain_Trade_YoY_BTM',\n    'India Seaborne Coal Imports, Million Tonnes': 'India_Coal_Imports_MT',\n    'Japan Seaborne Coal Imports, Million Tonnes': 'Japan_Coal_Imports_MT',\n    'Indonesia Seaborne Coal Exports, Million Tonnes': 'Indonesia_Coal_Exports_MT',\n    'Australia Seaborne Coal Exports, Million Tonnes': 'Australia_Coal_Exports_MT',\n    'World Seaborne Total Coal Trade, Billion Tonne-miles': 'World_Coal_Trade_BTM',\n    'World Seaborne Total Coal Trade, Million Tonnes': 'World_Coal_Trade_MT',\n    'World Seaborne Total Coal Trade, % Yr/Yr (tonnes)': 'World_Coal_Trade_YoY_MT',\n    'World Seaborne Total Coal Trade, % Yr/Yr (tonne-miles)': 'World_Coal_Trade_YoY_BTM'\n})\n\nprint(f\"\\nRenamed columns: {list(clarksons_annual.columns)}\")\n\nprint(f\"\\nAnnual Features Statistics:\")\nprint(clarksons_annual.describe())\n\nprint(f\"\\nMissing Value Summary:\")\nprint(clarksons_annual.isnull().sum())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: LOAD CLARKSONS ANNUAL DATA\n",
      "================================================================================\n",
      "\n",
      "Loading Clarksons annual data from: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\raw\\clarksons\\clarksons_annual_data.csv\n",
      "  Loaded 8 rows, 15 columns\n",
      "  Columns: ['Date', 'China Seaborne Coal Imports, Million Tonnes', 'China Seaborne Grain Imports, Million Tonnes', 'World Seaborne Grain Trade (including Soybeans), Million Tonnes', 'World Seaborne Grain Trade (including Soybeans), Billion Tonne-miles', 'World Seaborne Grain Trade (including Soybeans), % Yr/Yr (tonnes)', 'World Seaborne Grain Trade (including Soybeans), % Yr/Yr (tonne-miles)', 'India Seaborne Coal Imports, Million Tonnes', 'Japan Seaborne Coal Imports, Million Tonnes', 'Indonesia Seaborne Coal Exports, Million Tonnes', 'Australia Seaborne Coal Exports, Million Tonnes', 'World Seaborne Total Coal Trade, Billion Tonne-miles', 'World Seaborne Total Coal Trade, Million Tonnes', 'World Seaborne Total Coal Trade, % Yr/Yr (tonnes)', 'World Seaborne Total Coal Trade, % Yr/Yr (tonne-miles)']\n",
      "\n",
      "All available years in annual data: [np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025), np.int32(2026), np.int32(2027)]\n",
      "\n",
      "Using all 8 rows (no year filtering)\n",
      "  Date range: 2020 to 2027\n",
      "\n",
      "Renamed columns: ['Date', 'China_Coal_Imports_MT', 'China_Grain_Imports_MT', 'World_Grain_Trade_MT', 'World_Grain_Trade_BTM', 'World_Grain_Trade_YoY_MT', 'World_Grain_Trade_YoY_BTM', 'India_Coal_Imports_MT', 'Japan_Coal_Imports_MT', 'Indonesia_Coal_Exports_MT', 'Australia_Coal_Exports_MT', 'World_Coal_Trade_BTM', 'World_Coal_Trade_MT', 'World_Coal_Trade_YoY_MT', 'World_Coal_Trade_YoY_BTM']\n",
      "\n",
      "Annual Features Statistics:\n",
      "                      Date  China_Coal_Imports_MT  China_Grain_Imports_MT  \\\n",
      "count                    8               8.000000                8.000000   \n",
      "mean   2023-07-02 18:00:00             324.106250              141.747500   \n",
      "min    2020-01-01 00:00:00             233.660000              131.520000   \n",
      "25%    2021-10-01 18:00:00             269.965000              134.450000   \n",
      "50%    2023-07-02 12:00:00             342.635000              138.020000   \n",
      "75%    2025-04-02 06:00:00             363.047500              151.415000   \n",
      "max    2027-01-01 00:00:00             421.290000              155.080000   \n",
      "std                    NaN              67.178719                9.577032   \n",
      "\n",
      "       World_Grain_Trade_MT  World_Grain_Trade_BTM  World_Grain_Trade_YoY_MT  \\\n",
      "count              8.000000               8.000000                  8.000000   \n",
      "mean             540.341250            3769.706250                  2.461250   \n",
      "min              514.320000            3535.720000                 -2.770000   \n",
      "25%              521.547500            3666.730000                  1.672500   \n",
      "50%              536.135000            3710.235000                  2.185000   \n",
      "75%              555.642500            3835.967500                  2.825000   \n",
      "max              577.780000            4123.010000                  8.400000   \n",
      "std               23.425753             193.410317                  3.073121   \n",
      "\n",
      "       World_Grain_Trade_YoY_BTM  India_Coal_Imports_MT  \\\n",
      "count                    8.00000               8.000000   \n",
      "mean                     2.92875             239.442500   \n",
      "min                     -3.92000             203.980000   \n",
      "25%                      1.13000             231.912500   \n",
      "50%                      2.72000             242.240000   \n",
      "75%                      4.96000             250.140000   \n",
      "max                     10.19000             264.770000   \n",
      "std                      4.13888              18.510761   \n",
      "\n",
      "       Japan_Coal_Imports_MT  Indonesia_Coal_Exports_MT  \\\n",
      "count               8.000000                   8.000000   \n",
      "mean              165.788750                 488.773750   \n",
      "min               157.390000                 404.400000   \n",
      "25%               160.415000                 456.850000   \n",
      "50%               162.710000                 502.200000   \n",
      "75%               170.280000                 521.592500   \n",
      "max               178.080000                 556.500000   \n",
      "std                 7.724555                  51.336273   \n",
      "\n",
      "       Australia_Coal_Exports_MT  World_Coal_Trade_BTM  World_Coal_Trade_MT  \\\n",
      "count                   8.000000              8.000000             8.000000   \n",
      "mean                  358.262500           5263.013750          1278.832500   \n",
      "min                   339.190000           4550.510000          1182.200000   \n",
      "25%                   356.387500           4956.735000          1234.962500   \n",
      "50%                   358.485000           5415.110000          1279.035000   \n",
      "75%                   362.927500           5504.647500          1316.767500   \n",
      "max                   371.340000           5862.030000          1378.150000   \n",
      "std                     9.489124            428.786289            62.660056   \n",
      "\n",
      "       World_Coal_Trade_YoY_MT  World_Coal_Trade_YoY_BTM  \n",
      "count                 8.000000                  8.000000  \n",
      "mean                 -0.143750                  1.135000  \n",
      "min                  -9.050000                 -9.090000  \n",
      "25%                  -2.642500                 -2.372500  \n",
      "50%                  -0.330000                  1.170000  \n",
      "75%                   3.512500                  6.157500  \n",
      "max                   7.900000                 10.650000  \n",
      "std                   5.356014                  6.700333  \n",
      "\n",
      "Missing Value Summary:\n",
      "Date                         0\n",
      "China_Coal_Imports_MT        0\n",
      "China_Grain_Imports_MT       0\n",
      "World_Grain_Trade_MT         0\n",
      "World_Grain_Trade_BTM        0\n",
      "World_Grain_Trade_YoY_MT     0\n",
      "World_Grain_Trade_YoY_BTM    0\n",
      "India_Coal_Imports_MT        0\n",
      "Japan_Coal_Imports_MT        0\n",
      "Indonesia_Coal_Exports_MT    0\n",
      "Australia_Coal_Exports_MT    0\n",
      "World_Coal_Trade_BTM         0\n",
      "World_Coal_Trade_MT          0\n",
      "World_Coal_Trade_YoY_MT      0\n",
      "World_Coal_Trade_YoY_BTM     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Align All Features to Master Calendar (Forward-Fill)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.331359Z",
     "start_time": "2025-11-01T08:18:58.322354Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: ALIGN ALL FEATURES TO MASTER CALENDAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nMaster Calendar Details:\")\n",
    "print(f\"  Total days: {len(master_calendar)}\")\n",
    "print(f\"  Start: {master_calendar['Date'].min()}\")\n",
    "print(f\"  End: {master_calendar['Date'].max()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: ALIGN ALL FEATURES TO MASTER CALENDAR\n",
      "================================================================================\n",
      "\n",
      "Master Calendar Details:\n",
      "  Total days: 1153\n",
      "  Start: 2021-03-01 00:00:00\n",
      "  End: 2025-10-10 00:00:00\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1: Merge Daily Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.538181Z",
     "start_time": "2025-11-01T08:18:58.403778Z"
    }
   },
   "source": "print(\"\\n\" + \"-\"*80)\nprint(\"9.1: MERGING DAILY FEATURES\")\nprint(\"-\"*80)\n\n# Start with master calendar\nfeatures_daily = master_calendar.copy()\nprint(f\"\\nStarting with master calendar: {len(features_daily)} rows\")\n\n# Merge Baltic features (BPI, C5TC, P4_82, PDOPEX)\nprint(\"\\nMerging Baltic features (BPI, C5TC, P4_82, PDOPEX)...\")\nfeatures_daily = features_daily.merge(baltic_features, on='Date', how='left')\nprint(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n\n# Forward-fill PDOPEX (has value at 2021-03-01, needs forward-fill for subsequent dates)\nprint(\"\\n  Forward-filling PDOPEX (sparse daily feature)...\")\npdopex_before = features_daily['PDOPEX'].isnull().sum()\nfeatures_daily['PDOPEX'] = features_daily['PDOPEX'].fillna(method='ffill')\npdopex_after = features_daily['PDOPEX'].isnull().sum()\nprint(f\"    PDOPEX missing: {pdopex_before} → {pdopex_after} (filled {pdopex_before - pdopex_after} values)\")\n\n# Merge BFA wide (FFA forward curves)\nprint(\"\\nMerging BFA wide (FFA forward curves)...\")\nfeatures_daily = features_daily.merge(bfa_wide, on='Date', how='left')\nprint(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n\n# Merge Bunker data (VLSFO, MGO)\nprint(\"\\nMerging Bunker data (VLSFO, MGO)...\")\nfeatures_daily = features_daily.merge(bunker_df, on='Date', how='left')\nprint(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n\n# Merge Clarksons daily (Panamax_Idle_Pct, Atlantic_Port_Calls)\nprint(\"\\nMerging Clarksons daily (Panamax_Idle_Pct, Atlantic_Port_Calls)...\")\nfeatures_daily = features_daily.merge(clarksons_daily, on='Date', how='left')\nprint(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n\nprint(f\"\\n✅ Daily features merged. Total columns: {len(features_daily.columns)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9.1: MERGING DAILY FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Starting with master calendar: 1153 rows\n",
      "\n",
      "Merging Baltic features (BPI, C5TC, P4_82, PDOPEX)...\n",
      "  After merge: 1153 rows, 5 columns\n",
      "\n",
      "  Forward-filling PDOPEX (sparse daily feature)...\n",
      "    PDOPEX missing: 1134 → 0 (filled 1134 values)\n",
      "\n",
      "Merging BFA wide (FFA forward curves)...\n",
      "  After merge: 1153 rows, 27 columns\n",
      "\n",
      "Merging Bunker data (VLSFO, MGO)...\n",
      "  After merge: 1153 rows, 29 columns\n",
      "\n",
      "Merging Clarksons daily (Panamax_Idle_Pct, Atlantic_Port_Calls)...\n",
      "  After merge: 1153 rows, 31 columns\n",
      "\n",
      "✅ Daily features merged. Total columns: 31\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2: Merge and Forward-Fill Weekly Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.649127Z",
     "start_time": "2025-11-01T08:18:58.550071Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"9.2: MERGING AND FORWARD-FILLING WEEKLY FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nWeekly features: TC5yr_Atlantic, TC5yr_Pacific\")\n",
    "print(f\"  Weekly data rows: {len(clarksons_weekly)}\")\n",
    "print(f\"  Date range: {clarksons_weekly['Date'].min()} to {clarksons_weekly['Date'].max()}\")\n",
    "\n",
    "# Merge weekly data\n",
    "features_daily = features_daily.merge(clarksons_weekly, on='Date', how='left')\n",
    "print(f\"\\nAfter merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n",
    "\n",
    "# Check missing values before forward-fill\n",
    "weekly_cols = ['TC5yr_Atlantic', 'TC5yr_Pacific']\n",
    "print(f\"\\nMissing values BEFORE forward-fill:\")\n",
    "for col in weekly_cols:\n",
    "    missing_count = features_daily[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing_count} ({missing_count/len(features_daily)*100:.1f}%)\")\n",
    "\n",
    "# Forward-fill weekly data to daily frequency\n",
    "print(f\"\\nApplying forward-fill...\")\n",
    "features_daily[weekly_cols] = features_daily[weekly_cols].fillna(method='ffill')\n",
    "\n",
    "# Check missing values after forward-fill\n",
    "print(f\"\\nMissing values AFTER forward-fill:\")\n",
    "for col in weekly_cols:\n",
    "    missing_count = features_daily[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing_count}\")\n",
    "    if missing_count > 0:\n",
    "        print(f\"    ⚠️ WARNING: Still has missing values after forward-fill\")\n",
    "\n",
    "print(f\"\\n✅ Weekly features forward-filled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9.2: MERGING AND FORWARD-FILLING WEEKLY FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Weekly features: TC5yr_Atlantic, TC5yr_Pacific\n",
      "  Weekly data rows: 244\n",
      "  Date range: 2021-02-26 00:00:00 to 2025-10-24 00:00:00\n",
      "\n",
      "After merge: 1153 rows, 33 columns\n",
      "\n",
      "Missing values BEFORE forward-fill:\n",
      "  TC5yr_Atlantic: 922 (80.0%)\n",
      "  TC5yr_Pacific: 922 (80.0%)\n",
      "\n",
      "Applying forward-fill...\n",
      "\n",
      "Missing values AFTER forward-fill:\n",
      "  TC5yr_Atlantic: 4\n",
      "    ⚠️ WARNING: Still has missing values after forward-fill\n",
      "  TC5yr_Pacific: 4\n",
      "    ⚠️ WARNING: Still has missing values after forward-fill\n",
      "\n",
      "✅ Weekly features forward-filled\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3: Merge and Forward-Fill Monthly Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.704656Z",
     "start_time": "2025-11-01T08:18:58.672596Z"
    }
   },
   "source": "print(\"\\n\" + \"-\"*80)\nprint(\"9.3: MERGING AND FORWARD-FILLING MONTHLY FEATURES\")\nprint(\"-\"*80)\n\nmonthly_feature_cols = [\n    'Atlantic_IP_YoY', 'Pacific_IP_YoY', 'Panamax_Deliveries_DWT',\n    'Capesize_Orderbook_Pct', 'Panamax_Fleet_Growth_YoY', 'Panamax_Orderbook_Pct',\n    'Coal_Trade_YoY', 'Coal_Trade_YoY_3mma', 'Coal_Trade_Volume_Index',\n    'Grain_Trade_YoY', 'Grain_Trade_YoY_3mma', 'Grain_Trade_Volume_Index'\n]\n\nprint(f\"\\nMonthly features: {len(monthly_feature_cols)} columns\")\nprint(f\"  {monthly_feature_cols}\")\nprint(f\"\\n  Monthly data rows: {len(clarksons_monthly)}\")\nprint(f\"  Date range: {clarksons_monthly['Date'].min()} to {clarksons_monthly['Date'].max()}\")\n\n# Convert monthly dates to start of month for proper merging\nclarksons_monthly['Date'] = pd.to_datetime(clarksons_monthly['Date'])\nfeatures_daily['YearMonth'] = features_daily['Date'].dt.to_period('M').dt.to_timestamp()\nclarksons_monthly['YearMonth'] = clarksons_monthly['Date'].dt.to_period('M').dt.to_timestamp()\n\n# Merge monthly data on YearMonth\nprint(f\"\\nMerging on YearMonth column...\")\nfeatures_daily = features_daily.merge(\n    clarksons_monthly[['YearMonth'] + monthly_feature_cols],\n    on='YearMonth',\n    how='left'\n)\nprint(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n\n# Check missing values before forward-fill\nprint(f\"\\nMissing values BEFORE forward-fill:\")\nfor col in monthly_feature_cols:\n    missing_count = features_daily[col].isnull().sum()\n    print(f\"  {col}: {missing_count} ({missing_count/len(features_daily)*100:.1f}%)\")\n\n# Forward-fill monthly data to daily frequency\nprint(f\"\\nApplying forward-fill...\")\nfeatures_daily[monthly_feature_cols] = features_daily[monthly_feature_cols].fillna(method='ffill')\n\n# Check missing values after forward-fill\nprint(f\"\\nMissing values AFTER forward-fill:\")\nfor col in monthly_feature_cols:\n    missing_count = features_daily[col].isnull().sum()\n    print(f\"  {col}: {missing_count}\")\n    if missing_count > 0:\n        print(f\"    ⚠️ WARNING: Still has missing values after forward-fill\")\n\n# Drop temporary YearMonth column\nfeatures_daily = features_daily.drop(columns=['YearMonth'])\n\nprint(f\"\\n✅ Monthly features forward-filled\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9.3: MERGING AND FORWARD-FILLING MONTHLY FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Monthly features: 12 columns\n",
      "  ['Atlantic_IP_YoY', 'Pacific_IP_YoY', 'Panamax_Deliveries_DWT', 'Capesize_Orderbook_Pct', 'Panamax_Fleet_Growth_YoY', 'Panamax_Orderbook_Pct', 'Coal_Trade_YoY', 'Coal_Trade_YoY_3mma', 'Coal_Trade_Volume_Index', 'Grain_Trade_YoY', 'Grain_Trade_YoY_3mma', 'Grain_Trade_Volume_Index']\n",
      "\n",
      "  Monthly data rows: 57\n",
      "  Date range: 2021-02-01 00:00:00 to 2025-10-01 00:00:00\n",
      "\n",
      "Merging on YearMonth column...\n",
      "  After merge: 1153 rows, 46 columns\n",
      "\n",
      "Missing values BEFORE forward-fill:\n",
      "  Atlantic_IP_YoY: 73 (6.3%)\n",
      "  Pacific_IP_YoY: 73 (6.3%)\n",
      "  Panamax_Deliveries_DWT: 8 (0.7%)\n",
      "  Capesize_Orderbook_Pct: 0 (0.0%)\n",
      "  Panamax_Fleet_Growth_YoY: 0 (0.0%)\n",
      "  Panamax_Orderbook_Pct: 0 (0.0%)\n",
      "  Coal_Trade_YoY: 8 (0.7%)\n",
      "  Coal_Trade_YoY_3mma: 8 (0.7%)\n",
      "  Coal_Trade_Volume_Index: 8 (0.7%)\n",
      "  Grain_Trade_YoY: 8 (0.7%)\n",
      "  Grain_Trade_YoY_3mma: 8 (0.7%)\n",
      "  Grain_Trade_Volume_Index: 8 (0.7%)\n",
      "\n",
      "Applying forward-fill...\n",
      "\n",
      "Missing values AFTER forward-fill:\n",
      "  Atlantic_IP_YoY: 0\n",
      "  Pacific_IP_YoY: 0\n",
      "  Panamax_Deliveries_DWT: 0\n",
      "  Capesize_Orderbook_Pct: 0\n",
      "  Panamax_Fleet_Growth_YoY: 0\n",
      "  Panamax_Orderbook_Pct: 0\n",
      "  Coal_Trade_YoY: 0\n",
      "  Coal_Trade_YoY_3mma: 0\n",
      "  Coal_Trade_Volume_Index: 0\n",
      "  Grain_Trade_YoY: 0\n",
      "  Grain_Trade_YoY_3mma: 0\n",
      "  Grain_Trade_Volume_Index: 0\n",
      "\n",
      "✅ Monthly features forward-filled\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4: Interpolate and Forward-Fill Annual Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.772647Z",
     "start_time": "2025-11-01T08:18:58.740506Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"9.4: INTERPOLATING AND FORWARD-FILLING ANNUAL FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "annual_feature_cols = [\n",
    "    'China_Coal_Imports_MT', 'China_Grain_Imports_MT',\n",
    "    'World_Grain_Trade_MT', 'World_Grain_Trade_BTM',\n",
    "    'World_Grain_Trade_YoY_MT', 'World_Grain_Trade_YoY_BTM',\n",
    "    'India_Coal_Imports_MT', 'Japan_Coal_Imports_MT',\n",
    "    'Indonesia_Coal_Exports_MT', 'Australia_Coal_Exports_MT',\n",
    "    'World_Coal_Trade_BTM', 'World_Coal_Trade_MT',\n",
    "    'World_Coal_Trade_YoY_MT', 'World_Coal_Trade_YoY_BTM'\n",
    "]\n",
    "\n",
    "print(f\"\\nAnnual features: {len(annual_feature_cols)} columns\")\n",
    "print(f\"  Annual data rows: {len(clarksons_annual)}\")\n",
    "print(f\"  Date range: {clarksons_annual['Date'].min().year} to {clarksons_annual['Date'].max().year}\")\n",
    "\n",
    "# Convert annual dates to start of year\n",
    "features_daily['Year'] = features_daily['Date'].dt.year\n",
    "clarksons_annual['Year'] = clarksons_annual['Date'].dt.year\n",
    "\n",
    "# Merge annual data on Year\n",
    "print(f\"\\nMerging on Year column...\")\n",
    "features_daily = features_daily.merge(\n",
    "    clarksons_annual[['Year'] + annual_feature_cols],\n",
    "    on='Year',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"  After merge: {len(features_daily)} rows, {len(features_daily.columns)} columns\")\n",
    "\n",
    "# Check missing values before interpolation\n",
    "print(f\"\\nMissing values BEFORE interpolation/forward-fill:\")\n",
    "for col in annual_feature_cols:\n",
    "    missing_count = features_daily[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing_count} ({missing_count/len(features_daily)*100:.1f}%)\")\n",
    "\n",
    "# Apply linear interpolation then forward-fill for annual data\n",
    "print(f\"\\nApplying linear interpolation followed by forward-fill...\")\n",
    "for col in annual_feature_cols:\n",
    "    features_daily[col] = features_daily[col].interpolate(method='linear').fillna(method='ffill')\n",
    "\n",
    "# Check missing values after interpolation\n",
    "print(f\"\\nMissing values AFTER interpolation/forward-fill:\")\n",
    "for col in annual_feature_cols:\n",
    "    missing_count = features_daily[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing_count}\")\n",
    "    if missing_count > 0:\n",
    "        print(f\"    ⚠️ WARNING: Still has missing values after interpolation\")\n",
    "\n",
    "# Drop temporary Year column\n",
    "features_daily = features_daily.drop(columns=['Year'])\n",
    "\n",
    "print(f\"\\n✅ Annual features interpolated and forward-filled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9.4: INTERPOLATING AND FORWARD-FILLING ANNUAL FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Annual features: 14 columns\n",
      "  Annual data rows: 8\n",
      "  Date range: 2020 to 2027\n",
      "\n",
      "Merging on Year column...\n",
      "  After merge: 1153 rows, 60 columns\n",
      "\n",
      "Missing values BEFORE interpolation/forward-fill:\n",
      "  China_Coal_Imports_MT: 0 (0.0%)\n",
      "  China_Grain_Imports_MT: 0 (0.0%)\n",
      "  World_Grain_Trade_MT: 0 (0.0%)\n",
      "  World_Grain_Trade_BTM: 0 (0.0%)\n",
      "  World_Grain_Trade_YoY_MT: 0 (0.0%)\n",
      "  World_Grain_Trade_YoY_BTM: 0 (0.0%)\n",
      "  India_Coal_Imports_MT: 0 (0.0%)\n",
      "  Japan_Coal_Imports_MT: 0 (0.0%)\n",
      "  Indonesia_Coal_Exports_MT: 0 (0.0%)\n",
      "  Australia_Coal_Exports_MT: 0 (0.0%)\n",
      "  World_Coal_Trade_BTM: 0 (0.0%)\n",
      "  World_Coal_Trade_MT: 0 (0.0%)\n",
      "  World_Coal_Trade_YoY_MT: 0 (0.0%)\n",
      "  World_Coal_Trade_YoY_BTM: 0 (0.0%)\n",
      "\n",
      "Applying linear interpolation followed by forward-fill...\n",
      "\n",
      "Missing values AFTER interpolation/forward-fill:\n",
      "  China_Coal_Imports_MT: 0\n",
      "  China_Grain_Imports_MT: 0\n",
      "  World_Grain_Trade_MT: 0\n",
      "  World_Grain_Trade_BTM: 0\n",
      "  World_Grain_Trade_YoY_MT: 0\n",
      "  World_Grain_Trade_YoY_BTM: 0\n",
      "  India_Coal_Imports_MT: 0\n",
      "  Japan_Coal_Imports_MT: 0\n",
      "  Indonesia_Coal_Exports_MT: 0\n",
      "  Australia_Coal_Exports_MT: 0\n",
      "  World_Coal_Trade_BTM: 0\n",
      "  World_Coal_Trade_MT: 0\n",
      "  World_Coal_Trade_YoY_MT: 0\n",
      "  World_Coal_Trade_YoY_BTM: 0\n",
      "\n",
      "✅ Annual features interpolated and forward-filled\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"-\"*80)\nprint(\"9.5: APPLYING 1-DAY LAG TO ALL FEATURES (DATA LEAKAGE PREVENTION)\")\nprint(\"-\"*80)\n\nprint(\"\\nStrategy: Apply 1-day lag to ALL features (matching Round 1 approach)\")\nprint(\"  Rationale: Prevents data leakage by ensuring features used for prediction\")\nprint(\"             are only from information available BEFORE the prediction date\")\nprint(\"  Method: Shift all non-Date columns by 1 business day using .shift(1)\")\n\n# Get all feature columns (exclude Date)\nfeature_cols = [col for col in features_daily.columns if col != 'Date']\nprint(f\"\\nTotal features to lag: {len(feature_cols)}\")\n\n# Count missing values BEFORE lagging\nmissing_before = features_daily[feature_cols].isnull().sum().sum()\nprint(f\"\\nMissing values BEFORE lagging: {missing_before}\")\n\n# Apply 1-day lag to ALL features\nprint(f\"\\nApplying 1-day shift to all {len(feature_cols)} features...\")\nfeatures_daily[feature_cols] = features_daily[feature_cols].shift(1)\n\n# Count missing values AFTER lagging\nmissing_after = features_daily[feature_cols].isnull().sum().sum()\nprint(f\"Missing values AFTER lagging: {missing_after}\")\nprint(f\"  Additional missing values from lag: {missing_after - missing_before}\")\nprint(f\"  (This is expected: first row will have NaN for all features)\")\n\n# Verify first row has all NaN features (except Date)\nfirst_row_nan_count = features_daily.iloc[0][feature_cols].isnull().sum()\nprint(f\"\\nFirst row validation:\")\nprint(f\"  Date: {features_daily.iloc[0]['Date']}\")\nprint(f\"  Features with NaN: {first_row_nan_count}/{len(feature_cols)}\")\nif first_row_nan_count == len(feature_cols):\n    print(f\"  ✅ First row correctly has all features as NaN (due to 1-day lag)\")\nelse:\n    print(f\"  ⚠️ WARNING: Expected all features to be NaN in first row!\")\n\n# Show example of lag effect (second row should have first row's original values)\nprint(f\"\\nLag effect example (comparing dates):\")\nprint(f\"  Row 2 Date: {features_daily.iloc[1]['Date']}\")\nprint(f\"  Row 2 BPI: {features_daily.iloc[1]['BPI']}\")\nprint(f\"  (This value was originally from Row 1: {master_calendar.iloc[0]['Date']})\")\n\nprint(f\"\\n✅ 1-day lag applied to all features\")\nprint(f\"\\nFinal dataset shape: {features_daily.shape}\")\nprint(f\"  Rows: {len(features_daily)}\")\nprint(f\"  Columns: {len(features_daily.columns)} (1 Date + {len(feature_cols)} lagged features)\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.842434Z",
     "start_time": "2025-11-01T08:18:58.813791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9.5: APPLYING 1-DAY LAG TO ALL FEATURES (DATA LEAKAGE PREVENTION)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Strategy: Apply 1-day lag to ALL features (matching Round 1 approach)\n",
      "  Rationale: Prevents data leakage by ensuring features used for prediction\n",
      "             are only from information available BEFORE the prediction date\n",
      "  Method: Shift all non-Date columns by 1 business day using .shift(1)\n",
      "\n",
      "Total features to lag: 58\n",
      "\n",
      "Missing values BEFORE lagging: 600\n",
      "\n",
      "Applying 1-day shift to all 58 features...\n",
      "Missing values AFTER lagging: 658\n",
      "  Additional missing values from lag: 58\n",
      "  (This is expected: first row will have NaN for all features)\n",
      "\n",
      "First row validation:\n",
      "  Date: 2021-03-01 00:00:00\n",
      "  Features with NaN: 58/58\n",
      "  ✅ First row correctly has all features as NaN (due to 1-day lag)\n",
      "\n",
      "Lag effect example (comparing dates):\n",
      "  Row 2 Date: 2021-03-02 00:00:00\n",
      "  Row 2 BPI: 2086.0\n",
      "  (This value was originally from Row 1: 2021-03-01 00:00:00)\n",
      "\n",
      "✅ 1-day lag applied to all features\n",
      "\n",
      "Final dataset shape: (1153, 59)\n",
      "  Rows: 1153\n",
      "  Columns: 59 (1 Date + 58 lagged features)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": "### 9.5: Apply 1-Day Lag to ALL Features (Data Leakage Prevention)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Final Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.884175Z",
     "start_time": "2025-11-01T08:18:58.877175Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10: FINAL DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal Dataset Dimensions:\")\n",
    "print(f\"  Rows: {len(features_daily)}\")\n",
    "print(f\"  Columns: {len(features_daily.columns)}\")\n",
    "print(f\"  Date range: {features_daily['Date'].min()} to {features_daily['Date'].max()}\")\n",
    "\n",
    "print(f\"\\nColumn List ({len(features_daily.columns)} total):\")\n",
    "for i, col in enumerate(features_daily.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: FINAL DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Final Dataset Dimensions:\n",
      "  Rows: 1153\n",
      "  Columns: 59\n",
      "  Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Column List (59 total):\n",
      "   1. Date\n",
      "   2. BPI\n",
      "   3. C5TC\n",
      "   4. P4_82\n",
      "   5. PDOPEX\n",
      "   6. P1EA_82+1MON\n",
      "   7. P1EA_82+1Q\n",
      "   8. P1EA_82+2MON\n",
      "   9. P1EA_82+2Q\n",
      "  10. P1EA_82+3MON\n",
      "  11. P1EA_82+3Q\n",
      "  12. P1EA_82+4MON\n",
      "  13. P1EA_82+4Q\n",
      "  14. P1EA_82+5MON\n",
      "  15. P1EA_82CURMON\n",
      "  16. P1EA_82CURQ\n",
      "  17. P3EA_82+1MON\n",
      "  18. P3EA_82+1Q\n",
      "  19. P3EA_82+2MON\n",
      "  20. P3EA_82+2Q\n",
      "  21. P3EA_82+3MON\n",
      "  22. P3EA_82+3Q\n",
      "  23. P3EA_82+4MON\n",
      "  24. P3EA_82+4Q\n",
      "  25. P3EA_82+5MON\n",
      "  26. P3EA_82CURMON\n",
      "  27. P3EA_82CURQ\n",
      "  28. VLSFO\n",
      "  29. MGO\n",
      "  30. Panamax_Idle_Pct\n",
      "  31. Atlantic_Port_Calls\n",
      "  32. TC5yr_Atlantic\n",
      "  33. TC5yr_Pacific\n",
      "  34. Atlantic_IP_YoY\n",
      "  35. Pacific_IP_YoY\n",
      "  36. Panamax_Deliveries_DWT\n",
      "  37. Capesize_Orderbook_Pct\n",
      "  38. Panamax_Fleet_Growth_YoY\n",
      "  39. Panamax_Orderbook_Pct\n",
      "  40. Coal_Trade_YoY\n",
      "  41. Coal_Trade_YoY_3mma\n",
      "  42. Coal_Trade_Volume_Index\n",
      "  43. Grain_Trade_YoY\n",
      "  44. Grain_Trade_YoY_3mma\n",
      "  45. Grain_Trade_Volume_Index\n",
      "  46. China_Coal_Imports_MT\n",
      "  47. China_Grain_Imports_MT\n",
      "  48. World_Grain_Trade_MT\n",
      "  49. World_Grain_Trade_BTM\n",
      "  50. World_Grain_Trade_YoY_MT\n",
      "  51. World_Grain_Trade_YoY_BTM\n",
      "  52. India_Coal_Imports_MT\n",
      "  53. Japan_Coal_Imports_MT\n",
      "  54. Indonesia_Coal_Exports_MT\n",
      "  55. Australia_Coal_Exports_MT\n",
      "  56. World_Coal_Trade_BTM\n",
      "  57. World_Coal_Trade_MT\n",
      "  58. World_Coal_Trade_YoY_MT\n",
      "  59. World_Coal_Trade_YoY_BTM\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.938648Z",
     "start_time": "2025-11-01T08:18:58.923659Z"
    }
   },
   "source": "# Check for missing values across all features\nprint(\"\\n\" + \"-\"*80)\nprint(\"MISSING VALUE SUMMARY (ALL FEATURES)\")\nprint(\"-\"*80)\n\nmissing_summary = features_daily.isnull().sum()\nmissing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n\nprint(f\"\\n**IMPORTANT**: All features have been lagged by 1 day to prevent data leakage.\")\nprint(f\"  This means Row 1 (date={features_daily.iloc[0]['Date'].date()}) will have ALL features as NaN.\")\nprint(f\"  Total features: {len([col for col in features_daily.columns if col != 'Date'])}\")\nprint(f\"  Expected NaN count for Row 1: {len([col for col in features_daily.columns if col != 'Date'])}\")\n\nif len(missing_summary) > 0:\n    print(f\"\\n⚠️ Features with missing values: {len(missing_summary)}\")\n    print(f\"\\nBreakdown by category:\")\n    print(f\"\\n  1. Expected missing values (from 1-day lag):\")\n    print(f\"     - Row 1 has ALL {len([col for col in features_daily.columns if col != 'Date'])} features as NaN (due to lag)\")\n    print(f\"\\n  2. Additional missing values (from data availability):\")\n    \n    for col, count in missing_summary.items():\n        pct = count / len(features_daily) * 100\n        if count == 1:\n            print(f\"     - {col}: {count} ({pct:.1f}%) - from 1-day lag only\")\n        else:\n            print(f\"     - {col}: {count} ({pct:.1f}%)\")\nelse:\n    print(f\"\\n✅ No missing values detected in any feature (except expected lag-induced NaN in Row 1)!\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MISSING VALUE SUMMARY (ALL FEATURES)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**IMPORTANT**: All features have been lagged by 1 day to prevent data leakage.\n",
      "  This means Row 1 (date=2021-03-01) will have ALL features as NaN.\n",
      "  Total features: 58\n",
      "  Expected NaN count for Row 1: 58\n",
      "\n",
      "⚠️ Features with missing values: 58\n",
      "\n",
      "Breakdown by category:\n",
      "\n",
      "  1. Expected missing values (from 1-day lag):\n",
      "     - Row 1 has ALL 58 features as NaN (due to lag)\n",
      "\n",
      "  2. Additional missing values (from data availability):\n",
      "     - P3EA_82+4Q: 293 (25.4%)\n",
      "     - P1EA_82+4Q: 293 (25.4%)\n",
      "     - TC5yr_Atlantic: 5 (0.4%)\n",
      "     - TC5yr_Pacific: 5 (0.4%)\n",
      "     - MGO: 5 (0.4%)\n",
      "     - VLSFO: 5 (0.4%)\n",
      "     - P4_82: 1 (0.1%) - from 1-day lag only\n",
      "     - PDOPEX: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+1MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+1Q: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+3Q: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+3MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+5MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+4MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82CURQ: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+1MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+1Q: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82CURMON: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+2MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+2Q: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+3Q: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+3MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+2MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P1EA_82+2Q: 1 (0.1%) - from 1-day lag only\n",
      "     - BPI: 1 (0.1%) - from 1-day lag only\n",
      "     - C5TC: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82CURQ: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82CURMON: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+5MON: 1 (0.1%) - from 1-day lag only\n",
      "     - P3EA_82+4MON: 1 (0.1%) - from 1-day lag only\n",
      "     - Atlantic_Port_Calls: 1 (0.1%) - from 1-day lag only\n",
      "     - Panamax_Idle_Pct: 1 (0.1%) - from 1-day lag only\n",
      "     - Atlantic_IP_YoY: 1 (0.1%) - from 1-day lag only\n",
      "     - Pacific_IP_YoY: 1 (0.1%) - from 1-day lag only\n",
      "     - Panamax_Deliveries_DWT: 1 (0.1%) - from 1-day lag only\n",
      "     - Capesize_Orderbook_Pct: 1 (0.1%) - from 1-day lag only\n",
      "     - Panamax_Fleet_Growth_YoY: 1 (0.1%) - from 1-day lag only\n",
      "     - Panamax_Orderbook_Pct: 1 (0.1%) - from 1-day lag only\n",
      "     - Coal_Trade_YoY: 1 (0.1%) - from 1-day lag only\n",
      "     - Coal_Trade_YoY_3mma: 1 (0.1%) - from 1-day lag only\n",
      "     - Coal_Trade_Volume_Index: 1 (0.1%) - from 1-day lag only\n",
      "     - Grain_Trade_YoY: 1 (0.1%) - from 1-day lag only\n",
      "     - Grain_Trade_YoY_3mma: 1 (0.1%) - from 1-day lag only\n",
      "     - Grain_Trade_Volume_Index: 1 (0.1%) - from 1-day lag only\n",
      "     - China_Coal_Imports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - China_Grain_Imports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Grain_Trade_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Grain_Trade_BTM: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Grain_Trade_YoY_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Grain_Trade_YoY_BTM: 1 (0.1%) - from 1-day lag only\n",
      "     - India_Coal_Imports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - Japan_Coal_Imports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - Indonesia_Coal_Exports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - Australia_Coal_Exports_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Coal_Trade_BTM: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Coal_Trade_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Coal_Trade_YoY_MT: 1 (0.1%) - from 1-day lag only\n",
      "     - World_Coal_Trade_YoY_BTM: 1 (0.1%) - from 1-day lag only\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:58.995127Z",
     "start_time": "2025-11-01T08:18:58.986134Z"
    }
   },
   "source": [
    "# Check for duplicate dates\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DUPLICATE DATE CHECK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "duplicate_dates = features_daily['Date'].duplicated().sum()\n",
    "if duplicate_dates > 0:\n",
    "    print(f\"\\n⚠️ WARNING: {duplicate_dates} duplicate dates found!\")\n",
    "    print(features_daily[features_daily['Date'].duplicated(keep=False)][['Date']].head(20))\n",
    "else:\n",
    "    print(f\"\\n✅ No duplicate dates detected\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DUPLICATE DATE CHECK\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ No duplicate dates detected\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:59.056907Z",
     "start_time": "2025-11-01T08:18:59.048742Z"
    }
   },
   "source": [
    "# Verify date alignment with master calendar\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MASTER CALENDAR ALIGNMENT CHECK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nMaster calendar dates: {len(master_calendar)}\")\n",
    "print(f\"Features daily dates: {len(features_daily)}\")\n",
    "\n",
    "if len(features_daily) == len(master_calendar):\n",
    "    print(f\"\\n✅ Date counts match!\")\n",
    "    \n",
    "    # Check if dates are identical\n",
    "    dates_match = (features_daily['Date'].values == master_calendar['Date'].values).all()\n",
    "    if dates_match:\n",
    "        print(f\"✅ All dates perfectly aligned with master calendar\")\n",
    "    else:\n",
    "        print(f\"⚠️ WARNING: Date counts match but dates are not identical!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ WARNING: Date counts do not match!\")\n",
    "    print(f\"  Difference: {len(features_daily) - len(master_calendar)} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MASTER CALENDAR ALIGNMENT CHECK\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Master calendar dates: 1153\n",
      "Features daily dates: 1153\n",
      "\n",
      "✅ Date counts match!\n",
      "✅ All dates perfectly aligned with master calendar\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:59.093835Z",
     "start_time": "2025-11-01T08:18:59.064466Z"
    }
   },
   "source": [
    "# Display sample of final dataset\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SAMPLE OF FINAL DATASET (First 5 rows)\")\n",
    "print(\"-\"*80)\n",
    "print(features_daily.head())\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SAMPLE OF FINAL DATASET (Last 5 rows)\")\n",
    "print(\"-\"*80)\n",
    "print(features_daily.tail())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE OF FINAL DATASET (First 5 rows)\n",
      "--------------------------------------------------------------------------------\n",
      "        Date     BPI     C5TC   P4_82  PDOPEX  P1EA_82+1MON  P1EA_82+1Q  \\\n",
      "0 2021-03-01     NaN      NaN     NaN     NaN           NaN         NaN   \n",
      "1 2021-03-02  2086.0  11679.0  5836.0  4744.0       19451.0     18662.0   \n",
      "2 2021-03-03  2100.0  12152.0  5814.0  4744.0       20401.0     19234.0   \n",
      "3 2021-03-04  2161.0  13910.0  5871.0  4744.0       21117.0     19651.0   \n",
      "4 2021-03-05  2212.0  13874.0  5918.0  4744.0       21367.0     19890.0   \n",
      "\n",
      "   P1EA_82+2MON  P1EA_82+2Q  P1EA_82+3MON  ...  World_Grain_Trade_YoY_MT  \\\n",
      "0           NaN         NaN           NaN  ...                       NaN   \n",
      "1       18551.0     16651.0       17984.0  ...                      2.26   \n",
      "2       18884.0     17017.0       18417.0  ...                      2.26   \n",
      "3       19417.0     17234.0       18417.0  ...                      2.26   \n",
      "4       19651.0     17267.0       18651.0  ...                      2.26   \n",
      "\n",
      "   World_Grain_Trade_YoY_BTM  India_Coal_Imports_MT  Japan_Coal_Imports_MT  \\\n",
      "0                        NaN                    NaN                    NaN   \n",
      "1                       1.46                 203.98                  176.4   \n",
      "2                       1.46                 203.98                  176.4   \n",
      "3                       1.46                 203.98                  176.4   \n",
      "4                       1.46                 203.98                  176.4   \n",
      "\n",
      "   Indonesia_Coal_Exports_MT  Australia_Coal_Exports_MT  World_Coal_Trade_BTM  \\\n",
      "0                        NaN                        NaN                   NaN   \n",
      "1                     431.77                     365.83               4842.42   \n",
      "2                     431.77                     365.83               4842.42   \n",
      "3                     431.77                     365.83               4842.42   \n",
      "4                     431.77                     365.83               4842.42   \n",
      "\n",
      "   World_Coal_Trade_MT  World_Coal_Trade_YoY_MT  World_Coal_Trade_YoY_BTM  \n",
      "0                  NaN                      NaN                       NaN  \n",
      "1              1230.92                     4.12                      6.42  \n",
      "2              1230.92                     4.12                      6.42  \n",
      "3              1230.92                     4.12                      6.42  \n",
      "4              1230.92                     4.12                      6.42  \n",
      "\n",
      "[5 rows x 59 columns]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE OF FINAL DATASET (Last 5 rows)\n",
      "--------------------------------------------------------------------------------\n",
      "           Date     BPI     C5TC   P4_82  PDOPEX  P1EA_82+1MON  P1EA_82+1Q  \\\n",
      "1148 2025-10-06  1662.0  22595.0  8664.0  5301.0       15272.0     11266.0   \n",
      "1149 2025-10-07  1654.0  23453.0  8700.0  5301.0       15256.0     11274.0   \n",
      "1150 2025-10-08  1665.0  23927.0  8823.0  5301.0       15334.0     11289.0   \n",
      "1151 2025-10-09  1695.0  24252.0  8904.0  5301.0       15197.0     11224.0   \n",
      "1152 2025-10-10  1730.0  23101.0  8954.0  5301.0       15253.0     11241.0   \n",
      "\n",
      "      P1EA_82+2MON  P1EA_82+2Q  P1EA_82+3MON  ...  World_Grain_Trade_YoY_MT  \\\n",
      "1148       14359.0     12972.0       11783.0  ...                      1.65   \n",
      "1149       14434.0     13021.0       11796.0  ...                      1.65   \n",
      "1150       14540.0     13021.0       11803.0  ...                      1.65   \n",
      "1151       14397.0     12815.0       11708.0  ...                      1.65   \n",
      "1152       14390.0     12815.0       11733.0  ...                      1.65   \n",
      "\n",
      "      World_Grain_Trade_YoY_BTM  India_Coal_Imports_MT  Japan_Coal_Imports_MT  \\\n",
      "1148                       1.98                 233.65                 160.34   \n",
      "1149                       1.98                 233.65                 160.34   \n",
      "1150                       1.98                 233.65                 160.34   \n",
      "1151                       1.98                 233.65                 160.34   \n",
      "1152                       1.98                 233.65                 160.34   \n",
      "\n",
      "      Indonesia_Coal_Exports_MT  Australia_Coal_Exports_MT  \\\n",
      "1148                     528.68                     358.02   \n",
      "1149                     528.68                     358.02   \n",
      "1150                     528.68                     358.02   \n",
      "1151                     528.68                     358.02   \n",
      "1152                     528.68                     358.02   \n",
      "\n",
      "      World_Coal_Trade_BTM  World_Coal_Trade_MT  World_Coal_Trade_YoY_MT  \\\n",
      "1148               5497.25              1311.03                    -4.87   \n",
      "1149               5497.25              1311.03                    -4.87   \n",
      "1150               5497.25              1311.03                    -4.87   \n",
      "1151               5497.25              1311.03                    -4.87   \n",
      "1152               5497.25              1311.03                    -4.87   \n",
      "\n",
      "      World_Coal_Trade_YoY_BTM  \n",
      "1148                     -6.22  \n",
      "1149                     -6.22  \n",
      "1150                     -6.22  \n",
      "1151                     -6.22  \n",
      "1152                     -6.22  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:18:59.229636Z",
     "start_time": "2025-11-01T08:18:59.149520Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 11: SAVE PROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save features_raw_daily\n",
    "features_raw_daily_file = INTERMEDIATE_DIR / 'features_raw_daily.csv'\n",
    "features_daily.to_csv(features_raw_daily_file, index=False)\n",
    "print(f\"\\n✅ Features (raw daily) saved to: {features_raw_daily_file}\")\n",
    "print(f\"   Shape: {features_daily.shape}\")\n",
    "print(f\"   Columns: {len(features_daily.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DATA GATHERING COMPLETE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nSummary of saved files:\")\n",
    "print(f\"  1. Master calendar: {INTERMEDIATE_DIR / 'master_calendar.csv'}\")\n",
    "print(f\"  2. Labels: {INTERMEDIATE_DIR / 'labels.csv'}\")\n",
    "print(f\"  3. BFA wide: {INTERMEDIATE_DIR / 'bfa_wide.csv'}\")\n",
    "print(f\"  4. Features (raw daily): {INTERMEDIATE_DIR / 'features_raw_daily.csv'}\")\n",
    "\n",
    "print(\"\\n✅ Ready for next step: 02_feature_engineering.ipynb\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: SAVE PROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "✅ Features (raw daily) saved to: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\features_raw_daily.csv\n",
      "   Shape: (1153, 59)\n",
      "   Columns: 59\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATA GATHERING COMPLETE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Summary of saved files:\n",
      "  1. Master calendar: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\master_calendar.csv\n",
      "  2. Labels: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\labels.csv\n",
      "  3. BFA wide: C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\bfa_wide.csv\n",
      "  4. Features (raw daily): C:\\Users\\moame\\Documents\\GitHub\\MScFECapstoneProject\\round_2\\data\\processed\\intermediate\\features_raw_daily.csv\n",
      "\n",
      "✅ Ready for next step: 02_feature_engineering.ipynb\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
