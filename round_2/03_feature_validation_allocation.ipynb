{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Empirical Feature Validation & Allocation\n\n**Created:** October 31, 2025  \n**Purpose:** Data-driven feature selection and allocation to CORE vs ML sets  \n**Approach:** Random Forest + VIF → Allocate based on model requirements\n\n---\n\n## Why This Approach?\n\n**Previous Failure:** Theory-based pre-selection led to P1A validation failure\n- Example: `Grain_Trade_YoY` had -67.84 importance (actively harmful!)\n- Root cause: Pre-selected features based on domain knowledge without empirical validation\n\n**New Approach:** Let data guide feature selection\n1. [OK] **Notebook 02:** Created ~184 comprehensive features (ALL transformations)\n2. [OK] **Notebook 03 (THIS):** Empirical validation → Allocate to CORE/ML\n3.  **Notebook 04:** Data preparation with empirically-validated features\n\n---\n\n## Allocation Strategy\n\n### CORE Features (ARIMAX/SARIMAX Input)\n**Requirements:**\n- [OK] Stationary transformation (diff, pct, yoy, mom, ma30_dev)\n- [OK] Positive permutation importance (> 0)\n- [OK] VIF < 10 (low multicollinearity)\n- [OK] Economically interpretable\n-  **Target:** 8-12 features per route\n\n### ML Features (XGBoost Input)\n**Requirements:**\n- [OK] Positive or neutral importance (> -5)\n- [OK] Can include levels, non-stationary, correlated features\n- [OK] Complementary to CORE (prefer diversity)\n-  **Target:** 15-20 features per route\n\n---\n\n## Quality Gate Criteria\n\n| Metric | Threshold | P1A Target | P3A Target |\n|--------|-----------|------------|------------|\n| **Max Importance** | > 20 | [OK] | [OK] |\n| **Mean Importance** | > 5 | [OK] | [OK] |\n| **% Positive Features** | > 75% | [OK] | [OK] |\n| **Max VIF (CORE)** | < 10 | [OK] | [OK] |\n\n**Outcome:**\n- [OK] **PASS:** Proceed to Phase 3 (Data Preparation)\n- [WARN] **PARTIAL:** Proceed with caution, monitor failed route\n- [FAIL] **FAIL:** Return to Notebook 02 (revise features)\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:44.920537Z",
     "start_time": "2025-11-01T08:19:44.175814Z"
    }
   },
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Feature selection & validation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\n\nprint(\"[OK] Libraries imported\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Libraries imported\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:44.981059Z",
     "start_time": "2025-11-01T08:19:44.944079Z"
    }
   },
   "source": [
    "# Load comprehensive features from Notebook 02\nfeatures_all = pd.read_csv('data/processed/features/features_comprehensive.csv',\n                            index_col='Date', parse_dates=True)\n\n# Load labels\nlabels = pd.read_csv('data/processed/intermediate/labels.csv',\n                     index_col='Date', parse_dates=True)\n\nprint(\"[OK] Data loaded\")\nprint(f\"\\nComprehensive features: {features_all.shape}\")\nprint(f\"Date range: {features_all.index.min()} to {features_all.index.max()}\")\nprint(f\"\\nLabels: {labels.shape}\")\nprint(f\"Routes: {labels.columns.tolist()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Data loaded\n",
      "\n",
      "Comprehensive features: (1153, 128)\n",
      "Date range: 2021-03-01 00:00:00 to 2025-10-10 00:00:00\n",
      "\n",
      "Labels: (1153, 2)\n",
      "Routes: ['P1A_82', 'P3A_82']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:44.999572Z",
     "start_time": "2025-11-01T08:19:44.987577Z"
    }
   },
   "source": "# Check for features with excessive missing values (>90%)\nmissing_pct = (features_all.isnull().sum() / len(features_all) * 100).sort_values(ascending=False)\nexcessive_missing = missing_pct[missing_pct > 90]\n\nprint(\" DATA QUALITY VERIFICATION\")\nprint(\"=\" * 80)\n\nif len(excessive_missing) > 0:\n    print(f\"\\n[WARN]  Found {len(excessive_missing)} features with >90% missing values:\")\n    print(f\"\\n(These will be automatically excluded from validation)\\n\")\n    for feat, pct in excessive_missing.items():\n        print(f\"  - {feat:50s}: {pct:5.2f}%\")\n    \n    # Drop these features\n    features_all = features_all.drop(columns=excessive_missing.index)\n    print(f\"\\n[OK] Dropped {len(excessive_missing)} features\")\n    print(f\"   Remaining features: {features_all.shape[1]}\")\nelse:\n    print(\"\\n[OK] No features with >90% missing values\")\n\n# Verify no infinite values (should be cleaned in Notebook 02)\ninf_counts = np.isinf(features_all).sum()\nfeatures_with_inf = inf_counts[inf_counts > 0]\n\nif len(features_with_inf) > 0:\n    print(f\"\\n[FAIL] ERROR: Found {len(features_with_inf)} features with infinite values!\")\n    print(\"   This should NOT happen - data cleaning must occur in Notebook 02.\")\n    for feat, count in features_with_inf.items():\n        print(f\"  - {feat}: {count} infinite values\")\n    raise ValueError(\"Infinite values detected - data not properly cleaned in Notebook 02\")\nelse:\n    print(\"\\n[OK] No infinite values (data properly cleaned)\")\n\nprint(f\"\\nFinal feature count for validation: {features_all.shape[1]}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DATA QUALITY VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "[OK] No features with >90% missing values\n",
      "\n",
      "[OK] No infinite values (data properly cleaned)\n",
      "\n",
      "Final feature count for validation: 128\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Step 1 - Comprehensive RF Validation (ALL Features)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:19:45.034493Z",
     "start_time": "2025-11-01T08:19:45.021803Z"
    }
   },
   "source": [
    "def comprehensive_rf_validation(X, y, route_name, n_trees=200, n_repeats=10, random_state=42):\n    \"\"\"\n    Validate ALL features using Random Forest + Permutation Importance.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame\n        Feature matrix (all features)\n    y : pd.Series\n        Target variable (route rates)\n    route_name : str\n        Route identifier (e.g., 'P1A', 'P3A')\n    n_trees : int\n        Number of trees in Random Forest\n    n_repeats : int\n        Number of permutation repeats\n    random_state : int\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    results : pd.DataFrame\n        Feature rankings (Feature, Importance, Std)\n    rf_model : RandomForestRegressor\n        Trained model\n    X_train, X_test, y_train, y_test : splits\n        Training/test data (for later analysis)\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"COMPREHENSIVE RF VALIDATION: {route_name}\")\n    print(f\"{'=' * 80}\")\n\n    # Drop NaN rows (from transformations)\n    valid_mask = ~(X.isnull().any(axis=1) | y.isnull())\n    X_clean = X[valid_mask]\n    y_clean = y[valid_mask]\n\n    print(f\"\\nOriginal data: {len(X)} rows\")\n    print(f\"After dropping NaN: {len(X_clean)} rows ({len(X_clean)/len(X)*100:.1f}%)\")\n    print(f\"Features: {X_clean.shape[1]}\")\n\n    # Temporal 70/30 split (preserve time order)\n    split_idx = int(0.7 * len(X_clean))\n    X_train = X_clean.iloc[:split_idx]\n    X_test = X_clean.iloc[split_idx:]\n    y_train = y_clean.iloc[:split_idx]\n    y_test = y_clean.iloc[split_idx:]\n\n    print(f\"\\nTemporal Split:\")\n    print(f\"  Train: {len(X_train)} rows ({X_train.index.min().date()} to {X_train.index.max().date()})\")\n    print(f\"  Test:  {len(X_test)} rows ({X_test.index.min().date()} to {X_test.index.max().date()})\")\n\n    # Train Random Forest\n    print(f\"\\nTraining Random Forest...\")\n    print(f\"  Trees: {n_trees}\")\n    print(f\"  Max depth: 10\")\n    print(f\"  Random state: {random_state}\")\n    \n    rf = RandomForestRegressor(\n        n_estimators=n_trees,\n        max_depth=10,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        random_state=random_state,\n        n_jobs=-1,\n        verbose=0\n    )\n    rf.fit(X_train, y_train)\n    print(\"[OK] Training complete\")\n\n    # Evaluate on test set\n    y_pred_train = rf.predict(X_train)\n    y_pred_test = rf.predict(X_test)\n    \n    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n    r2_train = r2_score(y_train, y_pred_train)\n    r2_test = r2_score(y_test, y_pred_test)\n\n    print(f\"\\nModel Performance:\")\n    print(f\"  Train RMSE: ${rmse_train:,.2f}\")\n    print(f\"  Test RMSE:  ${rmse_test:,.2f}\")\n    print(f\"  Train R²:   {r2_train:.4f}\")\n    print(f\"  Test R²:    {r2_test:.4f}\")\n\n    # Permutation Importance (on test set)\n    print(f\"\\nComputing Permutation Importance...\")\n    print(f\"  Repeats: {n_repeats}\")\n    print(f\"  Scoring: neg_root_mean_squared_error\")\n    print(f\"  (This may take 2-5 minutes...)\")\n    \n    perm_imp = permutation_importance(\n        rf, X_test, y_test,\n        n_repeats=n_repeats,\n        scoring='neg_root_mean_squared_error',\n        random_state=random_state,\n        n_jobs=-1\n    )\n    print(\"[OK] Permutation importance complete\")\n\n    # Create results dataframe\n    results = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': perm_imp.importances_mean,\n        'Std': perm_imp.importances_std\n    }).sort_values('Importance', ascending=False).reset_index(drop=True)\n\n    # Summary statistics\n    print(f\"\\n{'=' * 80}\")\n    print(\"FEATURE IMPORTANCE SUMMARY\")\n    print(f\"{'=' * 80}\")\n    print(f\"  Total features:         {len(results)}\")\n    print(f\"  Max Importance:         {results['Importance'].max():10.2f}\")\n    print(f\"  Mean Importance:        {results['Importance'].mean():10.2f}\")\n    print(f\"  Median Importance:      {results['Importance'].median():10.2f}\")\n    print(f\"  % Positive (>0):        {(results['Importance'] > 0).mean() * 100:10.1f}%\")\n    print(f\"  % Strong (>10):         {(results['Importance'] > 10).mean() * 100:10.1f}%\")\n    print(f\"  Harmful (<-10):         {(results['Importance'] < -10).sum():10d} features\")\n    print(f\"  Catastrophic (<-50):    {(results['Importance'] < -50).sum():10d} features\")\n\n    # Show top 10 and bottom 10 features\n    print(f\"\\n TOP 10 FEATURES:\")\n    print(results.head(10).to_string(index=False))\n\n    print(f\"\\n BOTTOM 10 FEATURES:\")\n    print(results.tail(10).to_string(index=False))\n\n    return results, rf, X_train, X_test, y_train, y_test\n\nprint(\"[OK] RF validation function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RF validation function defined\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:15.503205Z",
     "start_time": "2025-11-01T08:19:45.055501Z"
    }
   },
   "source": [
    "# Run RF validation for P1A\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING P1A VALIDATION\")\nprint(\"=\"*80)\n\nfeature_ranking_p1a, rf_p1a, X_train_p1a, X_test_p1a, y_train_p1a, y_test_p1a = comprehensive_rf_validation(\n    X=features_all,\n    y=labels['P1A_82'],\n    route_name='P1A (Atlantic - Grain Route)',\n    n_trees=200,\n    n_repeats=10,\n    random_state=42\n)\n\n# Save ranking\nfeature_ranking_p1a.to_csv('data/processed/features/p1a_feature_ranking_all.csv', index=False)\nprint(\"\\n[OK] P1A feature ranking saved: data/processed/features/p1a_feature_ranking_all.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING P1A VALIDATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RF VALIDATION: P1A (Atlantic - Grain Route)\n",
      "================================================================================\n",
      "\n",
      "Original data: 1153 rows\n",
      "After dropping NaN: 1073 rows (93.1%)\n",
      "Features: 128\n",
      "\n",
      "Temporal Split:\n",
      "  Train: 751 rows (2021-04-14 to 2024-04-18)\n",
      "  Test:  322 rows (2024-04-19 to 2025-10-10)\n",
      "\n",
      "Training Random Forest...\n",
      "  Trees: 200\n",
      "  Max depth: 10\n",
      "  Random state: 42\n",
      "[OK] Training complete\n",
      "\n",
      "Model Performance:\n",
      "  Train RMSE: $1,025.83\n",
      "  Test RMSE:  $2,329.72\n",
      "  Train R²:   0.9860\n",
      "  Test R²:    0.6159\n",
      "\n",
      "Computing Permutation Importance...\n",
      "  Repeats: 10\n",
      "  Scoring: neg_root_mean_squared_error\n",
      "  (This may take 2-5 minutes...)\n",
      "[OK] Permutation importance complete\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE SUMMARY\n",
      "================================================================================\n",
      "  Total features:         128\n",
      "  Max Importance:            3088.02\n",
      "  Mean Importance:             23.97\n",
      "  Median Importance:           -0.00\n",
      "  % Positive (>0):              27.3%\n",
      "  % Strong (>10):                3.9%\n",
      "  Harmful (<-10):                  2 features\n",
      "  Catastrophic (<-50):             1 features\n",
      "\n",
      " TOP 10 FEATURES:\n",
      "                      Feature  Importance        Std\n",
      "                    BPI_level 3088.022530 120.169790\n",
      "         TC5yr_Atlantic_level   71.317859   9.331566\n",
      "            P1EA_CURMON_level   14.528496   1.809317\n",
      "              Atlantic_IP_yoy   13.300019   1.916568\n",
      "Coal_Trade_Volume_Index_vol30   13.154564  14.463436\n",
      "              P1EA_1MON_level    8.869905   1.505975\n",
      "  Panamax_Orderbook_Pct_vol30    7.705753   1.172678\n",
      "                      MGO_yoy    6.988966   2.458136\n",
      "                 PDOPEX_vol30    6.706575   3.641188\n",
      "                P3EA_1Q_level    5.354152  10.586123\n",
      "\n",
      " BOTTOM 10 FEATURES:\n",
      "                       Feature  Importance       Std\n",
      "          Panamax_Idle_Pct_yoy   -2.794368  0.874234\n",
      "  Panamax_Fleet_Growth_YoY_yoy   -3.798859  1.632489\n",
      "                 P1EA_1Q_level   -3.950094  0.717596\n",
      " Coal_Trade_Volume_Index_level   -4.054420  5.016024\n",
      "Grain_Trade_Volume_Index_level   -4.055506  0.599627\n",
      "                   P4_82_level   -5.197444  0.904303\n",
      "                Pacific_IP_yoy   -9.260177  0.685285\n",
      "             P3EA_CURMON_level   -9.416069  8.845869\n",
      "         Grain_Trade_YoY_level  -32.490297  4.119722\n",
      "               P3EA_1MON_level -111.473588 31.692629\n",
      "\n",
      "[OK] P1A feature ranking saved: data/processed/features/p1a_feature_ranking_all.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:38.991450Z",
     "start_time": "2025-11-01T08:20:15.736119Z"
    }
   },
   "source": [
    "# Run RF validation for P3A\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING P3A VALIDATION\")\nprint(\"=\"*80)\n\nfeature_ranking_p3a, rf_p3a, X_train_p3a, X_test_p3a, y_train_p3a, y_test_p3a = comprehensive_rf_validation(\n    X=features_all,\n    y=labels['P3A_82'],\n    route_name='P3A (Pacific - Coal + Grain Route)',\n    n_trees=200,\n    n_repeats=10,\n    random_state=42\n)\n\n# Save ranking\nfeature_ranking_p3a.to_csv('data/processed/features/p3a_feature_ranking_all.csv', index=False)\nprint(\"\\n[OK] P3A feature ranking saved: data/processed/features/p3a_feature_ranking_all.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING P3A VALIDATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE RF VALIDATION: P3A (Pacific - Coal + Grain Route)\n",
      "================================================================================\n",
      "\n",
      "Original data: 1153 rows\n",
      "After dropping NaN: 1073 rows (93.1%)\n",
      "Features: 128\n",
      "\n",
      "Temporal Split:\n",
      "  Train: 751 rows (2021-04-14 to 2024-04-18)\n",
      "  Test:  322 rows (2024-04-19 to 2025-10-10)\n",
      "\n",
      "Training Random Forest...\n",
      "  Trees: 200\n",
      "  Max depth: 10\n",
      "  Random state: 42\n",
      "[OK] Training complete\n",
      "\n",
      "Model Performance:\n",
      "  Train RMSE: $651.44\n",
      "  Test RMSE:  $1,554.16\n",
      "  Train R²:   0.9937\n",
      "  Test R²:    0.6735\n",
      "\n",
      "Computing Permutation Importance...\n",
      "  Repeats: 10\n",
      "  Scoring: neg_root_mean_squared_error\n",
      "  (This may take 2-5 minutes...)\n",
      "[OK] Permutation importance complete\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE SUMMARY\n",
      "================================================================================\n",
      "  Total features:         128\n",
      "  Max Importance:            1783.06\n",
      "  Mean Importance:             15.01\n",
      "  Median Importance:            0.00\n",
      "  % Positive (>0):              74.2%\n",
      "  % Strong (>10):                4.7%\n",
      "  Harmful (<-10):                  2 features\n",
      "  Catastrophic (<-50):             0 features\n",
      "\n",
      " TOP 10 FEATURES:\n",
      "                       Feature  Importance       Std\n",
      "                     BPI_level 1783.060927 70.211561\n",
      "                   P4_82_level  113.018079 11.604054\n",
      "   Panamax_Orderbook_Pct_vol30   17.580039  7.954505\n",
      "                       MGO_yoy   17.349434  3.160071\n",
      "Panamax_Fleet_Growth_YoY_vol30   13.796821  3.106985\n",
      "                     P4_82_yoy   10.107181  6.149886\n",
      "               Atlantic_IP_yoy    6.490443  2.080036\n",
      "        Panamax_Idle_Pct_vol30    4.651007  2.151705\n",
      "  Capesize_Orderbook_Pct_vol30    4.115066  3.075942\n",
      "                    P4_82_diff    2.857903  0.979944\n",
      "\n",
      " BOTTOM 10 FEATURES:\n",
      "                       Feature  Importance      Std\n",
      "  Panamax_Deliveries_DWT_vol30   -2.241427 4.710212\n",
      "                 P1EA_1Q_level   -2.285227 0.764516\n",
      "                     BPI_vol30   -2.426494 0.319244\n",
      "                    C5TC_level   -2.533229 3.703693\n",
      "                   VLSFO_vol30   -3.374094 0.751452\n",
      "Grain_Trade_Volume_Index_level   -3.745083 0.904818\n",
      "          Coal_Trade_YoY_level   -4.637141 8.532097\n",
      "               P3EA_1MON_level   -5.354541 3.531599\n",
      "  Panamax_Deliveries_DWT_level  -16.294721 2.355923\n",
      "          TC5yr_Atlantic_level  -16.499713 9.954702\n",
      "\n",
      "[OK] P3A feature ranking saved: data/processed/features/p3a_feature_ranking_all.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Step 2 - Initial Feature Screening"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.284576Z",
     "start_time": "2025-11-01T08:20:39.275130Z"
    }
   },
   "source": [
    "def screen_features(ranking_df, threshold=-10, route_name=''):\n    \"\"\"\n    Screen features based on importance threshold.\n    Drop harmful features (importance < threshold).\n    \n    Parameters:\n    -----------\n    ranking_df : pd.DataFrame\n        Feature ranking from RF validation\n    threshold : float\n        Minimum importance to keep (default: -10)\n        Features below this are considered actively harmful\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    keep_features : list\n        Features to keep (importance > threshold)\n    drop_features : list\n        Features to drop (importance <= threshold)\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"FEATURE SCREENING: {route_name}\")\n    print(f\"{'=' * 80}\")\n    print(f\"\\nThreshold: Importance > {threshold}\")\n\n    keep_features = ranking_df[ranking_df['Importance'] > threshold]['Feature'].tolist()\n    drop_features = ranking_df[ranking_df['Importance'] <= threshold]['Feature'].tolist()\n\n    print(f\"\\n SCREENING RESULTS:\")\n    print(f\"  [OK] Keep: {len(keep_features)} features ({len(keep_features)/len(ranking_df)*100:.1f}%)\")\n    print(f\"  [FAIL] Drop: {len(drop_features)} features ({len(drop_features)/len(ranking_df)*100:.1f}%)\")\n\n    if drop_features:\n        print(f\"\\n[FAIL] DROPPED FEATURES (harmful):\")\n        print(f\"\\n{'Rank':<6} {'Feature':<55} {'Importance':<12}\")\n        print(\"-\" * 75)\n        for i, feat in enumerate(drop_features[:20], 1):  # Show top 20 worst\n            imp = ranking_df[ranking_df['Feature'] == feat]['Importance'].values[0]\n            print(f\"{i:<6} {feat:<55} {imp:>10.2f}\")\n        if len(drop_features) > 20:\n            print(f\"\\n   ... and {len(drop_features) - 20} more\")\n    else:\n        print(f\"\\n[OK] No harmful features detected (all importance > {threshold})\")\n\n    return keep_features, drop_features\n\n\nprint(\"[OK] Screening function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Screening function defined\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.339586Z",
     "start_time": "2025-11-01T08:20:39.332583Z"
    }
   },
   "source": [
    "# Screen P1A features\n",
    "p1a_keep, p1a_drop = screen_features(\n",
    "    feature_ranking_p1a,\n",
    "    threshold=-10,\n",
    "    route_name='P1A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE SCREENING: P1A\n",
      "================================================================================\n",
      "\n",
      "Threshold: Importance > -10\n",
      "\n",
      " SCREENING RESULTS:\n",
      "  [OK] Keep: 126 features (98.4%)\n",
      "  [FAIL] Drop: 2 features (1.6%)\n",
      "\n",
      "[FAIL] DROPPED FEATURES (harmful):\n",
      "\n",
      "Rank   Feature                                                 Importance  \n",
      "---------------------------------------------------------------------------\n",
      "1      Grain_Trade_YoY_level                                       -32.49\n",
      "2      P3EA_1MON_level                                            -111.47\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.392492Z",
     "start_time": "2025-11-01T08:20:39.385118Z"
    }
   },
   "source": [
    "# Screen P3A features\n",
    "p3a_keep, p3a_drop = screen_features(\n",
    "    feature_ranking_p3a,\n",
    "    threshold=-10,\n",
    "    route_name='P3A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE SCREENING: P3A\n",
      "================================================================================\n",
      "\n",
      "Threshold: Importance > -10\n",
      "\n",
      " SCREENING RESULTS:\n",
      "  [OK] Keep: 126 features (98.4%)\n",
      "  [FAIL] Drop: 2 features (1.6%)\n",
      "\n",
      "[FAIL] DROPPED FEATURES (harmful):\n",
      "\n",
      "Rank   Feature                                                 Importance  \n",
      "---------------------------------------------------------------------------\n",
      "1      Panamax_Deliveries_DWT_level                                -16.29\n",
      "2      TC5yr_Atlantic_level                                        -16.50\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Step 3 - Feature Allocation Logic"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.431920Z",
     "start_time": "2025-11-01T08:20:39.419500Z"
    }
   },
   "source": [
    "def allocate_features(ranking_df, features_all_df, keep_features,\n                      core_target=10, ml_target=20, route_name=''):\n    \"\"\"\n    Allocate features into CORE (ARIMAX) vs ML (XGBoost) sets.\n    \n    CORE Requirements:\n    - Stationary transformation (diff, pct, yoy, mom, ma30_dev, vol30)\n    - Positive importance (> 0)\n    - Will check VIF < 10 in next step\n    - Target: 8-12 features\n    \n    ML Requirements:\n    - Positive or neutral importance (> -5)\n    - Can include levels, composite indices, correlated features\n    - Complementary to CORE (prefer diversity)\n    - Target: 15-20 features\n    \n    Parameters:\n    -----------\n    ranking_df : pd.DataFrame\n        Feature ranking\n    features_all_df : pd.DataFrame\n        All features data\n    keep_features : list\n        Features that passed screening\n    core_target : int\n        Target number of CORE features\n    ml_target : int\n        Target number of ML features\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    core_candidates : pd.DataFrame\n        CORE feature candidates (before VIF filtering)\n    ml_candidates : pd.DataFrame\n        ML feature candidates (before final selection)\n    ranking_filtered : pd.DataFrame\n        Full ranking with stationarity flag\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"FEATURE ALLOCATION: {route_name}\")\n    print(f\"{'=' * 80}\")\n\n    # Filter to keep_features only\n    ranking_filtered = ranking_df[ranking_df['Feature'].isin(keep_features)].copy()\n\n    print(f\"\\nFiltered features: {len(ranking_filtered)} (after screening)\")\n\n    # Identify stationary features (for CORE candidates)\n    stationary_suffixes = ['_diff', '_pct', '_yoy', '_mom', '_ma30_dev', '_vol30']\n    ranking_filtered['Stationary'] = ranking_filtered['Feature'].apply(\n        lambda x: any(x.endswith(suffix) for suffix in stationary_suffixes)\n    )\n\n    stationary_count = ranking_filtered['Stationary'].sum()\n    level_count = (~ranking_filtered['Stationary']).sum()\n\n    print(f\"\\nFeature types:\")\n    print(f\"  Stationary (transformations): {stationary_count}\")\n    print(f\"  Levels/Other:                 {level_count}\")\n\n    # CORE candidates: stationary + positive importance\n    print(f\"\\n{'=' * 80}\")\n    print(\"CORE CANDIDATES (ARIMAX Input)\")\n    print(f\"{'=' * 80}\")\n    print(f\"Requirements: Stationary + Positive Importance + VIF<10 (checked next)\")\n\n    core_candidates = ranking_filtered[\n        (ranking_filtered['Stationary'] == True) &\n        (ranking_filtered['Importance'] > 0)\n    ].head(core_target * 2)  # Select 2x target for VIF filtering\n\n    print(f\"\\nSelected: {len(core_candidates)} candidates (will filter to {core_target} via VIF)\")\n    print(f\"\\nTop {min(15, len(core_candidates))} CORE candidates:\")\n    print(core_candidates.head(15).to_string(index=False))\n\n    # ML candidates: positive/neutral importance\n    print(f\"\\n{'=' * 80}\")\n    print(\"ML CANDIDATES (XGBoost Input)\")\n    print(f\"{'=' * 80}\")\n    print(f\"Requirements: Importance > -5 (allows neutral features)\")\n\n    ml_candidates = ranking_filtered[\n        (ranking_filtered['Importance'] > -5)\n    ].head(ml_target * 2)  # Select 2x target for diversity\n\n    print(f\"\\nSelected: {len(ml_candidates)} candidates (will select best {ml_target})\")\n    print(f\"\\nTop {min(20, len(ml_candidates))} ML candidates:\")\n    print(ml_candidates.head(20).to_string(index=False))\n\n    return core_candidates, ml_candidates, ranking_filtered\n\n\nprint(\"[OK] Allocation function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Allocation function defined\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.458952Z",
     "start_time": "2025-11-01T08:20:39.443929Z"
    }
   },
   "source": [
    "# Allocate P1A features\n",
    "p1a_core_candidates, p1a_ml_candidates, p1a_ranking_filtered = allocate_features(\n",
    "    ranking_df=feature_ranking_p1a,\n",
    "    features_all_df=features_all,\n",
    "    keep_features=p1a_keep,\n",
    "    core_target=10,\n",
    "    ml_target=20,\n",
    "    route_name='P1A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ALLOCATION: P1A\n",
      "================================================================================\n",
      "\n",
      "Filtered features: 126 (after screening)\n",
      "\n",
      "Feature types:\n",
      "  Stationary (transformations): 96\n",
      "  Levels/Other:                 30\n",
      "\n",
      "================================================================================\n",
      "CORE CANDIDATES (ARIMAX Input)\n",
      "================================================================================\n",
      "Requirements: Stationary + Positive Importance + VIF<10 (checked next)\n",
      "\n",
      "Selected: 20 candidates (will filter to 10 via VIF)\n",
      "\n",
      "Top 15 CORE candidates:\n",
      "                       Feature  Importance       Std  Stationary\n",
      "               Atlantic_IP_yoy   13.300019  1.916568        True\n",
      " Coal_Trade_Volume_Index_vol30   13.154564 14.463436        True\n",
      "   Panamax_Orderbook_Pct_vol30    7.705753  1.172678        True\n",
      "                       MGO_yoy    6.988966  2.458136        True\n",
      "                  PDOPEX_vol30    6.706575  3.641188        True\n",
      "                     BPI_vol30    4.349367  7.067327        True\n",
      "   Coal_Trade_Volume_Index_yoy    2.588564  0.537171        True\n",
      "                       BPI_yoy    2.560062  1.213179        True\n",
      "  Capesize_Orderbook_Pct_vol30    2.413733  0.938572        True\n",
      "     Atlantic_Port_Calls_vol30    2.096386  0.903714        True\n",
      "                    C5TC_vol30    1.824027  2.041264        True\n",
      "                      C5TC_yoy    1.547696  1.746899        True\n",
      "Grain_Trade_Volume_Index_vol30    1.547653  2.478766        True\n",
      "                     P4_82_pct    0.655014  0.331894        True\n",
      "                     P4_82_yoy    0.629382  1.175919        True\n",
      "\n",
      "================================================================================\n",
      "ML CANDIDATES (XGBoost Input)\n",
      "================================================================================\n",
      "Requirements: Importance > -5 (allows neutral features)\n",
      "\n",
      "Selected: 40 candidates (will select best 20)\n",
      "\n",
      "Top 20 ML candidates:\n",
      "                      Feature  Importance        Std  Stationary\n",
      "                    BPI_level 3088.022530 120.169790       False\n",
      "         TC5yr_Atlantic_level   71.317859   9.331566       False\n",
      "            P1EA_CURMON_level   14.528496   1.809317       False\n",
      "              Atlantic_IP_yoy   13.300019   1.916568        True\n",
      "Coal_Trade_Volume_Index_vol30   13.154564  14.463436        True\n",
      "              P1EA_1MON_level    8.869905   1.505975       False\n",
      "  Panamax_Orderbook_Pct_vol30    7.705753   1.172678        True\n",
      "                      MGO_yoy    6.988966   2.458136        True\n",
      "                 PDOPEX_vol30    6.706575   3.641188        True\n",
      "                P3EA_1Q_level    5.354152  10.586123       False\n",
      " Panamax_Deliveries_DWT_level    5.261868   0.664328       False\n",
      "                    BPI_vol30    4.349367   7.067327        True\n",
      "       Panamax_Idle_Pct_level    3.899748   2.958658       False\n",
      "  Coal_Trade_Volume_Index_yoy    2.588564   0.537171        True\n",
      "                      BPI_yoy    2.560062   1.213179        True\n",
      " Capesize_Orderbook_Pct_vol30    2.413733   0.938572        True\n",
      "    Atlantic_Port_Calls_level    2.153314   0.437811       False\n",
      "    Atlantic_Port_Calls_vol30    2.096386   0.903714        True\n",
      "                   C5TC_vol30    1.824027   2.041264        True\n",
      "                     C5TC_yoy    1.547696   1.746899        True\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.491855Z",
     "start_time": "2025-11-01T08:20:39.478815Z"
    }
   },
   "source": [
    "# Allocate P3A features\n",
    "p3a_core_candidates, p3a_ml_candidates, p3a_ranking_filtered = allocate_features(\n",
    "    ranking_df=feature_ranking_p3a,\n",
    "    features_all_df=features_all,\n",
    "    keep_features=p3a_keep,\n",
    "    core_target=10,\n",
    "    ml_target=20,\n",
    "    route_name='P3A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ALLOCATION: P3A\n",
      "================================================================================\n",
      "\n",
      "Filtered features: 126 (after screening)\n",
      "\n",
      "Feature types:\n",
      "  Stationary (transformations): 96\n",
      "  Levels/Other:                 30\n",
      "\n",
      "================================================================================\n",
      "CORE CANDIDATES (ARIMAX Input)\n",
      "================================================================================\n",
      "Requirements: Stationary + Positive Importance + VIF<10 (checked next)\n",
      "\n",
      "Selected: 20 candidates (will filter to 10 via VIF)\n",
      "\n",
      "Top 15 CORE candidates:\n",
      "                       Feature  Importance      Std  Stationary\n",
      "   Panamax_Orderbook_Pct_vol30   17.580039 7.954505        True\n",
      "                       MGO_yoy   17.349434 3.160071        True\n",
      "Panamax_Fleet_Growth_YoY_vol30   13.796821 3.106985        True\n",
      "                     P4_82_yoy   10.107181 6.149886        True\n",
      "               Atlantic_IP_yoy    6.490443 2.080036        True\n",
      "        Panamax_Idle_Pct_vol30    4.651007 2.151705        True\n",
      "  Capesize_Orderbook_Pct_vol30    4.115066 3.075942        True\n",
      "                    P4_82_diff    2.857903 0.979944        True\n",
      "                       BPI_yoy    2.733056 0.893819        True\n",
      "                     P4_82_pct    2.036469 0.726526        True\n",
      "     Atlantic_Port_Calls_vol30    1.892614 0.790390        True\n",
      "                  PDOPEX_vol30    1.712074 0.115342        True\n",
      "                      BPI_diff    1.350277 0.293650        True\n",
      "                    C5TC_vol30    1.159832 0.600022        True\n",
      "  Grain_Trade_Volume_Index_yoy    1.052047 1.631862        True\n",
      "\n",
      "================================================================================\n",
      "ML CANDIDATES (XGBoost Input)\n",
      "================================================================================\n",
      "Requirements: Importance > -5 (allows neutral features)\n",
      "\n",
      "Selected: 40 candidates (will select best 20)\n",
      "\n",
      "Top 20 ML candidates:\n",
      "                       Feature  Importance       Std  Stationary\n",
      "                     BPI_level 1783.060927 70.211561       False\n",
      "                   P4_82_level  113.018079 11.604054       False\n",
      "   Panamax_Orderbook_Pct_vol30   17.580039  7.954505        True\n",
      "                       MGO_yoy   17.349434  3.160071        True\n",
      "Panamax_Fleet_Growth_YoY_vol30   13.796821  3.106985        True\n",
      "                     P4_82_yoy   10.107181  6.149886        True\n",
      "               Atlantic_IP_yoy    6.490443  2.080036        True\n",
      "        Panamax_Idle_Pct_vol30    4.651007  2.151705        True\n",
      "  Capesize_Orderbook_Pct_vol30    4.115066  3.075942        True\n",
      "                    P4_82_diff    2.857903  0.979944        True\n",
      "                       BPI_yoy    2.733056  0.893819        True\n",
      "                 P3EA_1Q_level    2.368873  1.309517       False\n",
      "                     P4_82_pct    2.036469  0.726526        True\n",
      "     Atlantic_Port_Calls_vol30    1.892614  0.790390        True\n",
      "                  PDOPEX_vol30    1.712074  0.115342        True\n",
      "                      BPI_diff    1.350277  0.293650        True\n",
      "                     MGO_level    1.311943  0.303453       False\n",
      "             P1EA_CURMON_level    1.200511  0.319100       False\n",
      "                    C5TC_vol30    1.159832  0.600022        True\n",
      "           TC5yr_Pacific_level    1.103144  0.242228       False\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Step 4 - VIF Analysis on CORE Candidates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.525987Z",
     "start_time": "2025-11-01T08:20:39.511862Z"
    }
   },
   "source": [
    "def calculate_vif_and_filter(features_df, candidate_features, target_count=10, route_name=''):\n    \"\"\"\n    Calculate VIF for CORE candidates and iteratively remove high-VIF features.\n    \n    Goal: Ensure all CORE features have VIF < 10 (ARIMAX requirement)\n    \n    Algorithm:\n    1. Calculate VIF for all candidates\n    2. If max VIF < 10: Done\n    3. Else: Remove feature with highest VIF, repeat\n    4. Stop when: VIF < 10 OR reached target_count\n    \n    Parameters:\n    -----------\n    features_df : pd.DataFrame\n        All features data\n    candidate_features : pd.DataFrame\n        CORE candidates with rankings\n    target_count : int\n        Target number of final CORE features\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    final_features : list\n        Final CORE features (VIF < 10)\n    final_vif_df : pd.DataFrame\n        VIF values for final features\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"VIF ANALYSIS: {route_name} CORE FEATURES\")\n    print(f\"{'=' * 80}\")\n\n    # Get feature data\n    feature_names = candidate_features['Feature'].tolist()\n    X = features_df[feature_names].dropna()\n\n    print(f\"\\nInitial candidates: {len(feature_names)}\")\n    print(f\"Target count: {target_count}\")\n    print(f\"Goal: VIF < 10 for all features\\n\")\n\n    iteration = 0\n    # Iteratively remove high-VIF features\n    while len(feature_names) > target_count:\n        iteration += 1\n        \n        # Calculate VIF\n        vif_data = []\n        for i, col in enumerate(feature_names):\n            try:\n                vif = variance_inflation_factor(X[feature_names].values, i)\n                vif_data.append({'Feature': col, 'VIF': vif})\n            except:\n                # Handle singular matrix (perfect collinearity)\n                vif_data.append({'Feature': col, 'VIF': np.inf})\n\n        vif_df = pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n\n        # Check if all VIF < 10\n        max_vif = vif_df['VIF'].max()\n        if max_vif < 10:\n            print(f\"\\n[OK] Iteration {iteration}: All features have VIF < 10 (max = {max_vif:.2f})\")\n            break\n\n        # Remove worst feature\n        worst_feature = vif_df.iloc[0]['Feature']\n        worst_vif = vif_df.iloc[0]['VIF']\n        print(f\"Iteration {iteration}: Removing {worst_feature[:50]:<50} (VIF = {worst_vif:.2f})\")\n\n        feature_names.remove(worst_feature)\n        X = X.drop(columns=[worst_feature])\n\n        if len(feature_names) == target_count:\n            print(f\"\\n[OK] Reached target count: {target_count} features\")\n            break\n\n    # Final VIF calculation\n    print(f\"\\n{'=' * 80}\")\n    print(f\"FINAL VIF REPORT\")\n    print(f\"{'=' * 80}\")\n\n    final_vif = []\n    for i, col in enumerate(feature_names):\n        try:\n            vif = variance_inflation_factor(X.values, i)\n            final_vif.append({'Feature': col, 'VIF': vif})\n        except:\n            final_vif.append({'Feature': col, 'VIF': np.inf})\n\n    final_vif_df = pd.DataFrame(final_vif).sort_values('VIF', ascending=False)\n\n    print(f\"\\n FINAL {route_name} CORE FEATURES ({len(feature_names)}):\")\n    print(f\"\\n{'Rank':<6} {'Feature':<55} {'VIF':<10}\")\n    print(\"-\" * 72)\n    for i, row in final_vif_df.iterrows():\n        print(f\"{i+1:<6} {row['Feature']:<55} {row['VIF']:>8.2f}\")\n\n    print(f\"\\n{'=' * 80}\")\n    print(f\"VIF Summary:\")\n    print(f\"  Max VIF:  {final_vif_df['VIF'].max():.2f}\")\n    print(f\"  Mean VIF: {final_vif_df['VIF'].mean():.2f}\")\n    print(f\"  Min VIF:  {final_vif_df['VIF'].min():.2f}\")\n\n    if final_vif_df['VIF'].max() < 10:\n        print(f\"\\n[OK] All features pass VIF < 10 threshold\")\n    else:\n        print(f\"\\n[WARN]  WARNING: Some features still have VIF >= 10\")\n        print(f\"   Consider: Return to feature engineering OR accept higher VIF\")\n\n    return feature_names, final_vif_df\n\n\nprint(\"[OK] VIF analysis function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] VIF analysis function defined\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.834774Z",
     "start_time": "2025-11-01T08:20:39.547089Z"
    }
   },
   "source": [
    "# VIF analysis for P1A CORE\np1a_core_final, p1a_core_vif = calculate_vif_and_filter(\n    features_df=features_all,\n    candidate_features=p1a_core_candidates,\n    target_count=10,\n    route_name='P1A'\n)\n\n# Save VIF report\np1a_core_vif.to_csv('data/processed/features/p1a_core_vif_report.csv', index=False)\nprint(\"\\n[OK] P1A CORE VIF report saved: data/processed/features/p1a_core_vif_report.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VIF ANALYSIS: P1A CORE FEATURES\n",
      "================================================================================\n",
      "\n",
      "Initial candidates: 20\n",
      "Target count: 10\n",
      "Goal: VIF < 10 for all features\n",
      "\n",
      "Iteration 1: Removing Atlantic_Port_Calls_vol30                          (VIF = 10.02)\n",
      "\n",
      "[OK] Iteration 2: All features have VIF < 10 (max = 8.42)\n",
      "\n",
      "================================================================================\n",
      "FINAL VIF REPORT\n",
      "================================================================================\n",
      "\n",
      " FINAL P1A CORE FEATURES (19):\n",
      "\n",
      "Rank   Feature                                                 VIF       \n",
      "------------------------------------------------------------------------\n",
      "16     BPI_pct                                                     8.42\n",
      "18     BPI_diff                                                    7.01\n",
      "6      BPI_vol30                                                   5.69\n",
      "10     C5TC_vol30                                                  4.61\n",
      "14     P4_82_yoy                                                   4.05\n",
      "8      BPI_yoy                                                     3.67\n",
      "13     P4_82_pct                                                   3.25\n",
      "2      Coal_Trade_Volume_Index_vol30                               3.21\n",
      "9      Capesize_Orderbook_Pct_vol30                                2.97\n",
      "3      Panamax_Orderbook_Pct_vol30                                 2.95\n",
      "12     Grain_Trade_Volume_Index_vol30                              2.70\n",
      "11     C5TC_yoy                                                    1.90\n",
      "5      PDOPEX_vol30                                                1.67\n",
      "1      Atlantic_IP_yoy                                             1.60\n",
      "7      Coal_Trade_Volume_Index_yoy                                 1.37\n",
      "4      MGO_yoy                                                     1.30\n",
      "15     Capesize_Orderbook_Pct_yoy                                  1.19\n",
      "17     P1EA_1MON_diff                                              1.07\n",
      "19     Atlantic_Port_Calls_diff                                    1.01\n",
      "\n",
      "================================================================================\n",
      "VIF Summary:\n",
      "  Max VIF:  8.42\n",
      "  Mean VIF: 3.14\n",
      "  Min VIF:  1.01\n",
      "\n",
      "[OK] All features pass VIF < 10 threshold\n",
      "\n",
      "[OK] P1A CORE VIF report saved: data/processed/features/p1a_core_vif_report.csv\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.931702Z",
     "start_time": "2025-11-01T08:20:39.839804Z"
    }
   },
   "source": [
    "# VIF analysis for P3A CORE\np3a_core_final, p3a_core_vif = calculate_vif_and_filter(\n    features_df=features_all,\n    candidate_features=p3a_core_candidates,\n    target_count=10,\n    route_name='P3A'\n)\n\n# Save VIF report\np3a_core_vif.to_csv('data/processed/features/p3a_core_vif_report.csv', index=False)\nprint(\"\\n[OK] P3A CORE VIF report saved: data/processed/features/p3a_core_vif_report.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VIF ANALYSIS: P3A CORE FEATURES\n",
      "================================================================================\n",
      "\n",
      "Initial candidates: 20\n",
      "Target count: 10\n",
      "Goal: VIF < 10 for all features\n",
      "\n",
      "Iteration 1: Removing BPI_pct                                            (VIF = 12.43)\n",
      "Iteration 2: Removing Atlantic_Port_Calls_vol30                          (VIF = 12.18)\n",
      "\n",
      "[OK] Iteration 3: All features have VIF < 10 (max = 6.89)\n",
      "\n",
      "================================================================================\n",
      "FINAL VIF REPORT\n",
      "================================================================================\n",
      "\n",
      " FINAL P3A CORE FEATURES (18):\n",
      "\n",
      "Rank   Feature                                                 VIF       \n",
      "------------------------------------------------------------------------\n",
      "16     P4_82_vol30                                                 6.89\n",
      "6      Panamax_Idle_Pct_vol30                                      5.88\n",
      "15     MGO_vol30                                                   5.82\n",
      "8      P4_82_diff                                                  4.56\n",
      "10     P4_82_pct                                                   4.46\n",
      "4      P4_82_yoy                                                   4.03\n",
      "3      Panamax_Fleet_Growth_YoY_vol30                              3.76\n",
      "13     C5TC_vol30                                                  3.74\n",
      "17     Coal_Trade_Volume_Index_vol30                               3.63\n",
      "9      BPI_yoy                                                     3.53\n",
      "1      Panamax_Orderbook_Pct_vol30                                 3.45\n",
      "7      Capesize_Orderbook_Pct_vol30                                3.11\n",
      "12     BPI_diff                                                    2.68\n",
      "5      Atlantic_IP_yoy                                             1.74\n",
      "11     PDOPEX_vol30                                                1.71\n",
      "2      MGO_yoy                                                     1.30\n",
      "18     C5TC_diff                                                   1.18\n",
      "14     Grain_Trade_Volume_Index_yoy                                1.16\n",
      "\n",
      "================================================================================\n",
      "VIF Summary:\n",
      "  Max VIF:  6.89\n",
      "  Mean VIF: 3.48\n",
      "  Min VIF:  1.16\n",
      "\n",
      "[OK] All features pass VIF < 10 threshold\n",
      "\n",
      "[OK] P3A CORE VIF report saved: data/processed/features/p3a_core_vif_report.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Step 5 - Finalize ML Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:39.956714Z",
     "start_time": "2025-11-01T08:20:39.947708Z"
    }
   },
   "source": [
    "def finalize_ml_features(core_features, ml_candidates, features_all_df, target_count=20, route_name=''):\n    \"\"\"\n    Select final ML features (complementary to CORE).\n    \n    Strategy:\n    1. Prioritize features NOT in CORE (for diversity)\n    2. Then select by importance ranking\n    3. Can include levels, correlated features (XGBoost robust)\n    \n    Parameters:\n    -----------\n    core_features : list\n        Final CORE features\n    ml_candidates : pd.DataFrame\n        ML candidates with rankings\n    features_all_df : pd.DataFrame\n        All features data\n    target_count : int\n        Target number of ML features\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    ml_final : list\n        Final ML features\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"FINALIZING ML FEATURES: {route_name}\")\n    print(f\"{'=' * 80}\")\n\n    # Prioritize features NOT in CORE (for diversity)\n    ml_candidates_sorted = ml_candidates.copy()\n    ml_candidates_sorted['In_CORE'] = ml_candidates_sorted['Feature'].isin(core_features)\n    ml_candidates_sorted = ml_candidates_sorted.sort_values(\n        ['In_CORE', 'Importance'], ascending=[True, False]\n    )\n\n    # Select top features\n    ml_final = ml_candidates_sorted.head(target_count)['Feature'].tolist()\n\n    overlap_count = len(set(ml_final) & set(core_features))\n    unique_count = len(ml_final) - overlap_count\n\n    print(f\"\\nSelected: {len(ml_final)} features\")\n    print(f\"  Overlap with CORE: {overlap_count} features ({overlap_count/len(ml_final)*100:.1f}%)\")\n    print(f\"  Unique to ML:      {unique_count} features ({unique_count/len(ml_final)*100:.1f}%)\")\n\n    # Show final ML features\n    ml_final_df = ml_candidates_sorted.head(target_count)[['Feature', 'Importance', 'In_CORE']]\n    \n    print(f\"\\n FINAL {route_name} ML FEATURES ({len(ml_final)}):\")\n    print(f\"\\n{'Rank':<6} {'Feature':<55} {'Importance':<12} {'In CORE':<10}\")\n    print(\"-\" * 85)\n    for i, (idx, row) in enumerate(ml_final_df.iterrows(), 1):\n        in_core_str = '[OK]' if row['In_CORE'] else ''\n        print(f\"{i:<6} {row['Feature']:<55} {row['Importance']:>10.2f}  {in_core_str:<10}\")\n\n    return ml_final\n\n\nprint(\"[OK] ML finalization function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] ML finalization function defined\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.002675Z",
     "start_time": "2025-11-01T08:20:39.993333Z"
    }
   },
   "source": [
    "# Finalize P1A ML features\n",
    "p1a_ml_final = finalize_ml_features(\n",
    "    core_features=p1a_core_final,\n",
    "    ml_candidates=p1a_ml_candidates,\n",
    "    features_all_df=features_all,\n",
    "    target_count=20,\n",
    "    route_name='P1A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINALIZING ML FEATURES: P1A\n",
      "================================================================================\n",
      "\n",
      "Selected: 20 features\n",
      "  Overlap with CORE: 0 features (0.0%)\n",
      "  Unique to ML:      20 features (100.0%)\n",
      "\n",
      " FINAL P1A ML FEATURES (20):\n",
      "\n",
      "Rank   Feature                                                 Importance   In CORE   \n",
      "-------------------------------------------------------------------------------------\n",
      "1      BPI_level                                                  3088.02            \n",
      "2      TC5yr_Atlantic_level                                         71.32            \n",
      "3      P1EA_CURMON_level                                            14.53            \n",
      "4      P1EA_1MON_level                                               8.87            \n",
      "5      P3EA_1Q_level                                                 5.35            \n",
      "6      Panamax_Deliveries_DWT_level                                  5.26            \n",
      "7      Panamax_Idle_Pct_level                                        3.90            \n",
      "8      Atlantic_Port_Calls_level                                     2.15            \n",
      "9      Atlantic_Port_Calls_vol30                                     2.10            \n",
      "10     TC5yr_Pacific_level                                           1.21            \n",
      "11     Panamax_Fleet_Growth_YoY_level                                0.77            \n",
      "12     C5TC_level                                                    0.53            \n",
      "13     C5TC_diff                                                     0.05            \n",
      "14     Atlantic_Port_Calls_pct                                       0.03            \n",
      "15     Panamax_Idle_Pct_pct                                          0.02            \n",
      "16     MGO_level                                                     0.01            \n",
      "17     P3EA_1Q_diff                                                  0.00            \n",
      "18     Coal_Trade_Volume_Index_diff                                  0.00            \n",
      "19     World_Coal_Trade_MT_level                                     0.00            \n",
      "20     Australia_Coal_Exports_MT_diff                                0.00            \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.032585Z",
     "start_time": "2025-11-01T08:20:40.023684Z"
    }
   },
   "source": [
    "# Finalize P3A ML features\n",
    "p3a_ml_final = finalize_ml_features(\n",
    "    core_features=p3a_core_final,\n",
    "    ml_candidates=p3a_ml_candidates,\n",
    "    features_all_df=features_all,\n",
    "    target_count=20,\n",
    "    route_name='P3A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINALIZING ML FEATURES: P3A\n",
      "================================================================================\n",
      "\n",
      "Selected: 20 features\n",
      "  Overlap with CORE: 0 features (0.0%)\n",
      "  Unique to ML:      20 features (100.0%)\n",
      "\n",
      " FINAL P3A ML FEATURES (20):\n",
      "\n",
      "Rank   Feature                                                 Importance   In CORE   \n",
      "-------------------------------------------------------------------------------------\n",
      "1      BPI_level                                                  1783.06            \n",
      "2      P4_82_level                                                 113.02            \n",
      "3      P3EA_1Q_level                                                 2.37            \n",
      "4      Atlantic_Port_Calls_vol30                                     1.89            \n",
      "5      MGO_level                                                     1.31            \n",
      "6      P1EA_CURMON_level                                             1.20            \n",
      "7      TC5yr_Pacific_level                                           1.10            \n",
      "8      World_Coal_Trade_MT_level                                     0.34            \n",
      "9      BPI_pct                                                       0.24            \n",
      "10     Panamax_Fleet_Growth_YoY_yoy                                  0.08            \n",
      "11     Panamax_Idle_Pct_level                                        0.08            \n",
      "12     VLSFO_pct                                                     0.07            \n",
      "13     VLSFO_diff                                                    0.03            \n",
      "14     C5TC_pct                                                      0.02            \n",
      "15     P1EA_1MON_level                                               0.01            \n",
      "16     Atlantic_Port_Calls_diff                                      0.00            \n",
      "17     Indonesia_Coal_Exports_MT_level                               0.00            \n",
      "18     Coal_Trade_Volume_Index_pct                                   0.00            \n",
      "19     Australia_Coal_Exports_MT_pct                                 0.00            \n",
      "20     Australia_Coal_Exports_MT_diff                                0.00            \n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Step 6 - Save Final Feature Sets & Allocation Decisions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.220820Z",
     "start_time": "2025-11-01T08:20:40.082114Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"SAVING FINAL FEATURE SETS & ALLOCATION DECISIONS\")\nprint(\"=\"*80)\n\n# Create allocation decision log\ndecisions = []\n\n# P1A CORE\nfor feat in p1a_core_final:\n    imp = feature_ranking_p1a[feature_ranking_p1a['Feature'] == feat]['Importance'].values[0]\n    vif = p1a_core_vif[p1a_core_vif['Feature'] == feat]['VIF'].values[0]\n    decisions.append({\n        'Route': 'P1A',\n        'Set': 'CORE',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': vif,\n        'Rationale': 'Stationary, positive importance, VIF < 10'\n    })\n\n# P1A ML\nfor feat in p1a_ml_final:\n    imp = feature_ranking_p1a[feature_ranking_p1a['Feature'] == feat]['Importance'].values[0]\n    in_core = feat in p1a_core_final\n    decisions.append({\n        'Route': 'P1A',\n        'Set': 'ML',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Rationale': f\"Positive/neutral importance{', also in CORE' if in_core else ', unique to ML'}\"\n    })\n\n# P3A CORE\nfor feat in p3a_core_final:\n    imp = feature_ranking_p3a[feature_ranking_p3a['Feature'] == feat]['Importance'].values[0]\n    vif = p3a_core_vif[p3a_core_vif['Feature'] == feat]['VIF'].values[0]\n    decisions.append({\n        'Route': 'P3A',\n        'Set': 'CORE',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': vif,\n        'Rationale': 'Stationary, positive importance, VIF < 10'\n    })\n\n# P3A ML\nfor feat in p3a_ml_final:\n    imp = feature_ranking_p3a[feature_ranking_p3a['Feature'] == feat]['Importance'].values[0]\n    in_core = feat in p3a_core_final\n    decisions.append({\n        'Route': 'P3A',\n        'Set': 'ML',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Rationale': f\"Positive/neutral importance{', also in CORE' if in_core else ', unique to ML'}\"\n    })\n\n# Save decisions\ndecisions_df = pd.DataFrame(decisions)\ndecisions_df.to_csv('data/processed/features/feature_allocation_decisions.csv', index=False)\nprint(\"\\n[OK] Allocation decisions saved: data/processed/features/feature_allocation_decisions.csv\")\nprint(f\"   Total decisions logged: {len(decisions_df)}\")\n\n# Save final feature sets (data)\nfeatures_all[p1a_core_final].to_csv('data/processed/features/p1a_core_features_final.csv')\nfeatures_all[p1a_ml_final].to_csv('data/processed/features/p1a_ml_features_final.csv')\nfeatures_all[p3a_core_final].to_csv('data/processed/features/p3a_core_features_final.csv')\nfeatures_all[p3a_ml_final].to_csv('data/processed/features/p3a_ml_features_final.csv')\n\nprint(\"\\n[OK] Final feature data saved:\")\nprint(f\"   - P1A CORE: {len(p1a_core_final)} features × {len(features_all)} rows\")\nprint(f\"   - P1A ML:   {len(p1a_ml_final)} features × {len(features_all)} rows\")\nprint(f\"   - P3A CORE: {len(p3a_core_final)} features × {len(features_all)} rows\")\nprint(f\"   - P3A ML:   {len(p3a_ml_final)} features × {len(features_all)} rows\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"[OK] ALL FINAL FEATURE SETS SAVED\")\nprint(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING FINAL FEATURE SETS & ALLOCATION DECISIONS\n",
      "================================================================================\n",
      "\n",
      "[OK] Allocation decisions saved: data/processed/features/feature_allocation_decisions.csv\n",
      "   Total decisions logged: 77\n",
      "\n",
      "[OK] Final feature data saved:\n",
      "   - P1A CORE: 19 features × 1153 rows\n",
      "   - P1A ML:   20 features × 1153 rows\n",
      "   - P3A CORE: 18 features × 1153 rows\n",
      "   - P3A ML:   20 features × 1153 rows\n",
      "\n",
      "================================================================================\n",
      "[OK] ALL FINAL FEATURE SETS SAVED\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Step 7 - Quality Gate Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.236698Z",
     "start_time": "2025-11-01T08:20:40.226863Z"
    }
   },
   "source": [
    "def validate_quality_gate(core_vif_df, ml_importance_df, route_name=''):\n    \"\"\"\n    Check if final feature sets pass quality gate.\n    \n    Quality Criteria:\n    - CORE: Max VIF < 10, Mean VIF < 5\n    - ML: Max Importance > 20, Mean Importance > 5, % Positive > 75%\n    \n    Parameters:\n    -----------\n    core_vif_df : pd.DataFrame\n        VIF report for CORE features\n    ml_importance_df : pd.DataFrame\n        Importance rankings for ML features\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    overall_pass : bool\n        True if passed quality gate\n    \"\"\"\n    print(f\"\\n{'=' * 80}\")\n    print(f\"QUALITY GATE VALIDATION: {route_name}\")\n    print(f\"{'=' * 80}\")\n\n    # CORE checks\n    max_vif = core_vif_df['VIF'].max()\n    mean_vif = core_vif_df['VIF'].mean()\n    core_pass_strict = max_vif < 10\n    core_pass_ideal = mean_vif < 5\n\n    print(f\"\\n CORE Features (ARIMAX):\")\n    print(f\"  Count:    {len(core_vif_df)}\")\n    print(f\"  Max VIF:  {max_vif:.2f} (threshold: < 10) {'[OK]' if core_pass_strict else '[FAIL]'}\")\n    print(f\"  Mean VIF: {mean_vif:.2f} (ideal: < 5) {'[OK]' if core_pass_ideal else '[WARN]'}\")\n\n    # ML checks\n    max_imp = ml_importance_df['Importance'].max()\n    mean_imp = ml_importance_df['Importance'].mean()\n    median_imp = ml_importance_df['Importance'].median()\n    pct_positive = (ml_importance_df['Importance'] > 0).mean() * 100\n    \n    ml_pass_max = max_imp > 20\n    ml_pass_mean = mean_imp > 5\n    ml_pass_pct = pct_positive > 75\n\n    print(f\"\\n ML Features (XGBoost):\")\n    print(f\"  Count:               {len(ml_importance_df)}\")\n    print(f\"  Max Importance:      {max_imp:.2f} (threshold: > 20) {'[OK]' if ml_pass_max else '[FAIL]'}\")\n    print(f\"  Mean Importance:     {mean_imp:.2f} (threshold: > 5) {'[OK]' if ml_pass_mean else '[FAIL]'}\")\n    print(f\"  Median Importance:   {median_imp:.2f}\")\n    print(f\"  % Positive Features: {pct_positive:.1f}% (threshold: > 75%) {'[OK]' if ml_pass_pct else '[FAIL]'}\")\n\n    # Overall decision\n    core_pass = core_pass_strict\n    ml_pass = ml_pass_max and ml_pass_mean and ml_pass_pct\n    overall_pass = core_pass and ml_pass\n\n    print(f\"\\n{'=' * 80}\")\n    print(f\"DECISION FOR {route_name}:\")\n    print(f\"{'=' * 80}\")\n    \n    if overall_pass:\n        print(f\"\\n[OK] {route_name} PASSED QUALITY GATE\")\n        print(f\"   - CORE features: VIF < 10 [OK]\")\n        print(f\"   - ML features: Strong importance [OK]\")\n        print(f\"   - Ready for modeling\")\n    elif core_pass and not ml_pass:\n        print(f\"\\n[WARN]  {route_name} PARTIAL PASS (CORE OK, ML WEAK)\")\n        print(f\"   - CORE features: VIF < 10 [OK]\")\n        print(f\"   - ML features: Below thresholds [FAIL]\")\n        print(f\"   - Consider: Proceed but expect ML underperformance\")\n    elif ml_pass and not core_pass:\n        print(f\"\\n[WARN]  {route_name} PARTIAL PASS (ML OK, CORE WEAK)\")\n        print(f\"   - CORE features: VIF >= 10 [FAIL]\")\n        print(f\"   - ML features: Strong importance [OK]\")\n        print(f\"   - Consider: Accept higher VIF OR revise CORE features\")\n    else:\n        print(f\"\\n[FAIL] {route_name} FAILED QUALITY GATE\")\n        print(f\"   - CORE features: VIF >= 10 [FAIL]\")\n        print(f\"   - ML features: Below thresholds [FAIL]\")\n        print(f\"   - Action: Return to Notebook 02 (feature engineering)\")\n\n    return overall_pass\n\n\nprint(\"[OK] Quality gate function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Quality gate function defined\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.263802Z",
     "start_time": "2025-11-01T08:20:40.257796Z"
    }
   },
   "source": [
    "# Validate P1A\n",
    "p1a_ml_importance = feature_ranking_p1a[feature_ranking_p1a['Feature'].isin(p1a_ml_final)]\n",
    "\n",
    "p1a_pass = validate_quality_gate(\n",
    "    core_vif_df=p1a_core_vif,\n",
    "    ml_importance_df=p1a_ml_importance,\n",
    "    route_name='P1A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUALITY GATE VALIDATION: P1A\n",
      "================================================================================\n",
      "\n",
      " CORE Features (ARIMAX):\n",
      "  Count:    19\n",
      "  Max VIF:  8.42 (threshold: < 10) [OK]\n",
      "  Mean VIF: 3.14 (ideal: < 5) [OK]\n",
      "\n",
      " ML Features (XGBoost):\n",
      "  Count:               20\n",
      "  Max Importance:      3088.02 (threshold: > 20) [OK]\n",
      "  Mean Importance:     160.21 (threshold: > 5) [OK]\n",
      "  Median Importance:   0.99\n",
      "  % Positive Features: 80.0% (threshold: > 75%) [OK]\n",
      "\n",
      "================================================================================\n",
      "DECISION FOR P1A:\n",
      "================================================================================\n",
      "\n",
      "[OK] P1A PASSED QUALITY GATE\n",
      "   - CORE features: VIF < 10 [OK]\n",
      "   - ML features: Strong importance [OK]\n",
      "   - Ready for modeling\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.294278Z",
     "start_time": "2025-11-01T08:20:40.287463Z"
    }
   },
   "source": [
    "# Validate P3A\n",
    "p3a_ml_importance = feature_ranking_p3a[feature_ranking_p3a['Feature'].isin(p3a_ml_final)]\n",
    "\n",
    "p3a_pass = validate_quality_gate(\n",
    "    core_vif_df=p3a_core_vif,\n",
    "    ml_importance_df=p3a_ml_importance,\n",
    "    route_name='P3A'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUALITY GATE VALIDATION: P3A\n",
      "================================================================================\n",
      "\n",
      " CORE Features (ARIMAX):\n",
      "  Count:    18\n",
      "  Max VIF:  6.89 (threshold: < 10) [OK]\n",
      "  Mean VIF: 3.48 (ideal: < 5) [OK]\n",
      "\n",
      " ML Features (XGBoost):\n",
      "  Count:               20\n",
      "  Max Importance:      1783.06 (threshold: > 20) [OK]\n",
      "  Mean Importance:     95.24 (threshold: > 5) [OK]\n",
      "  Median Importance:   0.08\n",
      "  % Positive Features: 100.0% (threshold: > 75%) [OK]\n",
      "\n",
      "================================================================================\n",
      "DECISION FOR P3A:\n",
      "================================================================================\n",
      "\n",
      "[OK] P3A PASSED QUALITY GATE\n",
      "   - CORE features: VIF < 10 [OK]\n",
      "   - ML features: Strong importance [OK]\n",
      "   - Ready for modeling\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.330243Z",
     "start_time": "2025-11-01T08:20:40.316284Z"
    }
   },
   "source": [
    "# Overall Sprint Decision\nprint(\"\\n\" + \"=\"*80)\nprint(\"OVERALL SPRINT DECISION\")\nprint(\"=\"*80)\n\nif p1a_pass and p3a_pass:\n    decision = \"[OK] PROCEED TO PHASE 3: Data Preparation\"\n    action = \"Both routes passed quality gate. Ready for Notebook 04.\"\n    status = \"PASS\"\nelif p1a_pass or p3a_pass:\n    decision = \"[WARN]  PARTIAL PASS: Proceed with Caution\"\n    if p1a_pass:\n        action = \"P1A passed, P3A weak. Proceed to modeling, monitor P3A performance.\"\n    else:\n        action = \"P3A passed, P1A weak. Proceed to modeling, monitor P1A performance.\"\n    status = \"PARTIAL\"\nelse:\n    decision = \"[FAIL] FAIL: Return to Phase 1 (Feature Engineering)\"\n    action = \"Both routes failed. Revise features in Notebook 02, iterate.\"\n    status = \"FAIL\"\n\nprint(f\"\\n{decision}\")\nprint(f\"\\n Action: {action}\")\n\n# Save decision\ndecision_summary = pd.DataFrame([{\n    'Date': pd.Timestamp.now(),\n    'P1A_Pass': p1a_pass,\n    'P3A_Pass': p3a_pass,\n    'Overall_Status': status,\n    'Decision': decision,\n    'Action': action,\n    'P1A_CORE_Count': len(p1a_core_final),\n    'P1A_ML_Count': len(p1a_ml_final),\n    'P3A_CORE_Count': len(p3a_core_final),\n    'P3A_ML_Count': len(p3a_ml_final),\n    'P1A_Max_VIF': p1a_core_vif['VIF'].max(),\n    'P3A_Max_VIF': p3a_core_vif['VIF'].max(),\n    'P1A_Max_Importance': p1a_ml_importance['Importance'].max(),\n    'P3A_Max_Importance': p3a_ml_importance['Importance'].max(),\n}])\n\ndecision_summary.to_csv('data/processed/features/quality_gate_decision.csv', index=False)\nprint(\"\\n[OK] Quality gate decision saved: data/processed/features/quality_gate_decision.csv\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"NOTEBOOK 03 COMPLETE\")\nprint(\"=\"*80)\nprint(\"\\n Summary:\")\nprint(f\"   - P1A CORE: {len(p1a_core_final)} features (VIF < 10: {p1a_pass})\")\nprint(f\"   - P1A ML:   {len(p1a_ml_final)} features\")\nprint(f\"   - P3A CORE: {len(p3a_core_final)} features (VIF < 10: {p3a_pass})\")\nprint(f\"   - P3A ML:   {len(p3a_ml_final)} features\")\nprint(f\"\\n Next Step: {action}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERALL SPRINT DECISION\n",
      "================================================================================\n",
      "\n",
      "[OK] PROCEED TO PHASE 3: Data Preparation\n",
      "\n",
      " Action: Both routes passed quality gate. Ready for Notebook 04.\n",
      "\n",
      "[OK] Quality gate decision saved: data/processed/features/quality_gate_decision.csv\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK 03 COMPLETE\n",
      "================================================================================\n",
      "\n",
      " Summary:\n",
      "   - P1A CORE: 19 features (VIF < 10: True)\n",
      "   - P1A ML:   20 features\n",
      "   - P3A CORE: 18 features (VIF < 10: True)\n",
      "   - P3A ML:   20 features\n",
      "\n",
      " Next Step: Both routes passed quality gate. Ready for Notebook 04.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "## Section 9: Step 8 - REDLINE: Reduce CORE to 6-8 Features\n\n**CRITICAL DECISION:** Reduce CORE features from 19/18 to 6-8 to address ARIMAX overfitting\n\n**Rationale:**\n- Previous ARIMAX tests showed severe overfitting (Val R² < -1.0)\n- Too many exogenous features relative to data (600 rows, 25 params = 24 obs/param)\n- Target: 6-8 features per route for better parameter-to-observation ratio\n\n**Selection Algorithm:**\n1. **Filter by importance:** Keep only features with Importance > 5 (strong predictors)\n2. **Prioritize daily features:** Prefer vol30 (score=3) > yoy/diff (score=2) > pct (score=1)\n3. **Data quality:** Prioritize features with < 5% missing values\n4. **Re-check VIF:** Ensure all final features have VIF < 10\n5. **Target:** Exactly 6-8 features\n\n**All decisions made programmatically with full audit trail**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def reduce_core_features_programmatic(core_features, core_vif_df, feature_ranking_df,\n                                      features_all_df, target_range=(6, 8), route_name=''):\n    \"\"\"\n    Reduce CORE features to 6-8 using programmatic criteria.\n    \n    Selection Algorithm:\n    1. Filter by importance > 5 (strong predictors)\n    2. Calculate feature type score: vol30=3, yoy/diff=2, pct/mom=1, other=0\n    3. Calculate data quality score: % non-missing values\n    4. Rank by: (importance_rank * 0.5) + (type_score * 0.3) + (quality_score * 0.2)\n    5. Select top 6-8, re-check VIF\n    \n    Parameters:\n    -----------\n    core_features : list\n        Current CORE features (19/18)\n    core_vif_df : pd.DataFrame\n        VIF values for CORE features\n    feature_ranking_df : pd.DataFrame\n        Feature importance rankings\n    features_all_df : pd.DataFrame\n        All feature data\n    target_range : tuple\n        (min, max) number of features to select\n    route_name : str\n        Route identifier\n        \n    Returns:\n    --------\n    final_features : list\n        Reduced CORE features (6-8)\n    reduction_audit : pd.DataFrame\n        Audit trail of selection decisions\n    \"\"\"\n    \n    print(f\"\\n{'=' * 80}\")\n    print(f\"PROGRAMMATIC CORE FEATURE REDUCTION: {route_name}\")\n    print(f\"{'=' * 80}\")\n    \n    print(f\"\\nInitial CORE features: {len(core_features)}\")\n    print(f\"Target: {target_range[0]}-{target_range[1]} features\")\n    \n    # Step 1: Get importance scores\n    feature_scores = []\n    for feat in core_features:\n        importance = feature_ranking_df[feature_ranking_df['Feature'] == feat]['Importance'].values[0]\n        vif = core_vif_df[core_vif_df['Feature'] == feat]['VIF'].values[0]\n        \n        # Data quality: % non-missing\n        data_quality = (1 - features_all_df[feat].isnull().sum() / len(features_all_df)) * 100\n        \n        # Feature type score (daily transformations preferred)\n        if feat.endswith('_vol30'):\n            type_score = 3\n            type_name = 'vol30 (30-day volatility)'\n        elif feat.endswith('_yoy') or feat.endswith('_diff'):\n            type_score = 2\n            type_name = 'yoy/diff (year-over-year or first diff)'\n        elif feat.endswith('_pct') or feat.endswith('_mom'):\n            type_score = 1\n            type_name = 'pct/mom (percentage change)'\n        else:\n            type_score = 0\n            type_name = 'other'\n        \n        feature_scores.append({\n            'Feature': feat,\n            'Importance': importance,\n            'VIF': vif,\n            'Data_Quality_Pct': data_quality,\n            'Type_Score': type_score,\n            'Type_Name': type_name\n        })\n    \n    scores_df = pd.DataFrame(feature_scores)\n    \n    # Step 2: Filter by importance > 5\n    print(f\"\\n Step 1: Filter by Importance > 5\")\n    strong_features = scores_df[scores_df['Importance'] > 5].copy()\n    print(f\"   Remaining: {len(strong_features)} features\")\n    \n    if len(strong_features) < target_range[0]:\n        # Relax threshold if too few features\n        print(f\"   [WARN]  Too few features with Importance > 5\")\n        print(f\"   Relaxing to Importance > 1\")\n        strong_features = scores_df[scores_df['Importance'] > 1].copy()\n        print(f\"   Remaining: {len(strong_features)} features\")\n    \n    # Step 3: Calculate composite score\n    print(f\"\\n Step 2: Calculate Composite Score\")\n    print(f\"   Formula: (Importance_rank * 0.5) + (Type_score * 0.3) + (Quality_score * 0.2)\")\n    \n    # Normalize scores to 0-1\n    strong_features['Importance_norm'] = (strong_features['Importance'] - strong_features['Importance'].min()) / (strong_features['Importance'].max() - strong_features['Importance'].min() + 1e-10)\n    strong_features['Type_score_norm'] = strong_features['Type_Score'] / 3.0  # Max score is 3\n    strong_features['Quality_norm'] = strong_features['Data_Quality_Pct'] / 100.0\n    \n    # Composite score\n    strong_features['Composite_Score'] = (\n        strong_features['Importance_norm'] * 0.5 +\n        strong_features['Type_score_norm'] * 0.3 +\n        strong_features['Quality_norm'] * 0.2\n    )\n    \n    # Sort by composite score\n    strong_features = strong_features.sort_values('Composite_Score', ascending=False)\n    \n    # Step 4: Select top 6-8 features\n    target_count = min(target_range[1], len(strong_features))\n    if target_count > target_range[0] and len(strong_features) >= target_range[0]:\n        # Prefer upper bound if possible\n        selected_features = strong_features.head(target_count)['Feature'].tolist()\n    else:\n        selected_features = strong_features.head(target_range[0])['Feature'].tolist()\n    \n    print(f\"\\n Step 3: Select Top {len(selected_features)} Features\")\n    \n    # Step 5: VIF re-check\n    print(f\"\\n Step 4: VIF Re-check\")\n    X_selected = features_all_df[selected_features].dropna()\n    \n    final_vif = []\n    for i, col in enumerate(selected_features):\n        try:\n            vif = variance_inflation_factor(X_selected.values, i)\n            final_vif.append({'Feature': col, 'VIF': vif})\n        except:\n            final_vif.append({'Feature': col, 'VIF': np.inf})\n    \n    final_vif_df = pd.DataFrame(final_vif)\n    max_vif = final_vif_df['VIF'].max()\n    \n    print(f\"   Max VIF: {max_vif:.2f}\")\n    \n    if max_vif >= 10:\n        print(f\"   [WARN]  Some features have VIF >= 10, removing highest VIF feature\")\n        worst_feat = final_vif_df.loc[final_vif_df['VIF'].idxmax(), 'Feature']\n        selected_features.remove(worst_feat)\n        print(f\"   Removed: {worst_feat}\")\n        \n        # Recalculate VIF\n        X_selected = features_all_df[selected_features].dropna()\n        final_vif = []\n        for i, col in enumerate(selected_features):\n            try:\n                vif = variance_inflation_factor(X_selected.values, i)\n                final_vif.append({'Feature': col, 'VIF': vif})\n            except:\n                final_vif.append({'Feature': col, 'VIF': np.inf})\n        \n        final_vif_df = pd.DataFrame(final_vif)\n        max_vif = final_vif_df['VIF'].max()\n        print(f\"   New Max VIF: {max_vif:.2f}\")\n    \n    # Create audit trail\n    audit_df = strong_features[strong_features['Feature'].isin(selected_features)].copy()\n    audit_df = audit_df.merge(final_vif_df, on='Feature', suffixes=('_old', '_new'))\n    audit_df = audit_df[['Feature', 'Importance', 'VIF_new', 'Data_Quality_Pct', \n                          'Type_Score', 'Type_Name', 'Composite_Score']]\n    audit_df = audit_df.sort_values('Composite_Score', ascending=False)\n    \n    # Display results\n    print(f\"\\n{'=' * 80}\")\n    print(f\"FINAL REDUCED CORE FEATURES: {route_name} ({len(selected_features)} features)\")\n    print(f\"{'=' * 80}\")\n    print(f\"\\n{'Rank':<6} {'Feature':<40} {'Imp':<8} {'VIF':<6} {'Quality':<8} {'Type':<6} {'Score':<8}\")\n    print(\"-\" * 90)\n    for i, row in audit_df.iterrows():\n        print(f\"{audit_df.index.tolist().index(i)+1:<6} {row['Feature']:<40} {row['Importance']:>6.2f}  {row['VIF_new']:>5.2f}  {row['Data_Quality_Pct']:>6.1f}%  {int(row['Type_Score']):>4}     {row['Composite_Score']:>6.4f}\")\n    \n    print(f\"\\n{'=' * 80}\")\n    print(f\"REDUCTION SUMMARY:\")\n    print(f\"{'=' * 80}\")\n    print(f\"  Initial features: {len(core_features)}\")\n    print(f\"  Final features:   {len(selected_features)}\")\n    print(f\"  Reduction:        {len(core_features) - len(selected_features)} features ({(1 - len(selected_features)/len(core_features))*100:.1f}%)\")\n    print(f\"  Max VIF:          {max_vif:.2f}\")\n    print(f\"  Feature types:\")\n    type_counts = audit_df['Type_Name'].value_counts()\n    for type_name, count in type_counts.items():\n        print(f\"    - {type_name}: {count} features\")\n    \n    return selected_features, audit_df\n\nprint(\"[OK] Feature reduction function defined\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.373763Z",
     "start_time": "2025-11-01T08:20:40.354239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Feature reduction function defined\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "# Apply reduction to P1A\np1a_core_reduced, p1a_reduction_audit = reduce_core_features_programmatic(\n    core_features=p1a_core_final,\n    core_vif_df=p1a_core_vif,\n    feature_ranking_df=feature_ranking_p1a,\n    features_all_df=features_all,\n    target_range=(6, 8),\n    route_name='P1A'\n)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.442240Z",
     "start_time": "2025-11-01T08:20:40.385277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROGRAMMATIC CORE FEATURE REDUCTION: P1A\n",
      "================================================================================\n",
      "\n",
      "Initial CORE features: 19\n",
      "Target: 6-8 features\n",
      "\n",
      " Step 1: Filter by Importance > 5\n",
      "   Remaining: 5 features\n",
      "   [WARN]  Too few features with Importance > 5\n",
      "   Relaxing to Importance > 1\n",
      "   Remaining: 12 features\n",
      "\n",
      " Step 2: Calculate Composite Score\n",
      "   Formula: (Importance_rank * 0.5) + (Type_score * 0.3) + (Quality_score * 0.2)\n",
      "\n",
      " Step 3: Select Top 8 Features\n",
      "\n",
      " Step 4: VIF Re-check\n",
      "   Max VIF: 5.16\n",
      "\n",
      "================================================================================\n",
      "FINAL REDUCED CORE FEATURES: P1A (8 features)\n",
      "================================================================================\n",
      "\n",
      "Rank   Feature                                  Imp      VIF    Quality  Type   Score   \n",
      "------------------------------------------------------------------------------------------\n",
      "1      Coal_Trade_Volume_Index_vol30             13.15   3.01    97.4%     3     0.9886\n",
      "2      Atlantic_IP_yoy                           13.30   1.54    99.9%     2     0.8998\n",
      "3      Panamax_Orderbook_Pct_vol30                7.71   2.59    97.4%     3     0.7568\n",
      "4      PDOPEX_vol30                               6.71   1.46    97.4%     3     0.7143\n",
      "5      MGO_yoy                                    6.99   1.11    98.9%     2     0.6292\n",
      "6      BPI_vol30                                  4.35   5.16    97.4%     3     0.6140\n",
      "7      Capesize_Orderbook_Pct_vol30               2.41   2.63    97.4%     3     0.5316\n",
      "8      C5TC_vol30                                 1.82   4.05    97.4%     3     0.5066\n",
      "\n",
      "================================================================================\n",
      "REDUCTION SUMMARY:\n",
      "================================================================================\n",
      "  Initial features: 19\n",
      "  Final features:   8\n",
      "  Reduction:        11 features (57.9%)\n",
      "  Max VIF:          5.16\n",
      "  Feature types:\n",
      "    - vol30 (30-day volatility): 6 features\n",
      "    - yoy/diff (year-over-year or first diff): 2 features\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "# Apply reduction to P3A\np3a_core_reduced, p3a_reduction_audit = reduce_core_features_programmatic(\n    core_features=p3a_core_final,\n    core_vif_df=p3a_core_vif,\n    feature_ranking_df=feature_ranking_p3a,\n    features_all_df=features_all,\n    target_range=(6, 8),\n    route_name='P3A'\n)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.495742Z",
     "start_time": "2025-11-01T08:20:40.461253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROGRAMMATIC CORE FEATURE REDUCTION: P3A\n",
      "================================================================================\n",
      "\n",
      "Initial CORE features: 18\n",
      "Target: 6-8 features\n",
      "\n",
      " Step 1: Filter by Importance > 5\n",
      "   Remaining: 5 features\n",
      "   [WARN]  Too few features with Importance > 5\n",
      "   Relaxing to Importance > 1\n",
      "   Remaining: 14 features\n",
      "\n",
      " Step 2: Calculate Composite Score\n",
      "   Formula: (Importance_rank * 0.5) + (Type_score * 0.3) + (Quality_score * 0.2)\n",
      "\n",
      " Step 3: Select Top 8 Features\n",
      "\n",
      " Step 4: VIF Re-check\n",
      "   Max VIF: 4.04\n",
      "\n",
      "================================================================================\n",
      "FINAL REDUCED CORE FEATURES: P3A (8 features)\n",
      "================================================================================\n",
      "\n",
      "Rank   Feature                                  Imp      VIF    Quality  Type   Score   \n",
      "------------------------------------------------------------------------------------------\n",
      "1      Panamax_Orderbook_Pct_vol30               17.58   3.14    97.4%     3     0.9948\n",
      "2      MGO_yoy                                   17.35   1.10    98.9%     2     0.8908\n",
      "3      Panamax_Fleet_Growth_YoY_vol30            13.80   3.28    97.4%     3     0.8803\n",
      "4      P4_82_yoy                                 10.11   1.05    98.9%     2     0.6717\n",
      "5      Panamax_Idle_Pct_vol30                     4.65   4.04    97.4%     3     0.6037\n",
      "6      Capesize_Orderbook_Pct_vol30               4.12   2.57    97.4%     3     0.5875\n",
      "7      Atlantic_IP_yoy                            6.49   1.32    99.9%     2     0.5643\n",
      "8      PDOPEX_vol30                               1.71   1.47    97.4%     3     0.5148\n",
      "\n",
      "================================================================================\n",
      "REDUCTION SUMMARY:\n",
      "================================================================================\n",
      "  Initial features: 18\n",
      "  Final features:   8\n",
      "  Reduction:        10 features (55.6%)\n",
      "  Max VIF:          4.04\n",
      "  Feature types:\n",
      "    - vol30 (30-day volatility): 5 features\n",
      "    - yoy/diff (year-over-year or first diff): 3 features\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": "# Update ML features: Add features dropped from CORE\nprint(\"\\n\" + \"=\"*80)\nprint(\"UPDATING ML FEATURE SETS\")\nprint(\"=\"*80)\n\n# P1A: Add dropped CORE features to ML set\np1a_dropped_from_core = [f for f in p1a_core_final if f not in p1a_core_reduced]\nprint(f\"\\nP1A:\")\nprint(f\"  Dropped from CORE: {len(p1a_dropped_from_core)} features\")\nprint(f\"  Current ML:        {len(p1a_ml_final)} features\")\n\n# Add dropped features to ML (avoid duplicates)\np1a_ml_updated = list(p1a_ml_final) + [f for f in p1a_dropped_from_core if f not in p1a_ml_final]\nprint(f\"  Updated ML:        {len(p1a_ml_updated)} features\")\n\n# P3A: Add dropped CORE features to ML set\np3a_dropped_from_core = [f for f in p3a_core_final if f not in p3a_core_reduced]\nprint(f\"\\nP3A:\")\nprint(f\"  Dropped from CORE: {len(p3a_dropped_from_core)} features\")\nprint(f\"  Current ML:        {len(p3a_ml_final)} features\")\n\n# Add dropped features to ML (avoid duplicates)\np3a_ml_updated = list(p3a_ml_final) + [f for f in p3a_dropped_from_core if f not in p3a_ml_final]\nprint(f\"  Updated ML:        {len(p3a_ml_updated)} features\")\n\n# Verify no overlap\np1a_overlap = set(p1a_core_reduced) & set(p1a_ml_updated)\np3a_overlap = set(p3a_core_reduced) & set(p3a_ml_updated)\n\nprint(f\"\\n OVERLAP CHECK:\")\nprint(f\"  P1A: {len(p1a_overlap)} overlapping features {'[OK]' if len(p1a_overlap) == 0 else '[FAIL]'}\")\nprint(f\"  P3A: {len(p3a_overlap)} overlapping features {'[OK]' if len(p3a_overlap) == 0 else '[FAIL]'}\")\n\nif len(p1a_overlap) > 0:\n    print(f\"\\n[FAIL] P1A overlap detected: {p1a_overlap}\")\n    \nif len(p3a_overlap) > 0:\n    print(f\"\\n[FAIL] P3A overlap detected: {p3a_overlap}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.514130Z",
     "start_time": "2025-11-01T08:20:40.505748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UPDATING ML FEATURE SETS\n",
      "================================================================================\n",
      "\n",
      "P1A:\n",
      "  Dropped from CORE: 11 features\n",
      "  Current ML:        20 features\n",
      "  Updated ML:        31 features\n",
      "\n",
      "P3A:\n",
      "  Dropped from CORE: 10 features\n",
      "  Current ML:        20 features\n",
      "  Updated ML:        30 features\n",
      "\n",
      " OVERLAP CHECK:\n",
      "  P1A: 0 overlapping features [OK]\n",
      "  P3A: 0 overlapping features [OK]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": "# Save v2 feature sets\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAVING V2 FEATURE SETS (6-8 CORE + UPDATED ML)\")\nprint(\"=\"*80)\n\n# Save reduced CORE features\nfeatures_all[p1a_core_reduced].to_csv('data/processed/features/p1a_core_features_final_v2.csv')\nfeatures_all[p3a_core_reduced].to_csv('data/processed/features/p3a_core_features_final_v2.csv')\n\nprint(\"\\n[OK] V2 CORE features saved:\")\nprint(f\"   - P1A: {len(p1a_core_reduced)} features × {len(features_all)} rows\")\nprint(f\"   - P3A: {len(p3a_core_reduced)} features × {len(features_all)} rows\")\n\n# Save updated ML features\nfeatures_all[p1a_ml_updated].to_csv('data/processed/features/p1a_ml_features_final_v2.csv')\nfeatures_all[p3a_ml_updated].to_csv('data/processed/features/p3a_ml_features_final_v2.csv')\n\nprint(\"\\n[OK] V2 ML features saved:\")\nprint(f\"   - P1A: {len(p1a_ml_updated)} features × {len(features_all)} rows\")\nprint(f\"   - P3A: {len(p3a_ml_updated)} features × {len(features_all)} rows\")\n\n# Save reduction audit trails\np1a_reduction_audit.to_csv('data/processed/features/p1a_core_reduction_audit_v2.csv', index=False)\np3a_reduction_audit.to_csv('data/processed/features/p3a_core_reduction_audit_v2.csv', index=False)\n\nprint(\"\\n[OK] Reduction audit trails saved:\")\nprint(f\"   - data/processed/features/p1a_core_reduction_audit_v2.csv\")\nprint(f\"   - data/processed/features/p3a_core_reduction_audit_v2.csv\")\n\n# Create comprehensive v2 allocation decisions\ndecisions_v2 = []\n\n# P1A CORE v2\nfor feat in p1a_core_reduced:\n    audit_row = p1a_reduction_audit[p1a_reduction_audit['Feature'] == feat].iloc[0]\n    decisions_v2.append({\n        'Route': 'P1A',\n        'Set': 'CORE_v2',\n        'Feature': feat,\n        'Importance': audit_row['Importance'],\n        'VIF': audit_row['VIF_new'],\n        'Data_Quality_Pct': audit_row['Data_Quality_Pct'],\n        'Type_Score': audit_row['Type_Score'],\n        'Composite_Score': audit_row['Composite_Score'],\n        'Rationale': f\"Selected via programmatic reduction: {audit_row['Type_Name']}, Composite Score={audit_row['Composite_Score']:.4f}\"\n    })\n\n# P1A ML v2 (moved from CORE)\nfor feat in p1a_dropped_from_core:\n    imp = feature_ranking_p1a[feature_ranking_p1a['Feature'] == feat]['Importance'].values[0]\n    decisions_v2.append({\n        'Route': 'P1A',\n        'Set': 'ML_v2',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Data_Quality_Pct': np.nan,\n        'Type_Score': np.nan,\n        'Composite_Score': np.nan,\n        'Rationale': 'Moved from CORE to ML (reduction)'\n    })\n\n# P1A ML v2 (original)\nfor feat in p1a_ml_final:\n    imp = feature_ranking_p1a[feature_ranking_p1a['Feature'] == feat]['Importance'].values[0]\n    decisions_v2.append({\n        'Route': 'P1A',\n        'Set': 'ML_v2',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Data_Quality_Pct': np.nan,\n        'Type_Score': np.nan,\n        'Composite_Score': np.nan,\n        'Rationale': 'Original ML feature'\n    })\n\n# P3A CORE v2\nfor feat in p3a_core_reduced:\n    audit_row = p3a_reduction_audit[p3a_reduction_audit['Feature'] == feat].iloc[0]\n    decisions_v2.append({\n        'Route': 'P3A',\n        'Set': 'CORE_v2',\n        'Feature': feat,\n        'Importance': audit_row['Importance'],\n        'VIF': audit_row['VIF_new'],\n        'Data_Quality_Pct': audit_row['Data_Quality_Pct'],\n        'Type_Score': audit_row['Type_Score'],\n        'Composite_Score': audit_row['Composite_Score'],\n        'Rationale': f\"Selected via programmatic reduction: {audit_row['Type_Name']}, Composite Score={audit_row['Composite_Score']:.4f}\"\n    })\n\n# P3A ML v2 (moved from CORE)\nfor feat in p3a_dropped_from_core:\n    imp = feature_ranking_p3a[feature_ranking_p3a['Feature'] == feat]['Importance'].values[0]\n    decisions_v2.append({\n        'Route': 'P3A',\n        'Set': 'ML_v2',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Data_Quality_Pct': np.nan,\n        'Type_Score': np.nan,\n        'Composite_Score': np.nan,\n        'Rationale': 'Moved from CORE to ML (reduction)'\n    })\n\n# P3A ML v2 (original)\nfor feat in p3a_ml_final:\n    imp = feature_ranking_p3a[feature_ranking_p3a['Feature'] == feat]['Importance'].values[0]\n    decisions_v2.append({\n        'Route': 'P3A',\n        'Set': 'ML_v2',\n        'Feature': feat,\n        'Importance': imp,\n        'VIF': np.nan,\n        'Data_Quality_Pct': np.nan,\n        'Type_Score': np.nan,\n        'Composite_Score': np.nan,\n        'Rationale': 'Original ML feature'\n    })\n\ndecisions_v2_df = pd.DataFrame(decisions_v2)\ndecisions_v2_df.to_csv('data/processed/features/feature_allocation_decisions_v2.csv', index=False)\n\nprint(\"\\n[OK] V2 allocation decisions saved:\")\nprint(f\"   - data/processed/features/feature_allocation_decisions_v2.csv\")\nprint(f\"   - Total entries: {len(decisions_v2_df)}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"V2 FEATURE SETS COMPLETE\")\nprint(\"=\"*80)\nprint(\"\\n Final Summary:\")\nprint(f\"  P1A CORE v2: {len(p1a_core_reduced)} features (was {len(p1a_core_final)})\")\nprint(f\"  P1A ML v2:   {len(p1a_ml_updated)} features (was {len(p1a_ml_final)})\")\nprint(f\"  P3A CORE v2: {len(p3a_core_reduced)} features (was {len(p3a_core_final)})\")\nprint(f\"  P3A ML v2:   {len(p3a_ml_updated)} features (was {len(p3a_ml_final)})\")\nprint(f\"\\n[OK] NO OVERLAP between CORE and ML sets\")\nprint(f\"\\n Ready for Notebook 04: ARIMAX Test Configurations\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T08:20:40.683202Z",
     "start_time": "2025-11-01T08:20:40.537137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING V2 FEATURE SETS (6-8 CORE + UPDATED ML)\n",
      "================================================================================\n",
      "\n",
      "[OK] V2 CORE features saved:\n",
      "   - P1A: 8 features × 1153 rows\n",
      "   - P3A: 8 features × 1153 rows\n",
      "\n",
      "[OK] V2 ML features saved:\n",
      "   - P1A: 31 features × 1153 rows\n",
      "   - P3A: 30 features × 1153 rows\n",
      "\n",
      "[OK] Reduction audit trails saved:\n",
      "   - data/processed/features/p1a_core_reduction_audit_v2.csv\n",
      "   - data/processed/features/p3a_core_reduction_audit_v2.csv\n",
      "\n",
      "[OK] V2 allocation decisions saved:\n",
      "   - data/processed/features/feature_allocation_decisions_v2.csv\n",
      "   - Total entries: 77\n",
      "\n",
      "================================================================================\n",
      "V2 FEATURE SETS COMPLETE\n",
      "================================================================================\n",
      "\n",
      " Final Summary:\n",
      "  P1A CORE v2: 8 features (was 19)\n",
      "  P1A ML v2:   31 features (was 20)\n",
      "  P3A CORE v2: 8 features (was 18)\n",
      "  P3A ML v2:   30 features (was 20)\n",
      "\n",
      "[OK] NO OVERLAP between CORE and ML sets\n",
      "\n",
      " Ready for Notebook 04: ARIMAX Test Configurations\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
